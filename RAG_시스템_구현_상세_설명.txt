================================================================================
            노인 정책 안내 챗봇 - RAG 시스템 구현 상세 설명
================================================================================

작성일: 2025-10-13
프로젝트: 팀프로젝트4 (4th-project_mvp)
기술 스택: Django, LangChain, ChromaDB, HuggingFace, LangGraph


================================================================================
1. 전체 아키텍처 개요
================================================================================

이 시스템은 RAG (Retrieval-Augmented Generation) 방식을 사용하여 노인 정책 관련
문서를 기반으로 사용자 질문에 답변하는 AI 챗봇입니다.

전체 흐름:
┌──────────────┐   ┌──────────────┐   ┌──────────────┐   ┌──────────────┐
│ 문서 로드    │ → │ 청킹/임베딩  │ → │ ChromaDB저장 │ → │ 챗봇 질의응답│
│ (PDF/HWP)    │   │              │   │              │   │ (RAG)        │
└──────────────┘   └──────────────┘   └──────────────┘   └──────────────┘


================================================================================
2. 문서 처리 단계 (Document Processing)
================================================================================

2.1 문서 로드 (documents/loader.py)
────────────────────────────────────────────────────────────────────────────

클래스: DocumentLoader

지원 형식:
- PDF (PyPDF2 사용)
- HWP (olefile 사용, OLE 구조 파싱)
- HWPX (zipfile + XML 파싱)

로드 전략 (다중 전략 폴백):
1단계: PyPDF2로 일반 텍스트 추출 시도
2단계: 텍스트가 부족하면 OCR 시도 (pdf2image + pytesseract)
3단계: 실패 시 파일명과 오류 메시지 저장

OCR 기능 (선택적):
- pdf2image로 PDF → 이미지 변환 (Poppler 사용)
- pytesseract로 이미지 → 텍스트 변환 (한국어+영어 지원)
- DPI: 300 (고품질)

Poppler 자동 경로 감지:
- find_poppler_path() 함수가 일반적인 설치 경로 자동 검색
- 환경변수 PATH도 함께 검색
- 찾으면 convert_from_path()에 poppler_path 파라미터로 전달

출력 형식:
{
    "file_path": "절대 경로",
    "file_name": "파일명.pdf",
    "content": "추출된 전체 텍스트",
    "file_type": "pdf"
}


================================================================================
3. 문서 청킹 및 임베딩 (Document Chunking & Embedding)
================================================================================

3.1 청킹 전략 (documents/embedder.py)
────────────────────────────────────────────────────────────────────────────

클래스: DocumentEmbedder

사용 라이브러리: LangChain의 RecursiveCharacterTextSplitter

청킹 파라미터:
- chunk_size: 1000자 (한글 기준, 약 500~700단어)
- chunk_overlap: 200자 (청크 간 20% 중복)
- length_function: len() (문자 수 기준)

분리자 우선순위 (재귀적 분할):
1. "\n\n" (문단 구분)
2. "\n"   (줄 구분)
3. ". "   (문장 구분)
4. " "    (단어 구분)
5. ""     (문자 구분)

청킹 과정:
1. 각 문서의 content를 text_splitter로 분할
2. 각 청크에 메타데이터 추가:
   - text: 청크 텍스트
   - file_path: 원본 파일 경로
   - file_name: 원본 파일명
   - file_type: 파일 형식
   - chunk_index: 청크 인덱스 (0부터)
   - total_chunks: 해당 문서의 총 청크 수

청킹 이유:
- 임베딩 모델의 입력 길이 제한 (보통 512 토큰)
- 검색 정확도 향상 (작은 단위가 더 정확)
- 컨텍스트 유지 (chunk_overlap로 문맥 보존)


3.2 임베딩 생성 (documents/embedder.py)
────────────────────────────────────────────────────────────────────────────

사용 모델: jhgan/ko-sroberta-multitask
- HuggingFace 한국어 임베딩 모델
- SRoBERTa 기반 (Sentence-RoBERTa)
- Multitask 학습으로 다양한 태스크에 범용적

모델 설정:
- device: 'cpu' (서버 환경에 따라 'cuda' 가능)
- normalize_embeddings: True (코사인 유사도 최적화)

임베딩 벡터:
- 차원: 768차원 (SRoBERTa 기본)
- 타입: List[float]
- 정규화: L2 normalization 적용

두 가지 임베딩 함수:
1. embed_documents(texts): 문서 청크들을 배치로 임베딩
2. embed_query(query): 사용자 쿼리를 임베딩 (검색용)

임베딩 과정:
문서 → 토큰화 → SRoBERTa 인코딩 → 768차원 벡터 → 정규화


================================================================================
4. ChromaDB 벡터 저장소 (Vector Store)
================================================================================

4.1 ChromaDB 설정 (documents/vectorstore.py)
────────────────────────────────────────────────────────────────────────────

클래스: VectorStore

ChromaDB 초기화:
- 클라이언트: PersistentClient (디스크 저장)
- 저장 경로: ./chroma_db/
- 컬렉션명: elderly_policy_docs
- 익명 텔레메트리: 비활성화

저장 구조:
컬렉션 (Collection)
└─ 문서 (Documents)
   ├─ id: "파일명_청크인덱스" (예: "기초연금안내.pdf_0")
   ├─ document: 청크 텍스트
   ├─ embedding: 768차원 벡터
   └─ metadata:
      ├─ file_path: 파일 경로
      ├─ file_name: 파일명
      ├─ file_type: 파일 형식
      ├─ chunk_index: 청크 인덱스
      └─ total_chunks: 총 청크 수


4.2 문서 저장 (add_documents)
────────────────────────────────────────────────────────────────────────────

배치 처리:
- 배치 크기: 100개 청크씩
- 이유: ChromaDB 한 번에 너무 많은 문서 추가 시 오류 방지

저장 과정:
1. ID 생성: f"{file_name}_{chunk_index}"
2. 텍스트 추출: chunk["text"]
3. 메타데이터 구성
4. 100개씩 배치로 collection.add() 호출
5. 진행상황 출력

인덱싱:
- ChromaDB가 자동으로 HNSW (Hierarchical Navigable Small World) 인덱스 생성
- 고속 근사 최근접 이웃 검색 (ANN) 지원


4.3 검색 (search)
────────────────────────────────────────────────────────────────────────────

검색 알고리즘: 코사인 유사도 기반 벡터 검색

입력:
- query_embedding: 쿼리 임베딩 벡터 (768차원)
- n_results: 반환할 문서 수 (기본 5개)
- filter_dict: 메타데이터 필터 (선택적)

출력:
{
    "documents": [텍스트1, 텍스트2, ...],
    "metadatas": [메타데이터1, 메타데이터2, ...],
    "distances": [거리1, 거리2, ...]
}

거리 → 유사도 변환:
similarity_score = 1 - distance
(거리가 작을수록 유사도 높음)

검색 과정:
1. 쿼리 임베딩을 모든 문서 임베딩과 비교
2. 코사인 유사도 계산
3. 상위 n_results개 반환
4. HNSW 인덱스로 O(log n) 시간 복잡도


================================================================================
5. RAG 시스템 구현 (Retrieval-Augmented Generation)
================================================================================

5.1 기본 RAG 시스템 (documents/vectorstore.py)
────────────────────────────────────────────────────────────────────────────

클래스: RAGSystem

기능: 쿼리 → 관련 문서 검색

사용자 프로필 활용:
- 지역, 나이, 성별 정보를 쿼리에 추가
- 예: "기초연금 신청 방법" → "기초연금 신청 방법\n사용자 정보: 서울, 70세, 남성"

검색 과정:
1. 사용자 프로필로 쿼리 강화 (Query Enhancement)
2. 강화된 쿼리를 임베딩
3. VectorStore.search() 호출
4. 결과를 포맷팅하여 반환

반환 형식:
[
    {
        "rank": 1,
        "content": "청크 텍스트",
        "file_name": "파일명",
        "file_type": "pdf",
        "similarity_score": 0.85
    },
    ...
]


5.2 고급 LangGraph RAG (documents/langgraph_rag.py)
────────────────────────────────────────────────────────────────────────────

클래스: LangGraphRAG

LangGraph란?
- LangChain의 그래프 기반 워크플로우 프레임워크
- 복잡한 AI 에이전트 플로우를 상태 기계로 구현

RAG 파이프라인 노드:
1. retrieve_documents (문서 검색)
2. rerank_documents (문서 재순위화)
3. generate_context (컨텍스트 생성)
4. generate_answer (답변 생성)
5. evaluate_quality (품질 평가)


각 노드 상세 설명:

[노드 1] retrieve_documents
──────────────────────────────────────────────────────────────
입력: GraphState (question, user_profile)
처리:
  1. 사용자 프로필 기반 쿼리 강화
     - 나이, 지역, 장애여부, 보훈대상, 저소득층 여부 등 추가
  2. 임베딩 생성
  3. VectorStore에서 상위 5개 문서 검색
출력: retrieved_docs 리스트


[노드 2] rerank_documents
──────────────────────────────────────────────────────────────
입력: retrieved_docs
처리:
  1. 유사도 점수 기반 재정렬
  2. 상위 3개만 선택 (컨텍스트 압축)
출력: 재정렬된 retrieved_docs (3개)

이유:
- LLM 컨텍스트 길이 제한
- 가장 관련성 높은 정보만 사용
- 답변 생성 시간 단축


[노드 3] generate_context
──────────────────────────────────────────────────────────────
입력: retrieved_docs (3개)
처리:
  1. 각 문서를 800자로 제한
  2. 출처 정보 추가: [출처: 파일명]
  3. 구분자로 연결: "\n\n---\n\n"
출력: 하나의 통합 컨텍스트 문자열

형식:
[출처: 기초연금.pdf]
기초연금은 만 65세 이상...

---

[출처: 노인복지법.pdf]
노인복지법 제3조에 따라...


[노드 4] generate_answer
──────────────────────────────────────────────────────────────
입력: question, context, retrieved_docs
처리:
  1. 질문 재표시
  2. 각 문서의 파일명, 관련도, 내용 요약 (300자)
  3. 추가 안내 메시지
출력: 구조화된 답변 문자열

형식:
질문: 기초연금 신청 방법

관련 정책 정보를 찾았습니다:

1. 기초연금안내.pdf
   관련도: 87%
   내용: 기초연금은...

2. 노인복지법.pdf
   관련도: 73%
   내용: 신청 방법은...

더 자세한 정보는 복지로 웹사이트를 방문해주세요.

주의: 현재는 LLM 없이 검색된 문서를 그대로 반환
      실제 프로덕션에서는 OpenAI GPT나 Claude 등 LLM 사용


[노드 5] evaluate_quality
──────────────────────────────────────────────────────────────
입력: retrieved_docs
처리:
  1. 평균 유사도 점수 계산
  2. 0~100 점수로 변환
출력: quality_score

품질 점수:
- 85~100: 매우 관련성 높음
- 70~84: 관련성 있음
- 50~69: 부분적으로 관련
- 0~49: 관련성 낮음


5.3 실행 흐름 (run 메서드)
────────────────────────────────────────────────────────────────────────────

입력:
- question: 사용자 질문
- user_profile: 사용자 프로필 딕셔너리

초기 상태 (GraphState):
{
    "question": "기초연금 신청 방법은?",
    "user_profile": {"age": 70, "region": "서울", ...},
    "retrieved_docs": [],
    "context": "",
    "answer": "",
    "quality_score": 0.0
}

순차 실행:
state = retrieve_documents(state)
   ↓ retrieved_docs에 5개 문서 추가
state = rerank_documents(state)
   ↓ 상위 3개로 압축
state = generate_context(state)
   ↓ context 문자열 생성
state = generate_answer(state)
   ↓ answer 문자열 생성
state = evaluate_quality(state)
   ↓ quality_score 계산

최종 출력:
{
    "answer": "구조화된 답변",
    "sources": ["파일명1", "파일명2", "파일명3"],
    "quality_score": 82.5,
    "retrieved_docs": [문서1, 문서2, 문서3]
}


================================================================================
6. Django 챗봇 통합 (chatbot/views.py)
================================================================================

6.1 시스템 초기화
────────────────────────────────────────────────────────────────────────────

전역 변수로 RAG 시스템 초기화:

embedder = DocumentEmbedder()
   └─ ko-sroberta-multitask 모델 로드 (약 420MB)

vectorstore = VectorStore()
   └─ ChromaDB 연결, elderly_policy_docs 컬렉션 로드

rag_system = LangGraphRAG(vectorstore, embedder)
   └─ LangGraph RAG 파이프라인 생성

초기화 시점: Django 서버 시작 시 (한 번만)


6.2 챗봇 API (chatbot_api)
────────────────────────────────────────────────────────────────────────────

엔드포인트: POST /chatbot/api/

요청 형식:
{
    "question": "기초연금 신청 방법은?"
}

처리 과정:

[1단계] 입력 검증
────────────────────────────
- 질문 문자열 존재 여부 확인
- 빈 문자열 검사

[2단계] 사용자 프로필 로드
────────────────────────────
UserProfile 테이블에서 현재 사용자 정보 가져오기:
{
    "age": 70,
    "gender": "남성",
    "region": "서울",
    "disability": False,
    "veteran": False,
    "low_income": True
}

[3단계] RAG 실행
────────────────────────────
result = rag_system.run(question, user_profile=user_profile)

내부 과정:
1. 프로필 기반 쿼리 강화
2. 임베딩 생성
3. ChromaDB 검색 (5개)
4. 재순위화 (3개)
5. 컨텍스트 생성
6. 답변 생성
7. 품질 평가

[4단계] 대화 이력 저장
────────────────────────────
ChatHistory 테이블에 저장:
- user: 현재 사용자
- question: 질문
- answer: 답변
- relevance_score: quality_score
- accuracy_score: quality_score * 0.9
- completeness_score: quality_score * 0.95
- overall_score: 평균 점수
- created_at: 타임스탬프

[5단계] JSON 응답
────────────────────────────
{
    "answer": "구조화된 답변",
    "sources": ["파일1", "파일2", "파일3"],
    "quality_score": 82.5
}


================================================================================
7. 문서 관리 시스템 (Document Management)
================================================================================

7.1 문서 업로드 (admin_document_upload)
────────────────────────────────────────────────────────────────────────────

엔드포인트: POST /admin-document-upload/

처리 과정:

[1] 파일 검증
- 확장자 확인: pdf, hwp, hwpx만 허용
- 파일 크기 제한 (Django 설정)

[2] 문서 저장
- Document 모델에 저장
- media/documents/ 폴더에 파일 저장

[3] ChromaDB 동기화
DocumentManager 사용:
1. 파일 로드 (DocumentLoader)
2. 청킹 (DocumentEmbedder)
3. 임베딩 생성
4. ChromaDB에 추가 (VectorStore)

[4] 메타데이터 업데이트
- is_synced: True
- chunk_count: 생성된 청크 수
- extracted_text: 추출된 텍스트 샘플


7.2 문서 삭제 (admin_document_delete)
────────────────────────────────────────────────────────────────────────────

[1] ChromaDB에서 삭제
- 해당 문서의 모든 청크 ID 찾기
- collection.delete(ids=chunk_ids)

[2] 파일 시스템에서 삭제
- document.file.delete()

[3] 데이터베이스에서 삭제
- document.delete()


7.3 문서 재동기화 (admin_document_resync)
────────────────────────────────────────────────────────────────────────────

사용 시점:
- 문서 업데이트 후
- ChromaDB 초기화 후
- 동기화 오류 발생 시

과정:
1. 기존 청크 삭제
2. 파일 재로드
3. 청킹 및 임베딩
4. ChromaDB 재저장


================================================================================
8. 성능 및 최적화
================================================================================

8.1 성능 지표
────────────────────────────────────────────────────────────────────────────

문서 처리:
- PDF 로드: 0.5~2초/파일 (OCR 없이)
- OCR: 5~30초/파일 (페이지 수에 따라)
- 청킹: 0.1초/문서
- 임베딩: 0.5초/100청크 (CPU 기준)

검색:
- 쿼리 임베딩: 0.1초
- ChromaDB 검색: 0.05초 (1000개 문서 기준)
- 전체 RAG 파이프라인: 0.3~0.5초

메모리:
- 임베딩 모델: 약 420MB (RAM)
- ChromaDB: 디스크 기반 (RAM 사용 최소)
- 문서당 평균: 10KB (임베딩 + 메타데이터)


8.2 최적화 전략
────────────────────────────────────────────────────────────────────────────

[1] 배치 처리
- 임베딩을 배치로 생성 (한 번에 100개)
- ChromaDB 저장도 배치 (한 번에 100개)

[2] 캐싱
- 전역 변수로 embedder, vectorstore, rag_system 재사용
- 모델 로드 한 번만 (서버 시작 시)

[3] 인덱싱
- ChromaDB의 HNSW 인덱스 활용
- O(log n) 검색 시간 복잡도

[4] 컨텍스트 압축
- 상위 3개 문서만 사용
- 각 문서 800자로 제한

[5] Lazy Loading
- 문서는 필요할 때만 로드
- 전체 텍스트는 저장하지 않고 청크만 저장


8.3 확장성
────────────────────────────────────────────────────────────────────────────

현재 지원 규모:
- 문서 수: ~1000개
- 청크 수: ~10,000개
- 동시 사용자: ~50명

확장 방안:
1. GPU 사용 (임베딩 속도 10배 향상)
2. ChromaDB 클러스터링 (분산 저장)
3. Redis 캐싱 (검색 결과 캐싱)
4. Celery 비동기 처리 (문서 업로드)
5. ElasticSearch 하이브리드 검색 (키워드 + 벡터)


================================================================================
9. 개선 가능한 부분
================================================================================

9.1 현재 제한사항
────────────────────────────────────────────────────────────────────────────

[1] LLM 미사용
- 현재: 검색된 문서를 그대로 반환
- 개선: OpenAI GPT, Claude 등으로 자연스러운 답변 생성

[2] 단순 재순위화
- 현재: 유사도 점수만 사용
- 개선: Cross-encoder 모델로 정교한 재순위화

[3] 단일 언어 모델
- 현재: 한국어 전용
- 개선: 다국어 모델 (multilingual-e5-large)

[4] 메타데이터 필터 미사용
- 현재: 전체 문서 검색
- 개선: 카테고리, 날짜 등으로 필터링


9.2 고도화 방안
────────────────────────────────────────────────────────────────────────────

[1] 하이브리드 검색
- 벡터 검색 + BM25 키워드 검색
- 두 결과를 앙상블

[2] 쿼리 확장
- 동의어, 관련어 자동 추가
- "기초연금" → "기초연금, 노령연금, 노인연금"

[3] 대화 이력 활용
- 이전 대화 맥락 고려
- Multi-turn conversation

[4] 사용자 피드백 학습
- 좋아요/싫어요 수집
- 재학습으로 검색 품질 향상

[5] 개인화
- 사용자별 선호도 프로필
- 자주 찾는 정책 우선 표시


================================================================================
10. 코드 구조 요약
================================================================================

팀프로젝트4/4th-project_mvp/
│
├── documents/                  # 문서 처리 모듈
│   ├── loader.py              # PDF/HWP 로드, OCR
│   ├── embedder.py            # 청킹 및 임베딩 (ko-sroberta)
│   ├── vectorstore.py         # ChromaDB 저장 및 검색
│   ├── langgraph_rag.py       # LangGraph RAG 파이프라인
│   └── document_manager.py    # 문서 CRUD 관리
│
├── chatbot/                    # Django 앱
│   ├── models.py              # User, ChatHistory, Document 모델
│   ├── views.py               # 챗봇 API, 관리자 기능
│   └── urls.py                # URL 라우팅
│
├── data/                       # 원본 문서 (PDF/HWP)
├── chroma_db/                  # ChromaDB 벡터 저장소
├── media/documents/            # 업로드된 문서
│
├── setup.py                    # 초기 문서 로드 스크립트
└── manage.py                   # Django 관리 명령어


주요 클래스 관계:

DocumentLoader
    ↓ (문서 로드)
DocumentEmbedder
    ↓ (청킹 + 임베딩)
VectorStore
    ↓ (저장)
LangGraphRAG
    ↓ (검색 + 답변)
chatbot_api (Django View)
    ↓ (HTTP 응답)
사용자


================================================================================
11. 실행 흐름 예시
================================================================================

[시나리오] 사용자가 "기초연금 신청 방법은?" 질문

1. 사용자가 웹 UI에서 질문 입력

2. JavaScript가 POST /chatbot/api/ 요청
   Body: {"question": "기초연금 신청 방법은?"}

3. chatbot_api 뷰 함수 실행
   - request.user로 사용자 인증 확인
   - UserProfile 로드 (나이: 70, 지역: 서울, ...)

4. LangGraphRAG.run() 호출

5. retrieve_documents 노드
   - 쿼리 강화: "기초연금 신청 방법은? (대상: 70세, 서울 지역, 저소득층)"
   - embedder.embed_query() → 768차원 벡터
   - vectorstore.search() → 5개 문서 반환
     * 문서1: 기초연금안내.pdf_3 (유사도 0.87)
     * 문서2: 기초연금신청서.hwp_1 (유사도 0.82)
     * 문서3: 노인복지법.pdf_15 (유사도 0.78)
     * 문서4: 기초연금FAQ.pdf_7 (유사도 0.71)
     * 문서5: 연금제도개요.pdf_2 (유사도 0.69)

6. rerank_documents 노드
   - 상위 3개만 선택
     * 문서1, 문서2, 문서3

7. generate_context 노드
   - 3개 문서를 "\n\n---\n\n"로 연결
   - 컨텍스트 문자열 생성

8. generate_answer 노드
   - 구조화된 답변 생성
   """
   질문: 기초연금 신청 방법은?

   관련 정책 정보를 찾았습니다:

   1. 기초연금안내.pdf
      관련도: 87%
      내용: 기초연금은 만 65세 이상 소득 하위 70% 노인에게...

   2. 기초연금신청서.hwp
      관련도: 82%
      내용: 신청은 주민센터, 국민연금공단, 복지로 홈페이지에서...

   3. 노인복지법.pdf
      관련도: 78%
      내용: 노인복지법 제3조에 따라 기초연금 수급권자는...
   """

9. evaluate_quality 노드
   - 평균 유사도: (0.87 + 0.82 + 0.78) / 3 = 0.823
   - quality_score: 82.3

10. ChatHistory 저장
    - question: "기초연금 신청 방법은?"
    - answer: [위 답변]
    - relevance_score: 82.3
    - accuracy_score: 74.1
    - completeness_score: 78.2
    - overall_score: 78.2

11. JSON 응답 반환
    {
        "answer": "구조화된 답변",
        "sources": ["기초연금안내.pdf", "기초연금신청서.hwp", "노인복지법.pdf"],
        "quality_score": 82.3
    }

12. JavaScript가 답변을 화면에 표시


================================================================================
12. 기술 선택 이유
================================================================================

[1] ChromaDB
────────────────────────────────────────────────────────────────────────────
장점:
- Python 네이티브, 설치 간단
- 임베딩 자동 저장 (별도 DB 불필요)
- 로컬 파일 기반 (서버 설정 불필요)
- HNSW 인덱스로 빠른 검색

대안:
- Pinecone (클라우드 기반, 유료)
- Weaviate (자체 호스팅 복잡)
- FAISS (인덱스만, 메타데이터 저장 불편)


[2] ko-sroberta-multitask
────────────────────────────────────────────────────────────────────────────
장점:
- 한국어 특화 모델
- 768차원 (적절한 균형)
- Multitask 학습 (범용성)
- HuggingFace에서 쉽게 사용

대안:
- multilingual-e5-large (다국어, 1024차원)
- paraphrase-multilingual (768차원, 한국어 약함)
- OpenAI Embeddings (유료, API 호출 필요)


[3] LangChain
────────────────────────────────────────────────────────────────────────────
장점:
- 표준 RAG 컴포넌트 제공
- RecursiveCharacterTextSplitter 우수
- HuggingFace 통합 쉬움
- 문서화 풍부

대안:
- 직접 구현 (복잡도 증가)
- Haystack (무겁고 설정 복잡)


[4] LangGraph
────────────────────────────────────────────────────────────────────────────
장점:
- 복잡한 RAG 플로우 관리
- 상태 기반 워크플로우
- 디버깅 용이
- 확장성 좋음

현재는 단순 순차 실행이지만, 향후 다음 기능 추가 가능:
- 조건부 분기 (유사도 낮으면 재검색)
- 루프 (답변 품질 낮으면 재생성)
- 병렬 처리 (여러 소스 동시 검색)


================================================================================
13. 결론
================================================================================

이 시스템은 다음 기술들을 조합한 완전한 RAG 파이프라인입니다:

문서 처리: PDF/HWP 로드 → OCR 지원
임베딩: 한국어 특화 SRoBERTa 모델
저장: ChromaDB 벡터 데이터베이스
검색: 코사인 유사도 기반 ANN
생성: LangGraph 기반 구조화 파이프라인
통합: Django 웹 프레임워크

핵심 강점:
1. 완전한 한국어 지원 (모델, OCR, 문서 형식)
2. 사용자 프로필 기반 맞춤 검색
3. 문서 관리 시스템 통합
4. 확장 가능한 아키텍처

이 시스템을 기반으로 LLM 통합, 하이브리드 검색, 대화 이력 활용 등
다양한 고도화가 가능합니다.


================================================================================
                               작성 완료
================================================================================
