# =========================================
# ğŸ” ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± ëª¨ë“ˆ
# =========================================

import os
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple
from abc import ABC, abstractmethod
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# LangChain ì„í¬íŠ¸
try:
    from langchain.schema import Document
    from langchain.schema.retriever import BaseRetriever
    from langchain.callbacks.manager import CallbackManagerForRetrieverRun
    from langchain_community.retrievers import BM25Retriever
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    # LangChain ì—†ì´ë„ ê¸°ë³¸ ê¸°ëŠ¥ êµ¬í˜„
    class Document:
        def __init__(self, page_content: str, metadata: Dict = None):
            self.page_content = page_content
            self.metadata = metadata or {}

logger = logging.getLogger(__name__)


class BaseWelfareRetriever(ABC):
    """ë³µì§€ ì •ì±… ë¦¬íŠ¸ë¦¬ë²„ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.documents = []
        self.is_initialized = False
    
    @abstractmethod
    def add_documents(self, documents: List[Document]) -> None:
        """ë¬¸ì„œ ì¶”ê°€"""
        pass
    
    @abstractmethod
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        pass
    
    def get_retriever_info(self) -> Dict[str, Any]:
        """ë¦¬íŠ¸ë¦¬ë²„ ì •ë³´ ë°˜í™˜"""
        return {
            'name': self.name,
            'document_count': len(self.documents),
            'is_initialized': self.is_initialized
        }


class KeywordRetriever(BaseWelfareRetriever):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ (TF-IDF + BM25)"""
    
    def __init__(self, use_bm25: bool = True):
        super().__init__("keyword_retriever")
        self.use_bm25 = use_bm25
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.bm25_retriever = None
        
        # ë…¸ì¸ë³µì§€ ì „ìš© í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        self.welfare_keywords = {
            "ë…¸ì¸": 2.0, "ì–´ë¥´ì‹ ": 2.0, "ê³ ë ¹ì": 2.0,
            "ë³µì§€": 1.8, "ì§€ì›": 1.8, "í˜œíƒ": 1.5,
            "ì˜ë£Œë¹„": 1.7, "ê°„ë³‘": 1.6, "ìš”ì–‘": 1.6, "ëŒë´„": 1.6,
            "ìƒí™œì§€ì›": 1.5, "ìˆ˜ë‹¹": 1.4, "ì—°ê¸ˆ": 1.4,
            "ì‹ ì²­": 1.3, "ëŒ€ìƒ": 1.3, "ìê²©": 1.3, "ì¡°ê±´": 1.3
        }
    
    def add_documents(self, documents: List[Document]) -> None:
        """ë¬¸ì„œ ì¶”ê°€ ë° ì¸ë±ì‹±"""
        self.documents = documents
        texts = [doc.page_content for doc in documents]
        
        # TF-IDF ë²¡í„°í™”
        self.tfidf_vectorizer = TfidfVectorizer(
            stop_words=None,  # í•œêµ­ì–´ ë¶ˆìš©ì–´ëŠ” ë³„ë„ ì²˜ë¦¬
            ngram_range=(1, 2),  # 1-2gram
            max_features=10000,
            token_pattern=r'[ê°€-í£]+|[a-zA-Z]+|\d+'  # í•œê¸€, ì˜ì–´, ìˆ«ì
        )
        
        try:
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
            logger.info(f"TF-IDF ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± ì™„ë£Œ: {self.tfidf_matrix.shape}")
        except Exception as e:
            logger.error(f"TF-IDF ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # BM25 ë¦¬íŠ¸ë¦¬ë²„ (LangChain ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
        if self.use_bm25 and LANGCHAIN_AVAILABLE:
            try:
                self.bm25_retriever = BM25Retriever.from_documents(documents)
                self.bm25_retriever.k = 5
                logger.info("BM25 ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"BM25 ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨, TF-IDFë§Œ ì‚¬ìš©: {e}")
                self.use_bm25 = False
        
        self.is_initialized = True
        logger.info(f"í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
    
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized:
            logger.error("ë¦¬íŠ¸ë¦¬ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        results = []
        
        # BM25 ê²€ìƒ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.use_bm25 and self.bm25_retriever:
            try:
                bm25_docs = self.bm25_retriever.get_relevant_documents(query)
                bm25_scores = self._calculate_keyword_boost(query, bm25_docs)
                
                for doc, score in zip(bm25_docs[:k], bm25_scores):
                    doc.metadata['retriever_type'] = 'bm25'
                    doc.metadata['score'] = score
                    results.append(doc)
                
            except Exception as e:
                logger.warning(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # TF-IDF ê²€ìƒ‰ (ë³´ì™„ ë˜ëŠ” ë©”ì¸)
        if not results or len(results) < k:
            tfidf_docs = self._tfidf_search(query, k)
            
            # BM25 ê²°ê³¼ì™€ ì¤‘ë³µ ì œê±°
            existing_contents = {doc.page_content for doc in results}
            
            for doc in tfidf_docs:
                if doc.page_content not in existing_contents and len(results) < k:
                    doc.metadata['retriever_type'] = 'tfidf'
                    results.append(doc)
        
        return results[:k]
    
    def _tfidf_search(self, query: str, k: int) -> List[Document]:
        """TF-IDF ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.tfidf_vectorizer.transform([query])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
            
            # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
            boosted_similarities = self._apply_keyword_boost(query, similarities)
            
            # ìƒìœ„ kê°œ ì„ íƒ
            top_indices = np.argsort(boosted_similarities)[::-1][:k]
            
            results = []
            for idx in top_indices:
                doc = self.documents[idx]
                # ë©”íƒ€ë°ì´í„°ì— ì ìˆ˜ ì¶”ê°€
                doc.metadata = doc.metadata.copy()  # ì›ë³¸ ë³€ê²½ ë°©ì§€
                doc.metadata['tfidf_score'] = float(similarities[idx])
                doc.metadata['boosted_score'] = float(boosted_similarities[idx])
                results.append(doc)
            
            return results
            
        except Exception as e:
            logger.error(f"TF-IDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _apply_keyword_boost(self, query: str, similarities: np.ndarray) -> np.ndarray:
        """ë³µì§€ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©"""
        boosted_similarities = similarities.copy()
        
        # ì¿¼ë¦¬ì—ì„œ ë³µì§€ í‚¤ì›Œë“œ ì°¾ê¸°
        query_keywords = [kw for kw in self.welfare_keywords.keys() if kw in query]
        
        if not query_keywords:
            return boosted_similarities
        
        # ê° ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸í•˜ì—¬ ê°€ì¤‘ì¹˜ ì ìš©
        for i, doc in enumerate(self.documents):
            boost_factor = 1.0
            
            for keyword in query_keywords:
                if keyword in doc.page_content:
                    boost_factor *= self.welfare_keywords[keyword]
            
            if boost_factor > 1.0:
                boosted_similarities[i] *= min(boost_factor, 3.0)  # ìµœëŒ€ 3ë°°ê¹Œì§€ ì¦í­
        
        return boosted_similarities
    
    def _calculate_keyword_boost(self, query: str, documents: List[Document]) -> List[float]:
        """BM25 ê²°ê³¼ì— ëŒ€í•œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        scores = []
        query_keywords = [kw for kw in self.welfare_keywords.keys() if kw in query]
        
        for doc in documents:
            boost_factor = 1.0
            
            for keyword in query_keywords:
                if keyword in doc.page_content:
                    boost_factor *= self.welfare_keywords[keyword]
            
            scores.append(min(boost_factor, 3.0))
        
        return scores


class SemanticRetriever(BaseWelfareRetriever):
    """ì˜ë¯¸ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ (ì„ë² ë”© ë²¡í„° ìœ ì‚¬ë„)"""
    
    def __init__(self, embedding_function, vector_store=None):
        super().__init__("semantic_retriever")
        self.embedding_function = embedding_function
        self.vector_store = vector_store
        self.document_embeddings = None
    
    def add_documents(self, documents: List[Document]) -> None:
        """ë¬¸ì„œ ì¶”ê°€ ë° ì„ë² ë”© ìƒì„±"""
        self.documents = documents
        
        # ë²¡í„°ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ ì„ë² ë”©
        if self.vector_store:
            # ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
            texts = [doc.page_content for doc in documents]
            metadatas = [doc.metadata for doc in documents]
            self.vector_store.add_documents(texts, metadatas)
            logger.info(f"ë²¡í„°ìŠ¤í† ì–´ì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€")
        else:
            # ì§ì ‘ ì„ë² ë”© ìƒì„±
            try:
                texts = [doc.page_content for doc in documents]
                
                if hasattr(self.embedding_function, 'embed_documents'):
                    # LangChain ìŠ¤íƒ€ì¼
                    self.document_embeddings = self.embedding_function.embed_documents(texts)
                else:
                    # ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜
                    self.document_embeddings = [
                        self.embedding_function.embed_texts([text])[0] 
                        for text in texts
                    ]
                
                self.document_embeddings = np.array(self.document_embeddings)
                logger.info(f"ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.document_embeddings.shape}")
                
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                raise
        
        self.is_initialized = True
    
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.is_initialized:
            logger.error("ë¦¬íŠ¸ë¦¬ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        if self.vector_store:
            # ë²¡í„°ìŠ¤í† ì–´ ì‚¬ìš©
            return self._vector_store_search(query, k)
        else:
            # ì§ì ‘ ì„ë² ë”© ë¹„êµ
            return self._embedding_search(query, k)
    
    def _vector_store_search(self, query: str, k: int) -> List[Document]:
        """ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ìš©í•œ ê²€ìƒ‰"""
        try:
            results = self.vector_store.similarity_search(query, k)
            
            # Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for result in results:
                doc = Document(
                    page_content=result['document'],
                    metadata={
                        **result['metadata'],
                        'retriever_type': 'semantic_vectorstore',
                        'similarity_score': result['similarity_score']
                    }
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _embedding_search(self, query: str, k: int) -> List[Document]:
        """ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            if hasattr(self.embedding_function, 'embed_query'):
                query_embedding = self.embedding_function.embed_query(query)
            else:
                query_embedding = self.embedding_function.embed_texts([query])[0]
            
            query_embedding = np.array(query_embedding).reshape(1, -1)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_embedding, self.document_embeddings)[0]
            
            # ì˜ë¯¸ì  ìœ ì‚¬ë„ í–¥ìƒì„ ìœ„í•œ í›„ì²˜ë¦¬
            enhanced_similarities = self._enhance_semantic_similarity(query, similarities)
            
            # ìƒìœ„ kê°œ ì„ íƒ
            top_indices = np.argsort(enhanced_similarities)[::-1][:k]
            
            results = []
            for idx in top_indices:
                doc = self.documents[idx]
                doc.metadata = doc.metadata.copy()
                doc.metadata.update({
                    'retriever_type': 'semantic_embedding',
                    'similarity_score': float(similarities[idx]),
                    'enhanced_score': float(enhanced_similarities[idx])
                })
                results.append(doc)
            
            return results
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _enhance_semantic_similarity(self, query: str, similarities: np.ndarray) -> np.ndarray:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ í–¥ìƒ (ë³µì§€ ì •ì±… íŠ¹í™”)"""
        enhanced = similarities.copy()
        
        # ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ ì˜ë¯¸ì  ê´€ë ¨ì„± ë¶„ì„
        query_lower = query.lower()
        
        # ì˜ë£Œ ê´€ë ¨ ì§ˆì˜ ì²˜ë¦¬
        if any(word in query_lower for word in ['ì˜ë£Œ', 'ì¹˜ë£Œ', 'ë³‘ì›', 'ì•½', 'ìˆ˜ìˆ ']):
            for i, doc in enumerate(self.documents):
                if any(word in doc.page_content for word in ['ì˜ë£Œë¹„', 'ì§„ë£Œ', 'ì¹˜ë£Œë¹„', 'ì•½ê°’']):
                    enhanced[i] *= 1.3
        
        # ëŒë´„ ê´€ë ¨ ì§ˆì˜ ì²˜ë¦¬
        if any(word in query_lower for word in ['ëŒë´„', 'ê°„ë³‘', 'ìš”ì–‘', 'ì¼€ì–´']):
            for i, doc in enumerate(self.documents):
                if any(word in doc.page_content for word in ['ëŒë´„ì„œë¹„ìŠ¤', 'ìš”ì–‘ë³´í˜¸ì‚¬', 'ê°„ë³‘ì¸']):
                    enhanced[i] *= 1.3
        
        # ê²½ì œì  ì§€ì› ê´€ë ¨ ì§ˆì˜ ì²˜ë¦¬
        if any(word in query_lower for word in ['ì§€ì›ê¸ˆ', 'ìˆ˜ë‹¹', 'ë³´ì¡°ê¸ˆ', 'ìƒí™œë¹„']):
            for i, doc in enumerate(self.documents):
                if any(word in doc.page_content for word in ['ì§€ì›', 'ìˆ˜ë‹¹', 'ê¸‰ì—¬', 'ë³´ì¡°ê¸ˆ']):
                    enhanced[i] *= 1.2
        
        return enhanced


class HybridRetriever(BaseWelfareRetriever):
    """í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ (í‚¤ì›Œë“œ + ì˜ë¯¸ ê¸°ë°˜ ê²°í•©)"""
    
    def __init__(self, 
                 keyword_retriever: KeywordRetriever,
                 semantic_retriever: SemanticRetriever,
                 keyword_weight: float = 0.3,
                 semantic_weight: float = 0.7):
        super().__init__("hybrid_retriever")
        self.keyword_retriever = keyword_retriever
        self.semantic_retriever = semantic_retriever
        self.keyword_weight = keyword_weight
        self.semantic_weight = semantic_weight
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = keyword_weight + semantic_weight
        self.keyword_weight /= total_weight
        self.semantic_weight /= total_weight
    
    def add_documents(self, documents: List[Document]) -> None:
        """ë‘ ë¦¬íŠ¸ë¦¬ë²„ì— ë¬¸ì„œ ì¶”ê°€"""
        self.documents = documents
        
        self.keyword_retriever.add_documents(documents)
        self.semantic_retriever.add_documents(documents)
        
        self.is_initialized = True
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
    
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì˜ë¯¸ ê¸°ë°˜ ê²°í•©)"""
        if not self.is_initialized:
            logger.error("ë¦¬íŠ¸ë¦¬ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ê° ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        expanded_k = min(k * 2, len(self.documents))
        
        keyword_results = self.keyword_retriever.retrieve(query, expanded_k)
        semantic_results = self.semantic_retriever.retrieve(query, expanded_k)
        
        # ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        combined_scores = self._combine_results(keyword_results, semantic_results, query)
        
        # ìƒìœ„ kê°œ ì„ íƒ
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        
        final_results = []
        for doc_content, score in sorted_results[:k]:
            # ì›ë³¸ ë¬¸ì„œ ì°¾ê¸°
            for doc in self.documents:
                if doc.page_content == doc_content:
                    result_doc = Document(
                        page_content=doc.page_content,
                        metadata={
                            **doc.metadata,
                            'retriever_type': 'hybrid',
                            'combined_score': score,
                            'keyword_weight': self.keyword_weight,
                            'semantic_weight': self.semantic_weight
                        }
                    )
                    final_results.append(result_doc)
                    break
        
        return final_results
    
    def _combine_results(self, 
                        keyword_results: List[Document], 
                        semantic_results: List[Document],
                        query: str) -> Dict[str, float]:
        """í‚¤ì›Œë“œì™€ ì˜ë¯¸ ê¸°ë°˜ ê²°ê³¼ ê²°í•©"""
        combined_scores = {}
        
        # í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ ì²˜ë¦¬
        for doc in keyword_results:
            content = doc.page_content
            
            # í‚¤ì›Œë“œ ì ìˆ˜ ì¶”ì¶œ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ í‚¤ í™•ì¸)
            keyword_score = 0.0
            for key in ['boosted_score', 'score', 'tfidf_score']:
                if key in doc.metadata:
                    keyword_score = float(doc.metadata[key])
                    break
            
            combined_scores[content] = keyword_score * self.keyword_weight
        
        # ì˜ë¯¸ ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ ì²˜ë¦¬
        for doc in semantic_results:
            content = doc.page_content
            
            # ì˜ë¯¸ ì ìˆ˜ ì¶”ì¶œ
            semantic_score = 0.0
            for key in ['enhanced_score', 'similarity_score', 'score']:
                if key in doc.metadata:
                    semantic_score = float(doc.metadata[key])
                    break
            
            if content in combined_scores:
                combined_scores[content] += semantic_score * self.semantic_weight
            else:
                combined_scores[content] = semantic_score * self.semantic_weight
        
        return combined_scores


class RetrieverComparator:
    """ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ ë° í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.retrievers = {}
        self.test_queries = [
            "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì› ë°©ë²•",
            "ì¹˜ë§¤ ì–´ë¥´ì‹  ëŒë´„ ì„œë¹„ìŠ¤",
            "ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì ìƒí™œë¹„ ì§€ì›",
            "ë…¸ì¸ ìš”ì–‘ë³´í˜¸ì‚¬ ì‹ ì²­ ì ˆì°¨",
            "ê³ ë ¹ì êµí†µë¹„ í• ì¸ í˜œíƒ"
        ]
    
    def register_retriever(self, name: str, retriever: BaseWelfareRetriever):
        """ë¦¬íŠ¸ë¦¬ë²„ ë“±ë¡"""
        self.retrievers[name] = retriever
        logger.info(f"ë¦¬íŠ¸ë¦¬ë²„ ë“±ë¡: {name}")
    
    def compare_retrievers(self, 
                          test_documents: List[Document],
                          custom_queries: List[str] = None) -> pd.DataFrame:
        """ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ"""
        
        queries = custom_queries or self.test_queries
        results = []
        
        for name, retriever in self.retrievers.items():
            logger.info(f"ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸: {name}")
            
            try:
                # ë¬¸ì„œ ì¶”ê°€
                retriever.add_documents(test_documents)
                
                # ê° ì¿¼ë¦¬ì— ëŒ€í•œ ì„±ëŠ¥ ì¸¡ì •
                query_results = []
                
                for query in queries:
                    retrieved_docs = retriever.retrieve(query, k=5)
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜)
                    relevance_score = self._calculate_relevance(query, retrieved_docs)
                    query_results.append(relevance_score)
                
                # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                avg_relevance = np.mean(query_results)
                
                result = {
                    'retriever': name,
                    'avg_relevance': round(avg_relevance, 3),
                    'document_count': len(test_documents),
                    'test_queries': len(queries),
                    'retriever_info': retriever.get_retriever_info()
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"{name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        df = pd.DataFrame(results)
        return df.sort_values('avg_relevance', ascending=False) if not df.empty else df
    
    def _calculate_relevance(self, query: str, retrieved_docs: List[Document]) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ë‹¨ìˆœ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        if not retrieved_docs:
            return 0.0
        
        query_words = set(query.lower().split())
        relevance_scores = []
        
        for doc in retrieved_docs:
            doc_words = set(doc.page_content.lower().split())
            
            # ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°
            intersection = len(query_words.intersection(doc_words))
            union = len(query_words.union(doc_words))
            
            jaccard_score = intersection / union if union > 0 else 0
            relevance_scores.append(jaccard_score)
        
        return np.mean(relevance_scores)
    
    def benchmark_speed(self, 
                       test_documents: List[Document],
                       num_queries: int = 10) -> pd.DataFrame:
        """ë¦¬íŠ¸ë¦¬ë²„ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        
        import time
        
        results = []
        test_query = "ë…¸ì¸ ë³µì§€ ì§€ì› ì œë„"
        
        for name, retriever in self.retrievers.items():
            try:
                # ë¬¸ì„œ ì¶”ê°€ ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                retriever.add_documents(test_documents)
                indexing_time = time.time() - start_time
                
                # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
                search_times = []
                for _ in range(num_queries):
                    start_time = time.time()
                    retriever.retrieve(test_query, k=5)
                    search_times.append(time.time() - start_time)
                
                avg_search_time = np.mean(search_times)
                
                result = {
                    'retriever': name,
                    'indexing_time': round(indexing_time, 3),
                    'avg_search_time': round(avg_search_time, 4),
                    'queries_per_second': round(1 / avg_search_time, 2),
                    'documents_indexed': len(test_documents)
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"{name} ì†ë„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        return pd.DataFrame(results)


def main():
    """ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ” ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
    test_docs = [
        Document(
            page_content="ë§Œ 65ì„¸ ì´ìƒ ë…¸ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ì˜ë£Œë¹„ë¥¼ ì›” ìµœëŒ€ 30ë§Œì›ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
            metadata={"category": "ì˜ë£Œì§€ì›", "region": "ì „êµ­"}
        ),
        Document(
            page_content="ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì ì–´ë¥´ì‹ ì—ê²Œ ìƒí™œë¹„ë¥¼ ì›” 50ë§Œì›ì”© ì§€ì›í•˜ëŠ” ì œë„ì…ë‹ˆë‹¤.",
            metadata={"category": "ìƒí™œì§€ì›", "region": "ì„œìš¸"}
        ),
        Document(
            page_content="ìš”ì–‘ë³´í˜¸ì‚¬ê°€ ì£¼ 3íšŒ ë°©ë¬¸í•˜ì—¬ ëŒë´„ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            metadata={"category": "ëŒë´„ì„œë¹„ìŠ¤", "region": "ë¶€ì‚°"}
        ),
        Document(
            page_content="ì¹˜ë§¤ ì–´ë¥´ì‹ ì„ ìœ„í•œ ì¸ì§€ê¸°ëŠ¥ í–¥ìƒ í”„ë¡œê·¸ë¨ì„ ìš´ì˜í•©ë‹ˆë‹¤.",
            metadata={"category": "ì˜ë£Œì§€ì›", "region": "ëŒ€êµ¬"}
        ),
        Document(
            page_content="ì €ì†Œë“ ë…¸ì¸ì„ ìœ„í•œ êµí†µë¹„ í• ì¸ í˜œíƒì„ ì œê³µí•©ë‹ˆë‹¤.",
            metadata={"category": "êµí†µì§€ì›", "region": "ì „êµ­"}
        )
    ]
    
    # í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸
    print("ğŸ“ í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸...")
    try:
        keyword_retriever = KeywordRetriever(use_bm25=False)  # BM25 ì—†ì´ í…ŒìŠ¤íŠ¸
        keyword_retriever.add_documents(test_docs)
        
        query = "ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì›"
        results = keyword_retriever.retrieve(query, k=3)
        
        print(f"ì¿¼ë¦¬: '{query}'")
        for i, doc in enumerate(results, 1):
            score = doc.metadata.get('boosted_score', 0)
            print(f"  [{i}] ì ìˆ˜: {score:.3f} - {doc.page_content[:50]}...")
        
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ì„±ëŠ¥ ë¹„êµ (ê°„ë‹¨ ë²„ì „)
    print(f"\nğŸ“Š ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ:")
    comparator = RetrieverComparator()
    
    try:
        # í‚¤ì›Œë“œ ë¦¬íŠ¸ë¦¬ë²„ë§Œ ë“±ë¡ (ì˜ë¯¸ ê¸°ë°˜ì€ ì„ë² ë”© í•¨ìˆ˜ í•„ìš”)
        comparator.register_retriever("keyword", KeywordRetriever(use_bm25=False))
        
        # ì„±ëŠ¥ ë¹„êµ
        performance_df = comparator.compare_retrievers(test_docs)
        
        if not performance_df.empty:
            print(performance_df.to_string(index=False))
        else:
            print("ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì—†ìŒ")
            
    except Exception as e:
        print(f"ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {e}")
    
    print(f"\nâœ… ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()