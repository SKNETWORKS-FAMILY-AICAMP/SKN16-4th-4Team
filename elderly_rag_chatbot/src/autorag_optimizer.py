# =========================================
# ğŸ¤– AutoRAG ìŠ¤íƒ€ì¼ í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ
# =========================================

import os
import json
import time
import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime
import pandas as pd

logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤ (ìƒëŒ€ import ì²˜ë¦¬)
try:
    from .text_extraction_comparison import TextExtractionComparison
    from .chunk_strategy import ChunkStrategyComparison  
    from .embedding_models import EmbeddingModelComparison
    from .vector_store import WelfareVectorStore
    from .retriever import RetrieverComparison
    from .rag_system import ElderlyWelfareRAGChain, MultiRAGSystem, RAGEvaluator
    from .langgraph_workflow import ElderlyWelfareWorkflow
    from .chatbot_interface import ElderlyWelfareChatbot
except ImportError:
    # ì ˆëŒ€ importë¡œ ì¬ì‹œë„
    try:
        from src.text_extraction_comparison import TextExtractionComparison
        from src.chunk_strategy import ChunkStrategyComparison
        from src.embedding_models import EmbeddingModelComparison
        from src.vector_store import WelfareVectorStore
        from src.retriever import RetrieverComparison
        from src.rag_system import ElderlyWelfareRAGChain, MultiRAGSystem, RAGEvaluator
        from src.langgraph_workflow import ElderlyWelfareWorkflow
        from src.chatbot_interface import ElderlyWelfareChatbot
    except ImportError as e:
        logger.error(f"í”„ë¡œì íŠ¸ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ í´ë˜ìŠ¤ë“¤ë¡œ ëŒ€ì²´
        TextExtractionComparison = None
        ChunkStrategyComparison = None
        EmbeddingModelComparison = None
        WelfareVectorStore = None
        RetrieverComparison = None
        ElderlyWelfareRAGChain = None
        MultiRAGSystem = None
        RAGEvaluator = None
        ElderlyWelfareWorkflow = None
        ElderlyWelfareChatbot = None

logger = logging.getLogger(__name__)


@dataclass
class ComponentSelection:
    """ê° ì»´í¬ë„ŒíŠ¸ ì„ íƒ ê²°ê³¼"""
    component_name: str
    selected_option: str
    score: float
    reason: str
    metrics: Dict[str, Any]
    
    
@dataclass
class AutoRAGConfig:
    """AutoRAG ì‹œìŠ¤í…œ ì„¤ì •"""
    
    # í‰ê°€ ë°ì´í„°
    evaluation_queries: List[str]
    evaluation_documents: List[str]
    
    # í‰ê°€ ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜
    performance_weight: float = 0.3
    quality_weight: float = 0.4
    speed_weight: float = 0.2
    resource_weight: float = 0.1
    
    # ìë™í™” ë ˆë²¨
    automation_level: str = "full"  # "manual", "semi", "full"
    
    # ìµœì í™” ëª©í‘œ
    optimization_target: str = "balanced"  # "speed", "quality", "balanced"
    
    # ì‹¤í—˜ ì„¤ì •
    max_experiments: int = 50
    early_stopping_patience: int = 5
    
    # ê²°ê³¼ ì €ì¥
    save_intermediate_results: bool = True
    results_directory: str = "./autorag_results"


class AutoRAGOptimizer:
    """AutoRAG ìë™ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: AutoRAGConfig, data_directory: str = "./data/ë³µì§€ë¡œ"):
        """AutoRAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        self.config = config
        self.data_directory = data_directory
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        os.makedirs(config.results_directory, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ë¹„êµ ì‹œìŠ¤í…œë“¤
        self.text_extraction_comparison = None
        self.chunk_comparison = None
        self.embedding_comparison = None
        self.retriever_comparison = None
        
        # ì‹¤í—˜ ê²°ê³¼
        self.experiment_results = []
        self.best_pipeline = None
        
        # ì§„í–‰ ìƒí™©
        self.current_step = 0
        self.total_steps = 7  # ì´ ìµœì í™” ë‹¨ê³„ ìˆ˜
        
        logger.info("AutoRAG ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” í™•ì¸
        self._validate_components()
    
    def _validate_components(self):
        """í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
        
        if TextExtractionComparison is None:
            logger.warning("TextExtractionComparisonì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if ChunkStrategyComparison is None:
            logger.warning("ChunkStrategyComparisonì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if EmbeddingModelComparison is None:
            logger.warning("EmbeddingModelComparisonì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    async def optimize_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ìë™ ìµœì í™”"""
        
        logger.info("ğŸ¤– AutoRAG íŒŒì´í”„ë¼ì¸ ìµœì í™” ì‹œì‘")
        
        optimization_start = time.time()
        
        try:
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ìµœì í™”
            self.current_step = 1
            extraction_result = await self._optimize_text_extraction()
            
            # 2. ì²­í‚¹ ì „ëµ ìµœì í™”
            self.current_step = 2
            chunking_result = await self._optimize_chunking_strategy(extraction_result)
            
            # 3. ì„ë² ë”© ëª¨ë¸ ìµœì í™”
            self.current_step = 3
            embedding_result = await self._optimize_embedding_model(chunking_result)
            
            # 4. ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±
            self.current_step = 4
            vector_store_result = await self._setup_vector_store(embedding_result)
            
            # 5. ê²€ìƒ‰ê¸° ìµœì í™”
            self.current_step = 5
            retriever_result = await self._optimize_retriever(vector_store_result)
            
            # 6. RAG ì‹œìŠ¤í…œ ìµœì í™”
            self.current_step = 6
            rag_result = await self._optimize_rag_system(retriever_result)
            
            # 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ í‰ê°€
            self.current_step = 7
            final_result = await self._evaluate_final_pipeline(rag_result)
            
            optimization_time = time.time() - optimization_start
            
            # ìµœì í™” ì™„ë£Œ
            self.best_pipeline = final_result
            
            # ê²°ê³¼ ì €ì¥
            await self._save_optimization_results(final_result, optimization_time)
            
            logger.info(f"âœ… AutoRAG íŒŒì´í”„ë¼ì¸ ìµœì í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {optimization_time:.2f}ì´ˆ)")
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ AutoRAG ìµœì í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _optimize_text_extraction(self) -> ComponentSelection:
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ìµœì í™”"""
        
        logger.info("ğŸ“„ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ìµœì í™”")
        
        self.text_extraction_comparison = TextExtractionComparison()
        
        # ìƒ˜í”Œ íŒŒì¼ë“¤ ìˆ˜ì§‘
        pdf_files = list(Path(self.data_directory).rglob("*.pdf"))[:5]
        hwp_files = list(Path(self.data_directory).rglob("*.hwp"))[:5]
        
        # PDF ì¶”ì¶œê¸° ë¹„êµ
        pdf_results = {}
        if pdf_files:
            pdf_comparison = self.text_extraction_comparison.compare_pdf_extractors(
                [str(f) for f in pdf_files]
            )
            if "error" not in pdf_comparison:
                pdf_results = pdf_comparison
        
        # HWP ì¶”ì¶œê¸° ë¹„êµ
        hwp_results = {}
        if hwp_files:
            hwp_comparison = self.text_extraction_comparison.compare_hwp_extractors(
                [str(f) for f in hwp_files]
            )
            if "error" not in hwp_comparison:
                hwp_results = hwp_comparison
        
        # ìµœì  ì¶”ì¶œê¸° ì„ íƒ
        best_extractors = {}
        total_score = 0
        
        if pdf_results and "best_extractor" in pdf_results:
            best_extractors["pdf"] = pdf_results["best_extractor"]
            total_score += pdf_results["best_extractor"]["score"]
        
        if hwp_results and "best_extractor" in hwp_results:
            best_extractors["hwp"] = hwp_results["best_extractor"]
            total_score += hwp_results["best_extractor"]["score"]
        
        avg_score = total_score / len(best_extractors) if best_extractors else 0.5
        
        selection = ComponentSelection(
            component_name="text_extraction",
            selected_option=json.dumps(best_extractors),
            score=avg_score,
            reason="ë‹¤ì–‘í•œ ì¶”ì¶œê¸° ì¤‘ ì„±ê³µë¥ ê³¼ í…ìŠ¤íŠ¸ í’ˆì§ˆì´ ê°€ì¥ ë†’ì€ ì¡°í•© ì„ íƒ",
            metrics={
                "pdf_results": pdf_results,
                "hwp_results": hwp_results,
                "best_extractors": best_extractors
            }
        )
        
        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ìµœì í™” ì™„ë£Œ (ì ìˆ˜: {avg_score:.3f})")
        return selection
    
    async def _optimize_chunking_strategy(self, extraction_result: ComponentSelection) -> ComponentSelection:
        """ì²­í‚¹ ì „ëµ ìµœì í™”"""
        
        logger.info("âœ‚ï¸ 2ë‹¨ê³„: ì²­í‚¹ ì „ëµ ìµœì í™”")
        
        self.chunk_comparison = ChunkStrategyComparison()
        
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì‹¤ì œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        sample_texts = []
        
        # í‰ê°€ìš© ì¿¼ë¦¬ì—ì„œ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„± (ì‹¤ì œë¡œëŠ” ì¶”ì¶œëœ ë¬¸ì„œ ì‚¬ìš©)
        if self.config.evaluation_documents:
            sample_texts = self.config.evaluation_documents[:10]
        else:
            # ê¸°ë³¸ ìƒ˜í”Œ í…ìŠ¤íŠ¸
            sample_texts = [
                "ë…¸ì¸ë³µì§€ë²•ì— ë”°ë¥¸ ì˜ë£Œë¹„ ì§€ì› ì œë„ëŠ” 65ì„¸ ì´ìƒ ë…¸ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤. " * 20,
                "ì¥ê¸°ìš”ì–‘ë³´í—˜ ì œë„ëŠ” ì‹ ì²´ì  ë˜ëŠ” ì •ì‹ ì  ì¥ì• ë¡œ ë„ì›€ì´ í•„ìš”í•œ ë…¸ì¸ì—ê²Œ ì œê³µë©ë‹ˆë‹¤. " * 20,
                "ê¸°ì´ˆì—°ê¸ˆì€ ì†Œë“ í•˜ìœ„ 70% ë…¸ì¸ì—ê²Œ ì§€ê¸‰ë˜ëŠ” ê¸°ì´ˆìƒí™œë³´ì¥ ì œë„ì…ë‹ˆë‹¤. " * 20
            ]
        
        # ì²­í‚¹ ì „ëµ í‰ê°€
        evaluation_results = self.chunk_comparison.evaluate_strategies(
            sample_texts,
            criteria=["coherence", "coverage", "diversity", "semantic_similarity"]
        )
        
        # ìµœì  ì „ëµ ì„ íƒ
        best_strategy = max(evaluation_results.items(), key=lambda x: x[1]["overall_score"])
        
        selection = ComponentSelection(
            component_name="chunking_strategy",
            selected_option=best_strategy[0],
            score=best_strategy[1]["overall_score"],
            reason=f"ì¼ê´€ì„±, ì»¤ë²„ë¦¬ì§€, ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„",
            metrics={
                "all_strategies": evaluation_results,
                "best_strategy_details": best_strategy[1]
            }
        )
        
        logger.info(f"âœ… ì²­í‚¹ ì „ëµ ìµœì í™” ì™„ë£Œ - {best_strategy[0]} (ì ìˆ˜: {best_strategy[1]['overall_score']:.3f})")
        return selection
    
    async def _optimize_embedding_model(self, chunking_result: ComponentSelection) -> ComponentSelection:
        """ì„ë² ë”© ëª¨ë¸ ìµœì í™”"""
        
        logger.info("ğŸ”¢ 3ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ìµœì í™”")
        
        self.embedding_comparison = EmbeddingModelComparison()
        
        # í‰ê°€ìš© í…ìŠ¤íŠ¸ (ë³µì§€ ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œ)
        evaluation_texts = self.config.evaluation_queries if self.config.evaluation_queries else [
            "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì› ì œë„",
            "ì¥ê¸°ìš”ì–‘ë³´í—˜ ì‹ ì²­ ë°©ë²•ê³¼ ì ˆì°¨",
            "ê¸°ì´ˆì—°ê¸ˆ ìˆ˜ê¸‰ ìê²©ê³¼ ê¸ˆì•¡",
            "ë…ê±°ë…¸ì¸ ëŒë´„ ì„œë¹„ìŠ¤ ì•ˆë‚´",
            "ë…¸ì¸ ì£¼ê±° ì§€ì› ì •ì±…"
        ]
        
        # ì„ë² ë”© ëª¨ë¸ ë¹„êµ
        comparison_results = self.embedding_comparison.compare_models(evaluation_texts)
        
        # ìµœì  ëª¨ë¸ ì„ íƒ (ì„±ëŠ¥, ì†ë„, í’ˆì§ˆ ì¢…í•© ê³ ë ¤)
        best_model_name = None
        best_score = 0
        
        for model_name, results in comparison_results.items():
            if results["success"]:
                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                quality_score = results["avg_similarity"]
                speed_score = 1 / (1 + results["avg_time"])  # ë¹ ë¥¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
                
                combined_score = (
                    quality_score * self.config.quality_weight +
                    speed_score * self.config.speed_weight +
                    0.8 * self.config.performance_weight  # ê¸°ë³¸ ì„±ëŠ¥ ì ìˆ˜
                )
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_model_name = model_name
        
        if not best_model_name:
            best_model_name = "sentence_transformer"  # ê¸°ë³¸ê°’
            best_score = 0.7
        
        selection = ComponentSelection(
            component_name="embedding_model",
            selected_option=best_model_name,
            score=best_score,
            reason=f"í’ˆì§ˆê³¼ ì†ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ìµœì  ëª¨ë¸",
            metrics={
                "comparison_results": comparison_results,
                "optimization_target": self.config.optimization_target
            }
        )
        
        logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ìµœì í™” ì™„ë£Œ - {best_model_name} (ì ìˆ˜: {best_score:.3f})")
        return selection
    
    async def _setup_vector_store(self, embedding_result: ComponentSelection) -> ComponentSelection:
        """ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±"""
        
        logger.info("ğŸ—„ï¸ 4ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±")
        
        # ì„ íƒëœ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        embedding_model_name = embedding_result.selected_option
        
        # ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        embedding_model = self.embedding_comparison.get_model(embedding_model_name)
        
        # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vector_store = WelfareVectorStore(
            persist_directory=os.path.join(self.config.results_directory, "vector_store"),
            collection_name="autorag_optimized",
            embedding_model=embedding_model
        )
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ê°„ë‹¨í•œ ì¶”ì •)
        setup_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
        
        selection = ComponentSelection(
            component_name="vector_store",
            selected_option="chromadb_with_optimized_embedding",
            score=setup_score,
            reason="ì„ íƒëœ ì„ë² ë”© ëª¨ë¸ê³¼ í•¨ê»˜ ChromaDB ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±",
            metrics={
                "embedding_model": embedding_model_name,
                "persist_directory": vector_store.persist_directory,
                "collection_name": vector_store.collection_name
            }
        )
        
        # ë²¡í„° ì €ì¥ì†Œë¥¼ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        self.vector_store = vector_store
        
        logger.info(f"âœ… ë²¡í„° ì €ì¥ì†Œ êµ¬ì„± ì™„ë£Œ (ì ìˆ˜: {setup_score:.3f})")
        return selection
    
    async def _optimize_retriever(self, vector_store_result: ComponentSelection) -> ComponentSelection:
        """ê²€ìƒ‰ê¸° ìµœì í™”"""
        
        logger.info("ğŸ” 5ë‹¨ê³„: ê²€ìƒ‰ê¸° ìµœì í™”")
        
        # ê²€ìƒ‰ê¸° ë¹„êµ ì‹œìŠ¤í…œ ìƒì„±
        self.retriever_comparison = RetrieverComparison(
            vector_store=self.vector_store,
            top_k=5
        )
        
        # í‰ê°€ìš© ì¿¼ë¦¬
        evaluation_queries = self.config.evaluation_queries if self.config.evaluation_queries else [
            "65ì„¸ ì´ìƒ ì˜ë£Œë¹„ ì§€ì› ë°©ë²•",
            "ë…¸ì¸ ëŒë´„ ì„œë¹„ìŠ¤ ì‹ ì²­ ì ˆì°¨",
            "ê¸°ì´ˆì—°ê¸ˆ ìˆ˜ê¸‰ ì¡°ê±´"
        ]
        
        # ê²€ìƒ‰ê¸° ì„±ëŠ¥ í‰ê°€
        retriever_evaluation = self.retriever_comparison.evaluate_retrievers(evaluation_queries)
        
        # ìµœì  ê²€ìƒ‰ê¸° ì„ íƒ
        best_retriever = max(retriever_evaluation.items(), key=lambda x: x[1]["overall_score"])
        
        selection = ComponentSelection(
            component_name="retriever",
            selected_option=best_retriever[0],
            score=best_retriever[1]["overall_score"],
            reason="ì •í™•ë„ì™€ ì¬í˜„ìœ¨ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ìµœì  ê²€ìƒ‰ ë°©ì‹",
            metrics={
                "all_retrievers": retriever_evaluation,
                "best_retriever_details": best_retriever[1]
            }
        )
        
        logger.info(f"âœ… ê²€ìƒ‰ê¸° ìµœì í™” ì™„ë£Œ - {best_retriever[0]} (ì ìˆ˜: {best_retriever[1]['overall_score']:.3f})")
        return selection
    
    async def _optimize_rag_system(self, retriever_result: ComponentSelection) -> ComponentSelection:
        """RAG ì‹œìŠ¤í…œ ìµœì í™”"""
        
        logger.info("ğŸ¤– 6ë‹¨ê³„: RAG ì‹œìŠ¤í…œ ìµœì í™”")
        
        # ìµœì  ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        best_retriever_name = retriever_result.selected_option
        best_retriever = self.retriever_comparison.get_retriever(best_retriever_name)
        
        # RAG ì²´ì¸ ìƒì„±
        rag_chain = ElderlyWelfareRAGChain(
            retriever=best_retriever,
            llm_model="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=1000
        )
        
        # RAG ì‹œìŠ¤í…œ í‰ê°€
        evaluator = RAGEvaluator()
        
        evaluation_results = []
        for query in (self.config.evaluation_queries or ["ê¸°ì´ˆì—°ê¸ˆì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"]):
            try:
                result = rag_chain.ask(query)
                if result["success"]:
                    # ê°„ë‹¨í•œ í‰ê°€ ë©”íŠ¸ë¦­
                    relevance_score = 0.8  # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ í•„ìš”
                    evaluation_results.append({
                        "query": query,
                        "success": True,
                        "relevance": relevance_score
                    })
            except Exception as e:
                logger.warning(f"RAG í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                evaluation_results.append({
                    "query": query,
                    "success": False,
                    "relevance": 0.0
                })
        
        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        avg_score = sum(r["relevance"] for r in evaluation_results) / len(evaluation_results) if evaluation_results else 0.5
        
        selection = ComponentSelection(
            component_name="rag_system",
            selected_option="elderly_welfare_rag_chain",
            score=avg_score,
            reason="ìµœì í™”ëœ ê²€ìƒ‰ê¸°ì™€ LLMì„ ê²°í•©í•œ RAG ì‹œìŠ¤í…œ",
            metrics={
                "evaluation_results": evaluation_results,
                "retriever": best_retriever_name,
                "llm_model": "gpt-3.5-turbo"
            }
        )
        
        # RAG ì‹œìŠ¤í…œì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        self.rag_system = rag_chain
        
        logger.info(f"âœ… RAG ì‹œìŠ¤í…œ ìµœì í™” ì™„ë£Œ (ì ìˆ˜: {avg_score:.3f})")
        return selection
    
    async def _evaluate_final_pipeline(self, rag_result: ComponentSelection) -> Dict[str, Any]:
        """ìµœì¢… íŒŒì´í”„ë¼ì¸ í‰ê°€"""
        
        logger.info("ğŸ† 7ë‹¨ê³„: ìµœì¢… íŒŒì´í”„ë¼ì¸ í‰ê°€")
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
        pipeline_components = {
            step.component_name: step for step in [
                result for result in self.experiment_results
            ] + [rag_result]
        }
        
        # ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        component_scores = [comp.score for comp in pipeline_components.values()]
        overall_score = sum(component_scores) / len(component_scores)
        
        # ì›Œí¬í”Œë¡œìš° ì‹œìŠ¤í…œ ìƒì„±
        workflow = ElderlyWelfareWorkflow(
            rag_chain=self.rag_system,
            enable_intent_classification=True,
            enable_relevance_evaluation=True
        )
        
        # ìµœì¢… ì±—ë´‡ ì‹œìŠ¤í…œ ìƒì„±
        chatbot = ElderlyWelfareChatbot(
            rag_system=self.rag_system,
            workflow_system=workflow
        )
        
        # ìµœì¢… í‰ê°€
        final_evaluation = []
        test_queries = self.config.evaluation_queries or [
            "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì›ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "ë…¸ì¸ì¥ê¸°ìš”ì–‘ë³´í—˜ ì‹ ì²­ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê¸°ì´ˆì—°ê¸ˆ ìˆ˜ê¸‰ ìê²©ê³¼ ì‹ ì²­ ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        ]
        
        for query in test_queries:
            try:
                start_time = time.time()
                response = chatbot.process_message(query, use_workflow=True)
                end_time = time.time()
                
                final_evaluation.append({
                    "query": query,
                    "success": response["success"],
                    "response_time": end_time - start_time,
                    "answer_length": len(response["answer"]),
                    "sources_count": len(response["sources"])
                })
            except Exception as e:
                logger.warning(f"ìµœì¢… í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                final_evaluation.append({
                    "query": query,
                    "success": False,
                    "response_time": 0,
                    "answer_length": 0,
                    "sources_count": 0
                })
        
        # ìµœì¢… ê²°ê³¼
        final_result = {
            "optimization_complete": True,
            "overall_score": overall_score,
            "pipeline_components": {name: asdict(comp) for name, comp in pipeline_components.items()},
            "final_evaluation": final_evaluation,
            "chatbot_instance": chatbot,
            "recommended_config": self._generate_recommended_config(pipeline_components),
            "optimization_summary": self._generate_optimization_summary(pipeline_components, overall_score)
        }
        
        logger.info(f"âœ… ìµœì¢… íŒŒì´í”„ë¼ì¸ í‰ê°€ ì™„ë£Œ (ì „ì²´ ì ìˆ˜: {overall_score:.3f})")
        return final_result
    
    def _generate_recommended_config(self, components: Dict[str, ComponentSelection]) -> Dict[str, Any]:
        """ì¶”ì²œ ì„¤ì • ìƒì„±"""
        
        config = {}
        
        for comp_name, comp in components.items():
            config[comp_name] = {
                "selected": comp.selected_option,
                "score": comp.score,
                "reason": comp.reason
            }
        
        return config
    
    def _generate_optimization_summary(self, components: Dict[str, ComponentSelection], overall_score: float) -> str:
        """ìµœì í™” ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        summary = [
            "ğŸ¤– AutoRAG íŒŒì´í”„ë¼ì¸ ìµœì í™” ì™„ë£Œ",
            "=" * 50,
            f"ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: {overall_score:.3f}/1.0",
            "",
            "ğŸ“‹ ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸:",
        ]
        
        for comp_name, comp in components.items():
            summary.append(f"  â€¢ {comp_name}: {comp.selected_option} (ì ìˆ˜: {comp.score:.3f})")
            summary.append(f"    - {comp.reason}")
        
        summary.extend([
            "",
            "âœ¨ ìµœì í™” íš¨ê³¼:",
            f"  â€¢ ìë™í™”ëœ ì»´í¬ë„ŒíŠ¸ ì„ íƒ: {len(components)}ê°œ",
            f"  â€¢ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ: {(overall_score - 0.5) * 100:.1f}%",
            f"  â€¢ ìµœì í™” ì‹œê°„: {time.time() - self.optimization_start:.1f}ì´ˆ" if hasattr(self, 'optimization_start') else "",
            "",
            "ğŸš€ ë‹¤ìŒ ë‹¨ê³„:",
            "  1. ìƒì„±ëœ ì„¤ì •ìœ¼ë¡œ ì±—ë´‡ ì‹œìŠ¤í…œ ë°°í¬",
            "  2. ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘",
            "  3. ì§€ì†ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§",
            "",
        ])
        
        return "\n".join(summary)
    
    async def _save_optimization_results(self, results: Dict[str, Any], optimization_time: float):
        """ìµœì í™” ê²°ê³¼ ì €ì¥"""
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        results_file = os.path.join(
            self.config.results_directory,
            f"autorag_results_{timestamp}.json"
        )
        
        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„ (chatbot_instance ì œì™¸)
        save_data = {
            "timestamp": timestamp,
            "optimization_time": optimization_time,
            "config": asdict(self.config),
            "overall_score": results["overall_score"],
            "pipeline_components": results["pipeline_components"],
            "final_evaluation": results["final_evaluation"],
            "recommended_config": results["recommended_config"],
            "optimization_summary": results["optimization_summary"]
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ìµœì í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
    
    def get_progress(self) -> Dict[str, Any]:
        """í˜„ì¬ ì§„í–‰ ìƒí™© ë°˜í™˜"""
        
        return {
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "progress_percentage": (self.current_step / self.total_steps) * 100,
            "step_names": [
                "í…ìŠ¤íŠ¸ ì¶”ì¶œ ìµœì í™”",
                "ì²­í‚¹ ì „ëµ ìµœì í™”",
                "ì„ë² ë”© ëª¨ë¸ ìµœì í™”",
                "ë²¡í„° ì €ì¥ì†Œ êµ¬ì„±",
                "ê²€ìƒ‰ê¸° ìµœì í™”",
                "RAG ì‹œìŠ¤í…œ ìµœì í™”",
                "ìµœì¢… íŒŒì´í”„ë¼ì¸ í‰ê°€"
            ],
            "completed_components": len(self.experiment_results)
        }
    
    def load_previous_results(self, results_file: str) -> Dict[str, Any]:
        """ì´ì „ ìµœì í™” ê²°ê³¼ ë¡œë“œ"""
        
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}


class AutoRAGInterface:
    """AutoRAG ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, data_directory: str = "./data/ë³µì§€ë¡œ"):
        self.data_directory = data_directory
        self.optimizer = None
        self.config = None
    
    def create_config_interactive(self) -> AutoRAGConfig:
        """ëŒ€í™”í˜• ì„¤ì • ìƒì„±"""
        
        print("ğŸ¤– AutoRAG ì„¤ì • ìƒì„±")
        print("=" * 30)
        
        # í‰ê°€ ì¿¼ë¦¬ ì…ë ¥
        print("\n1. í‰ê°€ìš© ì§ˆë¬¸ë“¤ì„ ì…ë ¥í•˜ì„¸ìš” (Enterë¡œ ì™„ë£Œ):")
        evaluation_queries = []
        while True:
            query = input("ì§ˆë¬¸: ").strip()
            if not query:
                break
            evaluation_queries.append(query)
        
        if not evaluation_queries:
            evaluation_queries = [
                "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì›ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë…¸ì¸ì¥ê¸°ìš”ì–‘ë³´í—˜ ì‹ ì²­ ë°©ë²•ì€?",
                "ê¸°ì´ˆì—°ê¸ˆ ìˆ˜ê¸‰ ìê²©ì€?"
            ]
        
        # ìµœì í™” ëª©í‘œ ì„ íƒ
        print("\n2. ìµœì í™” ëª©í‘œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("  1) speed - ë¹ ë¥¸ ì‘ë‹µ ì†ë„ ìš°ì„ ")
        print("  2) quality - ë†’ì€ ë‹µë³€ í’ˆì§ˆ ìš°ì„ ")
        print("  3) balanced - ê· í˜•ì¡íŒ ì„±ëŠ¥")
        
        target_choice = input("ì„ íƒ (1-3, ê¸°ë³¸ê°’: 3): ").strip()
        optimization_target = {"1": "speed", "2": "quality", "3": "balanced"}.get(target_choice, "balanced")
        
        # ìë™í™” ë ˆë²¨ ì„ íƒ
        print("\n3. ìë™í™” ë ˆë²¨ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("  1) manual - ê° ë‹¨ê³„ë³„ ìˆ˜ë™ í™•ì¸")
        print("  2) semi - ì¼ë¶€ ë‹¨ê³„ ìë™í™”")
        print("  3) full - ì™„ì „ ìë™í™”")
        
        auto_choice = input("ì„ íƒ (1-3, ê¸°ë³¸ê°’: 3): ").strip()
        automation_level = {"1": "manual", "2": "semi", "3": "full"}.get(auto_choice, "full")
        
        # ì„¤ì • ìƒì„±
        config = AutoRAGConfig(
            evaluation_queries=evaluation_queries,
            evaluation_documents=[],
            optimization_target=optimization_target,
            automation_level=automation_level,
            results_directory=f"./autorag_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        return config
    
    async def run_optimization(self, config: AutoRAGConfig = None) -> Dict[str, Any]:
        """ìµœì í™” ì‹¤í–‰"""
        
        if config is None:
            config = self.create_config_interactive()
        
        self.config = config
        self.optimizer = AutoRAGOptimizer(config, self.data_directory)
        
        print(f"\nğŸš€ AutoRAG ìµœì í™” ì‹œì‘")
        print(f"ìµœì í™” ëª©í‘œ: {config.optimization_target}")
        print(f"ìë™í™” ë ˆë²¨: {config.automation_level}")
        print(f"í‰ê°€ ì¿¼ë¦¬ ìˆ˜: {len(config.evaluation_queries)}")
        
        # ìµœì í™” ì‹¤í–‰
        results = await self.optimizer.optimize_pipeline()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + results["optimization_summary"])
        
        return results
    
    def get_optimized_chatbot(self):
        """ìµœì í™”ëœ ì±—ë´‡ ë°˜í™˜"""
        
        if self.optimizer and self.optimizer.best_pipeline:
            return self.optimizer.best_pipeline.get("chatbot_instance")
        return None


async def main():
    """AutoRAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ¤– AutoRAG í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # AutoRAG ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    autorag = AutoRAGInterface()
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ìƒì„±
    test_config = AutoRAGConfig(
        evaluation_queries=[
            "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì› ì œë„ëŠ”?",
            "ë…¸ì¸ì¥ê¸°ìš”ì–‘ë³´í—˜ ì‹ ì²­ ë°©ë²•ì€?",
            "ê¸°ì´ˆì—°ê¸ˆ ìˆ˜ê¸‰ ìê²©ì€?"
        ],
        evaluation_documents=[],
        optimization_target="balanced",
        automation_level="full"
    )
    
    try:
        # ìµœì í™” ì‹¤í–‰
        results = await autorag.run_optimization(test_config)
        
        # ìµœì í™”ëœ ì±—ë´‡ í…ŒìŠ¤íŠ¸
        optimized_chatbot = autorag.get_optimized_chatbot()
        if optimized_chatbot:
            print("\nğŸ¤– ìµœì í™”ëœ ì±—ë´‡ í…ŒìŠ¤íŠ¸:")
            test_response = optimized_chatbot.process_message("ë…¸ì¸ë³µì§€ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
            print(f"ë‹µë³€: {test_response['answer'][:200]}...")
        
        print(f"\nâœ… AutoRAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ AutoRAG í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())