# =========================================
# ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹œìŠ¤í…œ
# =========================================

import os
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from abc import ABC, abstractmethod
import hashlib
import re

# PDF ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False

try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

try:
    import pdfminer
    from pdfminer.high_level import extract_text as pdfminer_extract_text
    PDFMINER_AVAILABLE = True
except ImportError:
    PDFMINER_AVAILABLE = False

# HWP ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import olefile
    OLEFILE_AVAILABLE = True
except ImportError:
    OLEFILE_AVAILABLE = False

try:
    import hwp5
    HWP5_AVAILABLE = True
except ImportError:
    HWP5_AVAILABLE = False

try:
    import pyhwp
    PYHWP_AVAILABLE = True
except ImportError:
    PYHWP_AVAILABLE = False

# í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re
from collections import Counter
import pandas as pd

logger = logging.getLogger(__name__)


def clean_text(text: str, remove_unicode: bool = True, markdown_format: bool = False) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
    
    Args:
        text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
        remove_unicode: ìœ ë‹ˆì½”ë“œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ì—¬ë¶€
        markdown_format: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì ìš© ì—¬ë¶€
    
    Returns:
        ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    
    cleaned = text
    
    # ìœ ë‹ˆì½”ë“œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
    if remove_unicode:
        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€
        cleaned = re.sub(r'[^\x20-\x7Eê°€-í£ã„±-ã…ã…-ã…£\s\n\r\t]', '', cleaned)
        # ì œì–´ ë¬¸ì ì œê±° (íƒ­, ê°œí–‰, ìºë¦¬ì§€ ë¦¬í„´ ì œì™¸)
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned)
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì ìš©
    if markdown_format:
        lines = cleaned.split('\n')
        markdown_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                markdown_lines.append('')
                continue
            
            # ì œëª© ê°ì§€ ë° ë³€í™˜ (ë‹¨ë… ì¤„ì— ìˆëŠ” ì§§ì€ í…ìŠ¤íŠ¸)
            if len(line) < 100 and not line.endswith(('.', '?', '!')):
                if any(keyword in line for keyword in ['ì¡°ë¡€', 'ê·œì¹™', 'ì§€ì¹¨', 'ê³„íš', 'ì‚¬ì—…', 'ì•ˆë‚´']):
                    markdown_lines.append(f'# {line}')
                elif any(keyword in line for keyword in ['ì œ', 'ì¡°', 'í•­', 'í˜¸']):
                    markdown_lines.append(f'## {line}')
                else:
                    markdown_lines.append(line)
            # ëª©ë¡ í•­ëª© ê°ì§€ ë° ë³€í™˜
            elif re.match(r'^\s*[-â€¢â—¦â–ªâ–«]\s*', line):
                item_text = re.sub(r'^\s*[-â€¢â—¦â–ªâ–«]\s*', '', line)
                markdown_lines.append(f'- {item_text}')
            # ë²ˆí˜¸ ëª©ë¡ ê°ì§€ ë° ë³€í™˜
            elif re.match(r'^\s*\d+\.\s*', line):
                markdown_lines.append(line)
            # ì¼ë°˜ ë¬¸ë‹¨
            else:
                markdown_lines.append(line)
        
        cleaned = '\n'.join(markdown_lines)
    
    # ê³µë°± ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
    cleaned = re.sub(r'[ \t]+', ' ', cleaned)  # ê³µë°±ê³¼ íƒ­ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)  # 3ê°œ ì´ìƒ ì—°ì† ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
    cleaned = re.sub(r'^\s+|\s+$', '', cleaned, flags=re.MULTILINE)  # ê° ì¤„ì˜ ì•ë’¤ ê³µë°± ì œê±°
    
    return cleaned.strip()


class BaseTextExtractor(ABC):
    """í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.performance_metrics = {}
    
    @abstractmethod
    def extract_text(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        pass
    
    def extract_with_metrics(self, file_path: str, clean_unicode: bool = True, 
                           markdown_format: bool = False) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        start_time = time.time()
        
        try:
            raw_text = self.extract_text(file_path)
            # í…ìŠ¤íŠ¸ ì •ë¦¬ ì ìš©
            text = clean_text(raw_text, remove_unicode=clean_unicode, 
                            markdown_format=markdown_format)
            success = True
            error = None
        except Exception as e:
            text = ""
            raw_text = ""
            success = False
            error = str(e)
        
        end_time = time.time()
        
        # í…ìŠ¤íŠ¸ ì˜ˆì‹œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ)
        text_examples = self._extract_text_samples(text) if text else {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "extractor_name": self.name,
            "file_path": file_path,
            "success": success,
            "error": error,
            "extraction_time": end_time - start_time,
            "text_length": len(text) if text else 0,
            "line_count": text.count('\n') if text else 0,
            "word_count": len(text.split()) if text else 0,
            "text_preview": text[:200] + "..." if len(text) > 200 else text,
            "text_examples": text_examples,
            "text_hash": hashlib.md5(text.encode()).hexdigest() if text else None,
            "raw_text_length": len(raw_text) if raw_text else 0,
            "cleaning_applied": {"unicode": clean_unicode, "markdown": markdown_format}
        }
        
        return {
            "text": text,
            "metrics": metrics
        }
    
    def _extract_text_samples(self, text: str) -> Dict[str, str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ì–‘í•œ ìƒ˜í”Œ ì¶”ì¶œ"""
        if not text or len(text) < 100:
            return {"short": text}
        
        samples = {}
        text_len = len(text)
        
        # ì‹œì‘ ë¶€ë¶„
        samples["beginning"] = text[:150] + "..." if len(text) > 150 else text
        
        # ì¤‘ê°„ ë¶€ë¶„
        if text_len > 300:
            mid_start = text_len // 2 - 75
            mid_end = text_len // 2 + 75
            samples["middle"] = "..." + text[mid_start:mid_end] + "..."
        
        # ë ë¶€ë¶„
        if text_len > 150:
            samples["ending"] = "..." + text[-150:]
        
        return samples


# =========================================
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°ë“¤
# =========================================

class PyPDF2Extractor(BaseTextExtractor):
    """PyPDF2 ê¸°ë°˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("PyPDF2")
    
    def is_available(self) -> bool:
        return PYPDF2_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # í˜ì´ì§€ ìˆ˜ í™•ì¸
                if hasattr(pdf_reader, 'pages') and pdf_reader.pages:
                    for page in pdf_reader.pages:
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        except Exception as e:
                            logger.warning(f"PyPDF2 í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                else:
                    logger.warning("PyPDF2: í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
        except Exception as e:
            logger.error(f"PyPDF2 íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
        
        return text.strip()


class PDFPlumberExtractor(BaseTextExtractor):
    """pdfplumber ê¸°ë°˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("pdfplumber")
    
    def is_available(self) -> bool:
        return PDFPLUMBER_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        text = ""
        try:
            with pdfplumber.open(file_path) as pdf:
                # í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ í™•ì¸
                if hasattr(pdf, 'pages') and pdf.pages:
                    for page in pdf.pages:
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + "\n"
                        except Exception as e:
                            logger.warning(f"PDFPlumber í˜ì´ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            continue
                else:
                    logger.warning("PDFPlumber: í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
        except Exception as e:
            logger.error(f"PDFPlumber íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
        
        return text.strip()


class PyMuPDFExtractor(BaseTextExtractor):
    """PyMuPDF ê¸°ë°˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("PyMuPDF")
    
    def is_available(self) -> bool:
        return PYMUPDF_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        text = ""
        doc = None
        try:
            doc = fitz.open(file_path)
            
            # í˜ì´ì§€ ìˆ˜ í™•ì¸
            page_count = getattr(doc, 'page_count', 0)
            if not isinstance(page_count, int) or page_count <= 0:
                logger.warning(f"PyMuPDF: ìœ íš¨í•˜ì§€ ì•Šì€ í˜ì´ì§€ ìˆ˜ - {page_count}")
                return ""
            
            for page_num in range(page_count):
                try:
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text:
                        text += page_text + "\n"
                except Exception as e:
                    logger.warning(f"PyMuPDF í˜ì´ì§€ {page_num} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"PyMuPDF íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise
        finally:
            if doc:
                doc.close()
        
        return text.strip()


class PDFMinerExtractor(BaseTextExtractor):
    """pdfminer ê¸°ë°˜ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("pdfminer")
    
    def is_available(self) -> bool:
        return PDFMINER_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("pdfminerê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        return pdfminer_extract_text(file_path)


# =========================================
# HWP í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°ë“¤
# =========================================

class OleFileHWPExtractor(BaseTextExtractor):
    """olefile ê¸°ë°˜ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("olefile")
    
    def is_available(self) -> bool:
        return OLEFILE_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("olefileì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            with olefile.OleFileIO(file_path) as ole:
                # HWP íŒŒì¼ êµ¬ì¡° ë¶„ì„
                streams = ole.listdir()
                text_content = ""
                
                logger.info(f"HWP íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ìˆ˜: {len(streams)}ê°œ")
                
                # HWP 5.0 ì´ìƒì˜ ê²½ìš° BodyText ìŠ¤íŠ¸ë¦¼ ìš°ì„  ê²€ìƒ‰
                priority_streams = []
                other_streams = []
                
                for stream_path in streams:
                    stream_name = '/'.join(str(s) for s in stream_path).lower()
                    
                    # ìš°ì„ ìˆœìœ„ ìŠ¤íŠ¸ë¦¼ (BodyText ê´€ë ¨)
                    if any(keyword in stream_name for keyword in [
                        'bodytext', 'section', 'paragraph', 'contents'
                    ]):
                        priority_streams.append(stream_path)
                    else:
                        other_streams.append(stream_path)
                
                # ìš°ì„ ìˆœìœ„ ìŠ¤íŠ¸ë¦¼ë¶€í„° ì²˜ë¦¬
                all_streams_to_try = priority_streams + other_streams
                
                for stream_path in all_streams_to_try:
                    try:
                        stream_data = ole.get_stream(stream_path).read()
                        
                        # ìŠ¤íŠ¸ë¦¼ í¬ê¸° í™•ì¸
                        if len(stream_data) < 50:
                            continue
                        
                        # HWP íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ë°ì´í„° íŒ¨í„´ ê²€ìƒ‰
                        extracted_text = self._extract_hwp_text_from_stream(stream_data)
                        
                        if extracted_text:
                            text_content += extracted_text + " "
                            
                    except Exception as e:
                        logger.debug(f"ìŠ¤íŠ¸ë¦¼ {stream_path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                result = text_content.strip()
                if result:
                    logger.info(f"HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
                else:
                    logger.warning("HWPì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    # ì›ì‹œ ë°”ì´ë„ˆë¦¬ ë¶„ì„ ì‹œë„
                    result = self._extract_text_from_binary(file_path)
                
                return result
        
        except Exception as e:
            logger.error(f"olefileì„ ì‚¬ìš©í•œ HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def _extract_hwp_text_from_stream(self, stream_data: bytes) -> str:
        """ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        extracted_texts = []
        
        # HWP íŠ¹í™” í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰
        try:
            # CP949ë¡œ ìš°ì„  ì‹œë„ (HWP ê¸°ë³¸ ì¸ì½”ë”©)
            text = stream_data.decode('cp949', errors='replace')
            
            # ì •ìƒì ì¸ í•œê¸€ ë¬¸ì¥ íŒ¨í„´ë§Œ ì¶”ì¶œ
            # ìµœì†Œ 3ê¸€ì ì´ìƒì˜ í•œê¸€ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë¬¸ì¥
            korean_sentences = re.findall(r'[ê°€-í£]{2,}(?:\s+[ê°€-í£]{2,})*(?:[.\s]|$)', text)
            
            if korean_sentences:
                # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ í•„í„°ë§
                valid_sentences = []
                for sentence in korean_sentences:
                    sentence = sentence.strip()
                    # ìµœì†Œ ê¸¸ì´ì™€ í•œê¸€ ë¹„ìœ¨ ì²´í¬
                    if len(sentence) >= 5:
                        hangul_count = len(re.findall(r'[ê°€-í£]', sentence))
                        if hangul_count >= len(sentence) * 0.7:  # í•œê¸€ì´ 70% ì´ìƒ
                            valid_sentences.append(sentence)
                
                if valid_sentences:
                    result = ' '.join(valid_sentences[:100])  # ìµœëŒ€ 100ê°œ ë¬¸ì¥
                    # ì—°ì† ê³µë°± ì •ë¦¬
                    result = re.sub(r'\s+', ' ', result)
                    return result[:3000]  # ìµœëŒ€ 3000ì
        
        except Exception:
            pass
        
        # ëŒ€ì²´ ë°©ë²•: UTF-8ë¡œ ì‹œë„
        try:
            text = stream_data.decode('utf-8', errors='ignore')
            korean_words = re.findall(r'[ê°€-í£]{2,}', text)
            if korean_words and len(korean_words) >= 5:
                result = ' '.join(korean_words[:200])
                return result[:2000]
        except Exception:
            pass
        
        return ""
    
    def _extract_text_from_binary(self, file_path: str) -> str:
        """ë°”ì´ë„ˆë¦¬ íŒŒì¼ì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ íŒ¨í„´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            with open(file_path, 'rb') as f:
                # ì „ì²´ íŒŒì¼ ì½ê¸° (ë„ˆë¬´ í¬ë©´ ì¼ë¶€ë§Œ)
                file_data = f.read(1024 * 1024)  # ìµœëŒ€ 1MB
                
                extracted_sentences = []
                
                # CP949 ë””ì½”ë”©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                try:
                    text = file_data.decode('cp949', errors='replace')
                    
                    # ì™„ì „í•œ í•œê¸€ ë¬¸ì¥ íŒ¨í„´ ì°¾ê¸°
                    sentence_patterns = [
                        r'[ê°€-í£]{2,}(?:\s+[ê°€-í£]{1,}){2,}',  # í•œê¸€ ë‹¨ì–´ 3ê°œ ì´ìƒ
                        r'[ê°€-í£]+(?:\s+[ê°€-í£]+)*\s*[.:!?]',   # ë¬¸ì¥ ë¶€í˜¸ë¡œ ëë‚˜ëŠ” ë¬¸ì¥
                        r'ì œ\d+[ì¡°í•­ëª©]\s+[ê°€-í£]{3,}',         # ë²•ê·œ ì¡°í•­
                        r'[ê°€-í£]{3,}\s+[ê°€-í£]{3,}\s+[ê°€-í£]{3,}',  # ì—°ì†ëœ ë‹¨ì–´
                    ]
                    
                    for pattern in sentence_patterns:
                        matches = re.findall(pattern, text)
                        for match in matches:
                            # í’ˆì§ˆ ì²´í¬
                            if len(match) >= 6 and len(match) <= 100:
                                hangul_ratio = len(re.findall(r'[ê°€-í£]', match)) / len(match)
                                if hangul_ratio >= 0.6:  # í•œê¸€ 60% ì´ìƒ
                                    extracted_sentences.append(match.strip())
                    
                    if extracted_sentences:
                        # ì¤‘ë³µ ì œê±°í•˜ê³  í’ˆì§ˆìˆœ ì •ë ¬
                        unique_sentences = list(set(extracted_sentences))
                        unique_sentences.sort(key=len, reverse=True)
                        
                        result = ' '.join(unique_sentences[:30])  # ìµœëŒ€ 30ê°œ ë¬¸ì¥
                        result = re.sub(r'\s+', ' ', result)  # ê³µë°± ì •ë¦¬
                        
                        logger.info(f"ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
                        return result[:2000]  # ìµœëŒ€ 2000ì
                
                except Exception as e:
                    logger.debug(f"CP949 ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                
                # ëŒ€ì²´ ë°©ë²•: EUC-KR ì‹œë„
                try:
                    text = file_data.decode('euc-kr', errors='replace')
                    korean_words = re.findall(r'[ê°€-í£]{3,}', text)
                    if korean_words:
                        result = ' '.join(korean_words[:100])
                        logger.info(f"EUC-KR ì¶”ì¶œ ì„±ê³µ: {len(result)}ì")
                        return result[:1500]
                except Exception:
                    pass
                
        except Exception as e:
            logger.debug(f"ë°”ì´ë„ˆë¦¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return ""


class HWP5Extractor(BaseTextExtractor):
    """hwp5 ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("hwp5")
    
    def is_available(self) -> bool:
        return HWP5_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("hwp5ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            from hwp5.binmodel import Document
            from hwp5.xmlmodel import Text
            
            doc = Document(file_path)
            text_content = []
            
            # ë¬¸ì„œì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for section in doc.bodytext.sections:
                for paragraph in section.paragraphs:
                    for text_run in paragraph.text_runs:
                        if hasattr(text_run, 'text'):
                            text_content.append(text_run.text)
            
            return '\n'.join(text_content)
        
        except Exception as e:
            logger.warning(f"hwp5ë¥¼ ì‚¬ìš©í•œ HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""


class PyHWPExtractor(BaseTextExtractor):
    """pyhwp ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ HWP í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__("pyhwp")
    
    def is_available(self) -> bool:
        return PYHWP_AVAILABLE
    
    def extract_text(self, file_path: str) -> str:
        if not self.is_available():
            raise ImportError("pyhwpê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # pyhwpë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            import pyhwp
            
            # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œ ë¼ì´ë¸ŒëŸ¬ë¦¬ APIì— ë§ê²Œ ìˆ˜ì • í•„ìš”
            text = pyhwp.extract_text(file_path)
            return text if text else ""
        
        except Exception as e:
            logger.warning(f"pyhwpë¥¼ ì‚¬ìš©í•œ HWP ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""


# =========================================
# í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œ
# =========================================

class TextExtractionComparison:
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # PDF ì¶”ì¶œê¸°ë“¤
        self.pdf_extractors = [
            PyPDF2Extractor(),
            PDFPlumberExtractor(),
            PyMuPDFExtractor(),
            PDFMinerExtractor()
        ]
        
        # HWP ì¶”ì¶œê¸°ë“¤
        self.hwp_extractors = [
            OleFileHWPExtractor(),
            HWP5Extractor(),
            PyHWPExtractor()
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸°ë§Œ í•„í„°ë§
        self.available_pdf_extractors = [e for e in self.pdf_extractors if e.is_available()]
        self.available_hwp_extractors = [e for e in self.hwp_extractors if e.is_available()]
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œê¸°: {[e.name for e in self.available_pdf_extractors]}")
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ HWP ì¶”ì¶œê¸°: {[e.name for e in self.available_hwp_extractors]}")
    
    def compare_pdf_extractors(self, pdf_files: List[str]) -> Dict[str, Any]:
        """PDF ì¶”ì¶œê¸°ë“¤ ì„±ëŠ¥ ë¹„êµ"""
        
        if not self.available_pdf_extractors:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"error": "ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        results = []
        
        for pdf_file in pdf_files:
            if not os.path.exists(pdf_file):
                logger.warning(f"PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file}")
                continue
            
            logger.info(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘: {pdf_file}")
            
            for extractor in self.available_pdf_extractors:
                result = extractor.extract_with_metrics(pdf_file)
                # metrics ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ì¶”ê°€ ì •ë³´ ì¶”ê°€
                if "metrics" in result:
                    metric_result = result["metrics"].copy()
                    metric_result["file_name"] = os.path.basename(pdf_file)
                    metric_result["extractor"] = extractor.name
                    results.append(metric_result)
                else:
                    # ì´ì „ í˜•ì‹ê³¼ì˜ í˜¸í™˜ì„±
                    result["file_name"] = os.path.basename(pdf_file)
                    result["extractor"] = extractor.name
                    results.append(result)
        
        return self._analyze_extraction_results(results, "PDF")
    
    def compare_hwp_extractors(self, hwp_files: List[str]) -> Dict[str, Any]:
        """HWP ì¶”ì¶œê¸°ë“¤ ì„±ëŠ¥ ë¹„êµ"""
        
        if not self.available_hwp_extractors:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ HWP ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {"error": "ì‚¬ìš© ê°€ëŠ¥í•œ HWP ì¶”ì¶œê¸°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        results = []
        
        for hwp_file in hwp_files:
            if not os.path.exists(hwp_file):
                logger.warning(f"HWP íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hwp_file}")
                continue
            
            logger.info(f"HWP íŒŒì¼ ì²˜ë¦¬ ì¤‘: {hwp_file}")
            
            for extractor in self.available_hwp_extractors:
                result = extractor.extract_with_metrics(hwp_file)
                # metrics ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ì¶”ê°€ ì •ë³´ ì¶”ê°€
                if "metrics" in result:
                    metric_result = result["metrics"].copy()
                    metric_result["file_name"] = os.path.basename(hwp_file)
                    metric_result["extractor"] = extractor.name
                    results.append(metric_result)
                else:
                    # ì´ì „ í˜•ì‹ê³¼ì˜ í˜¸í™˜ì„±
                    result["file_name"] = os.path.basename(hwp_file)
                    result["extractor"] = extractor.name
                    results.append(result)
        
        return self._analyze_extraction_results(results, "HWP")
    
    def simple_compare(self, file_path: str = None) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ë¹„êµ ë©”ì„œë“œ - ë©”íŠ¸ë¦­ ì˜¤ë¥˜ ë°©ì§€"""
        if file_path and os.path.exists(file_path):
            file_ext = Path(file_path).suffix.lower()
            if file_ext == '.pdf':
                extractors = self.available_pdf_extractors
            elif file_ext == '.hwp':
                extractors = self.available_hwp_extractors
            else:
                return {"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤"}
        else:
            # ê¸°ë³¸ ì¶”ì¶œê¸° ì •ë³´ ë°˜í™˜
            return {
                "extractors": {
                    "pdf": [e.name for e in self.available_pdf_extractors],
                    "hwp": [e.name for e in self.available_hwp_extractors]
                },
                "total_extractors": len(self.available_pdf_extractors) + len(self.available_hwp_extractors)
            }
        
        # ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬
        detailed_results = []
        
        for extractor in extractors:
            try:
                result = extractor.extract_with_metrics(file_path)
                
                # ë©”íŠ¸ë¦­ êµ¬ì¡° ì •ë¦¬
                if "metrics" in result:
                    metrics = result["metrics"]
                    detailed_result = {
                        "extractor": extractor.name,
                        "success": metrics.get("success", False),
                        "extraction_time": metrics.get("extraction_time", 0),
                        "text_length": metrics.get("text_length", 0),
                        "text_preview": metrics.get("text_preview", ""),
                        "error": metrics.get("error") if not metrics.get("success", False) else None
                    }
                else:
                    # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ì²˜ë¦¬
                    detailed_result = {
                        "extractor": extractor.name,
                        "success": result.get("success", False),
                        "extraction_time": result.get("extraction_time", 0),
                        "text_length": result.get("text_length", 0),
                        "text_preview": result.get("text_preview", ""),
                        "error": result.get("error") if not result.get("success", False) else None
                    }
                
                detailed_results.append(detailed_result)
                
            except Exception as e:
                detailed_results.append({
                    "extractor": extractor.name,
                    "success": False,
                    "extraction_time": 0,
                    "text_length": 0,
                    "text_preview": "",
                    "error": str(e)
                })
        
        # ìµœê³  ì„±ëŠ¥ ì¶”ì¶œê¸° ì„ ì •
        successful_results = [r for r in detailed_results if r["success"]]
        best_extractor = None
        
        if successful_results:
            # ë‹¨ìˆœí•œ ì ìˆ˜ ê³„ì‚° (í…ìŠ¤íŠ¸ ê¸¸ì´ / ì‹œê°„)
            for result in successful_results:
                time_taken = max(result["extraction_time"], 0.001)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                result["score"] = result["text_length"] / time_taken
            
            best_result = max(successful_results, key=lambda x: x["score"])
            best_extractor = {
                "name": best_result["extractor"],
                "score": best_result["score"]
            }
        
        return {
            "results": {
                "detailed_results": detailed_results,
                "best_extractor": best_extractor,
                "file_path": file_path,
                "successful_extractors": len(successful_results),
                "total_extractors": len(detailed_results)
            }
        }
    
    def _analyze_extraction_results(self, results: List[Dict], file_type: str) -> Dict[str, Any]:
        """ì¶”ì¶œ ê²°ê³¼ ë¶„ì„"""
        
        if not results:
            return {"error": f"{file_type} ì¶”ì¶œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ê²°ê³¼ ì •ë¦¬ - ë©”íŠ¸ë¦­ êµ¬ì¡°ì— ë§ì¶° ì¡°ì •
        processed_results = []
        for result in results:
            # ì´ë¯¸ ë©”íŠ¸ë¦­ í˜•íƒœì¸ ê²½ìš°ì™€ ì „ì²´ ê²°ê³¼ í˜•íƒœì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
            if "extractor_name" in result:
                # ì´ë¯¸ ë©”íŠ¸ë¦­ í˜•íƒœ
                processed_results.append(result)
            elif "metrics" in result:
                # ì „ì²´ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                metrics = result["metrics"].copy()
                if "text" in result:
                    metrics["text_content"] = result["text"]
                processed_results.append(metrics)
            else:
                # ê¸°ë³¸ ê²°ê³¼ í˜•íƒœ
                processed_results.append(result)
        
        if not processed_results:
            return {"error": f"{file_type} ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì¶”ì¶œê¸°ë³„ ì„±ëŠ¥ ë¶„ì„ (pandas ì—†ì´)
        extractor_performance = {}
        
        # ì¶”ì¶œê¸°ë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™”
        extractor_groups = {}
        for result in processed_results:
            extractor_name = result.get('extractor_name', result.get('extractor', 'unknown'))
            if extractor_name not in extractor_groups:
                extractor_groups[extractor_name] = []
            extractor_groups[extractor_name].append(result)
        
        for extractor_name, extractor_results in extractor_groups.items():
            successful_results = [r for r in extractor_results if r.get('success', False)]
            
            success_rate = len(successful_results) / len(extractor_results) if extractor_results else 0
            
            if successful_results:
                avg_time = sum(r.get('extraction_time', 0) for r in successful_results) / len(successful_results)
                avg_text_length = sum(r.get('text_length', 0) for r in successful_results) / len(successful_results)
                avg_word_count = sum(r.get('word_count', 0) for r in successful_results) / len(successful_results)
                
                # í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
                text_contents = [r.get('text_content', '') for r in successful_results if r.get('text_content')]
                quality_score = self._evaluate_text_quality_simple(text_contents)
            else:
                avg_time = 0
                avg_text_length = 0
                avg_word_count = 0
                quality_score = 0
            
            extractor_performance[extractor_name] = {
                "success_rate": success_rate,
                "avg_extraction_time": avg_time,
                "avg_text_length": avg_text_length,
                "avg_word_count": avg_word_count,
                "text_quality_score": quality_score,
                "processed_files": len(extractor_results)
            }
        
        # ìµœì  ì¶”ì¶œê¸° ì„ ì •
        best_extractor = self._select_best_extractor(extractor_performance)
        
    def _evaluate_text_quality_simple(self, text_contents: List[str]) -> float:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€"""
        if not text_contents:
            return 0.0
        
        quality_scores = []
        for text in text_contents:
            if not text:
                quality_scores.append(0.0)
                continue
                
            score = 0.0
            # í•œê¸€ í¬í•¨ ì—¬ë¶€
            if re.search(r'[ê°€-í£]', text):
                score += 0.4
            # ì ì ˆí•œ ê¸¸ì´
            if len(text) > 100:
                score += 0.3
            # íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ í™•ì¸
            special_ratio = len(re.findall(r'[^\w\sê°€-í£]', text)) / len(text)
            if special_ratio < 0.2:
                score += 0.3
                
            quality_scores.append(score)
        
        return sum(quality_scores) / len(quality_scores)
        
        # ìƒì„¸ ê²°ê³¼
        detailed_results = []
        for result in processed_results:
            detailed_results.append({
                "file_name": result.get("file_name"),
                "extractor": result["metrics"]["extractor_name"],
                "success": result["metrics"]["success"],
                "extraction_time": result["metrics"]["extraction_time"],
                "text_length": result["metrics"]["text_length"],
                "word_count": result["metrics"]["word_count"],
                "error": result["metrics"]["error"],
                "text_preview": result["metrics"]["text_preview"]
            })
        
        return {
            "file_type": file_type,
            "total_files": len(set(r["file_name"] for r in results)),
            "total_extractions": len(results),
            "extractor_performance": extractor_performance,
            "best_extractor": best_extractor,
            "detailed_results": detailed_results,
            "summary": self._generate_summary(extractor_performance, file_type)
        }
    
    def _evaluate_text_quality(self, texts: List[str]) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€"""
        
        if not texts:
            return 0.0
        
        quality_scores = []
        
        for text in texts:
            if not text:
                quality_scores.append(0.0)
                continue
            
            # ê¸°ë³¸ í’ˆì§ˆ ì§€í‘œë“¤
            score = 0.0
            
            # 1. í…ìŠ¤íŠ¸ ê¸¸ì´ (ì ì ˆí•œ ê¸¸ì´)
            if 100 <= len(text) <= 10000:
                score += 0.2
            elif len(text) > 50:
                score += 0.1
            
            # 2. í•œê¸€ ë¹„ìœ¨
            korean_chars = len(re.findall(r'[ê°€-í£]', text))
            if korean_chars > 0:
                korean_ratio = korean_chars / len(text)
                score += min(korean_ratio * 0.3, 0.3)
            
            # 3. ë‹¨ì–´ êµ¬ì¡° (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ë“¤)
            words = text.split()
            if len(words) > 10:
                score += 0.2
            elif len(words) > 5:
                score += 0.1
            
            # 4. ë¬¸ì¥ êµ¬ì¡° (ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±)
            sentences = re.split(r'[.!?]\s*', text)
            if len(sentences) > 3:
                score += 0.2
            elif len(sentences) > 1:
                score += 0.1
            
            # 5. íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì )
            special_chars = len(re.findall(r'[^\w\sê°€-í£.,!?]', text))
            special_ratio = special_chars / len(text) if len(text) > 0 else 0
            if special_ratio < 0.1:
                score += 0.1
            
            quality_scores.append(min(score, 1.0))
        
        return sum(quality_scores) / len(quality_scores)
    
    def _select_best_extractor(self, performance: Dict[str, Dict]) -> Dict[str, Any]:
        """ìµœì  ì¶”ì¶œê¸° ì„ ì •"""
        
        if not performance:
            return {"name": None, "score": 0.0}
        
        extractor_scores = {}
        
        for name, metrics in performance.items():
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
            score = (
                metrics["success_rate"] * 0.4 +           # ì„±ê³µë¥  40%
                metrics["text_quality_score"] * 0.3 +     # í…ìŠ¤íŠ¸ í’ˆì§ˆ 30%
                (1 / (1 + metrics["avg_extraction_time"])) * 0.2 +  # ì†ë„ 20%
                min(metrics["avg_text_length"] / 1000, 1.0) * 0.1   # í…ìŠ¤íŠ¸ ê¸¸ì´ 10%
            )
            
            extractor_scores[name] = score
        
        best_name = max(extractor_scores.keys(), key=lambda k: extractor_scores[k])
        best_score = extractor_scores[best_name]
        
        return {
            "name": best_name,
            "score": best_score,
            "metrics": performance[best_name],
            "all_scores": extractor_scores
        }
    
    def _generate_summary(self, performance: Dict, file_type: str) -> str:
        """ì„±ëŠ¥ ë¹„êµ ìš”ì•½ ìƒì„±"""
        
        if not performance:
            return f"{file_type} ì¶”ì¶œê¸° ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        summary_lines = [f"\n=== {file_type} í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° ì„±ëŠ¥ ë¹„êµ ===\n"]
        
        # ê° ì¶”ì¶œê¸°ë³„ ìš”ì•½
        for name, metrics in performance.items():
            summary_lines.append(f"ğŸ“„ {name}:")
            summary_lines.append(f"  - ì„±ê³µë¥ : {metrics['success_rate']:.1%}")
            summary_lines.append(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {metrics['avg_extraction_time']:.2f}ì´ˆ")
            summary_lines.append(f"  - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {metrics['avg_text_length']:.0f}ì")
            summary_lines.append(f"  - í…ìŠ¤íŠ¸ í’ˆì§ˆ: {metrics['text_quality_score']:.2f}/1.0")
            summary_lines.append("")
        
        return "\n".join(summary_lines)
    
    def get_best_extractor_for_file(self, file_path: str) -> Optional[BaseTextExtractor]:
        """íŒŒì¼ì— ëŒ€í•œ ìµœì  ì¶”ì¶œê¸° ë°˜í™˜"""
        
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            if self.available_pdf_extractors:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ìµœì  ì¶”ì¶œê¸° ì„ ì •
                test_results = []
                for extractor in self.available_pdf_extractors[:2]:  # ì²˜ìŒ 2ê°œë§Œ í…ŒìŠ¤íŠ¸
                    try:
                        result = extractor.extract_with_metrics(file_path)
                        if result["metrics"]["success"]:
                            test_results.append((extractor, result["metrics"]["text_length"]))
                    except:
                        continue
                
                if test_results:
                    # ê°€ì¥ ë§ì€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œ ì¶”ì¶œê¸° ì„ íƒ
                    return max(test_results, key=lambda x: x[1])[0]
                else:
                    return self.available_pdf_extractors[0]
        
        elif file_ext == '.hwp':
            if self.available_hwp_extractors:
                return self.available_hwp_extractors[0]
        
        return None


def main():
    """í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²• ë¹„êµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë¹„êµ ì‹œìŠ¤í…œ ìƒì„±
    comparison = TextExtractionComparison()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¶”ì¶œê¸° í™•ì¸
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œê¸°: {[e.name for e in comparison.available_pdf_extractors]}")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ HWP ì¶”ì¶œê¸°: {[e.name for e in comparison.available_hwp_extractors]}")
    
    # ìƒ˜í”Œ íŒŒì¼ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
    test_pdf_files = []
    test_hwp_files = []
    
    # data ë””ë ‰í† ë¦¬ì—ì„œ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸°
    data_dir = Path("../data/ë³µì§€ë¡œ")
    if data_dir.exists():
        test_pdf_files = list(data_dir.rglob("*.pdf"))[:3]  # ìµœëŒ€ 3ê°œ
        test_hwp_files = list(data_dir.rglob("*.hwp"))[:3]  # ìµœëŒ€ 3ê°œ
    
    # PDF ì¶”ì¶œê¸° ë¹„êµ
    if test_pdf_files and comparison.available_pdf_extractors:
        print(f"\nğŸ” PDF ì¶”ì¶œê¸° ë¹„êµ í…ŒìŠ¤íŠ¸ ({len(test_pdf_files)}ê°œ íŒŒì¼)")
        pdf_results = comparison.compare_pdf_extractors([str(f) for f in test_pdf_files])
        
        if "error" not in pdf_results:
            print(pdf_results["summary"])
            best_pdf = pdf_results["best_extractor"]
            print(f"ğŸ† ìµœì  PDF ì¶”ì¶œê¸°: {best_pdf['name']} (ì ìˆ˜: {best_pdf['score']:.3f})")
    
    # HWP ì¶”ì¶œê¸° ë¹„êµ  
    if test_hwp_files and comparison.available_hwp_extractors:
        print(f"\nğŸ” HWP ì¶”ì¶œê¸° ë¹„êµ í…ŒìŠ¤íŠ¸ ({len(test_hwp_files)}ê°œ íŒŒì¼)")
        hwp_results = comparison.compare_hwp_extractors([str(f) for f in test_hwp_files])
        
        if "error" not in hwp_results:
            print(hwp_results["summary"])
            best_hwp = hwp_results["best_extractor"]
            print(f"ğŸ† ìµœì  HWP ì¶”ì¶œê¸°: {best_hwp['name']} (ì ìˆ˜: {best_hwp['score']:.3f})")
    
    print(f"\nâœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¹„êµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()