# =========================================
# ğŸ—„ï¸ ChromaDB ë²¡í„°ìŠ¤í† ì–´ ëª¨ë“ˆ
# =========================================

import os
import uuid
import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
import pandas as pd
import numpy as np

# ChromaDB ì„í¬íŠ¸
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False

# LangChain ì„í¬íŠ¸ (ì„ íƒì )
try:
    from langchain_community.vectorstores import Chroma
    from langchain.schema import Document
    LANGCHAIN_CHROMA_AVAILABLE = True
except ImportError:
    LANGCHAIN_CHROMA_AVAILABLE = False

logger = logging.getLogger(__name__)


class WelfareVectorStore:
    """ë…¸ì¸ë³µì§€ ì •ì±… ì „ìš© ChromaDB ë²¡í„°ìŠ¤í† ì–´"""
    
    def __init__(self, 
                 collection_name: str = "elderly_welfare_policies",
                 persist_directory: str = None,
                 embedding_function=None):
        """
        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
            persist_directory: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            embedding_function: ì„ë² ë”© í•¨ìˆ˜
        """
        if not CHROMADB_AVAILABLE:
            raise ImportError("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install chromadb")
        
        self.collection_name = collection_name
        self.persist_directory = persist_directory or "./chroma_db"
        self.embedding_function = embedding_function
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = None
        self.collection = None
        
        # ë©”íƒ€ë°ì´í„° ê´€ë¦¬
        self.document_metadata = {}
        
        # ì´ˆê¸°í™”
        self._initialize_client()
    
    def _initialize_client(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.client = chromadb.PersistentClient(path=self.persist_directory)
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
            try:
                self.collection = self.client.get_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function
                )
                logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ: {self.collection_name}")
            except Exception:
                # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    embedding_function=self.embedding_function,
                    metadata={"description": "ë…¸ì¸ë³µì§€ ì •ì±… ë¬¸ì„œ ì»¬ë ‰ì…˜"}
                )
                logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
            
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            self._load_metadata()
            
        except Exception as e:
            logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _load_metadata(self):
        """ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        metadata_path = Path(self.persist_directory) / "document_metadata.json"
        
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    self.document_metadata = json.load(f)
                logger.info(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.document_metadata)}ê°œ ë¬¸ì„œ")
            except Exception as e:
                logger.warning(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.document_metadata = {}
        else:
            self.document_metadata = {}
    
    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_path = Path(self.persist_directory) / "document_metadata.json"
        
        try:
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.document_metadata, f, ensure_ascii=False, indent=2)
            logger.debug("ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def add_documents(self, 
                     texts: List[str], 
                     metadatas: List[Dict] = None,
                     embeddings: List[List[float]] = None,
                     ids: List[str] = None) -> List[str]:
        """ë¬¸ì„œë“¤ì„ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€"""
        
        if not texts:
            logger.warning("ì¶”ê°€í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ID ìƒì„±
        if ids is None:
            ids = [str(uuid.uuid4()) for _ in texts]
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì •
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        # ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ í¬í•¨
        enhanced_metadatas = []
        for i, metadata in enumerate(metadatas):
            enhanced_metadata = {
                **metadata,
                "document_id": ids[i],
                "text_length": len(texts[i]),
                "created_at": datetime.now().isoformat(),
                "collection": self.collection_name
            }
            
            # ë³µì§€ ì •ì±… ê´€ë ¨ ë¶„ë¥˜
            enhanced_metadata.update(self._classify_welfare_document(texts[i]))
            enhanced_metadatas.append(enhanced_metadata)
        
        try:
            # ChromaDBì— ë¬¸ì„œ ì¶”ê°€
            if embeddings is not None:
                self.collection.add(
                    embeddings=embeddings,
                    documents=texts,
                    metadatas=enhanced_metadatas,
                    ids=ids
                )
            else:
                self.collection.add(
                    documents=texts,
                    metadatas=enhanced_metadatas,
                    ids=ids
                )
            
            # ë¡œì»¬ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            for i, doc_id in enumerate(ids):
                self.document_metadata[doc_id] = {
                    "text": texts[i],
                    "metadata": enhanced_metadatas[i]
                }
            
            self._save_metadata()
            
            logger.info(f"{len(texts)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            return ids
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def _classify_welfare_document(self, text: str) -> Dict[str, Any]:
        """ë³µì§€ ë¬¸ì„œ ìë™ ë¶„ë¥˜"""
        classification = {
            "welfare_category": "ê¸°íƒ€",
            "target_age": None,
            "support_type": [],
            "keywords": []
        }
        
        text_lower = text.lower()
        
        # ë³µì§€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        if any(keyword in text for keyword in ["ì˜ë£Œ", "ê±´ê°•", "ì¹˜ë£Œ", "ì§„ë£Œ"]):
            classification["welfare_category"] = "ì˜ë£Œì§€ì›"
        elif any(keyword in text for keyword in ["ëŒë´„", "ê°„ë³‘", "ìš”ì–‘", "ì¼€ì–´"]):
            classification["welfare_category"] = "ëŒë´„ì„œë¹„ìŠ¤"
        elif any(keyword in text for keyword in ["ìƒí™œë¹„", "ìˆ˜ë‹¹", "ì—°ê¸ˆ", "ê¸‰ì—¬"]):
            classification["welfare_category"] = "ìƒí™œì§€ì›"
        elif any(keyword in text for keyword in ["ì£¼ê±°", "ì„ëŒ€", "ì£¼íƒ", "ê±°ì£¼"]):
            classification["welfare_category"] = "ì£¼ê±°ì§€ì›"
        elif any(keyword in text for keyword in ["êµí†µ", "ì´ë™", "ì°¨ëŸ‰", "ë²„ìŠ¤"]):
            classification["welfare_category"] = "êµí†µì§€ì›"
        
        # ëŒ€ìƒ ì—°ë ¹ ì¶”ì¶œ
        import re
        age_patterns = [
            r'(\d+)ì„¸ ì´ìƒ',
            r'ë§Œ\s*(\d+)ì„¸',
            r'(\d+)ì„¸ ë…¸ì¸'
        ]
        
        for pattern in age_patterns:
            match = re.search(pattern, text)
            if match:
                classification["target_age"] = int(match.group(1))
                break
        
        # ì§€ì› ìœ í˜•
        support_types = []
        if "ì‹ ì²­" in text:
            support_types.append("ì‹ ì²­í•„ìš”")
        if any(word in text for word in ["ë¬´ë£Œ", "ì§€ì›", "ê¸‰ì—¬"]):
            support_types.append("ì •ë¶€ì§€ì›")
        if "ì†Œë“" in text or "ê¸°ì´ˆìƒí™œ" in text:
            support_types.append("ì†Œë“ê¸°ë°˜")
        
        classification["support_type"] = support_types
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        welfare_keywords = [
            "ë…¸ì¸", "ì–´ë¥´ì‹ ", "ê³ ë ¹ì", "ë³µì§€", "ì§€ì›", "í˜œíƒ",
            "ì˜ë£Œë¹„", "ê°„ë³‘", "ìš”ì–‘", "ëŒë´„", "ìƒí™œì§€ì›", "ìˆ˜ë‹¹"
        ]
        
        found_keywords = [kw for kw in welfare_keywords if kw in text]
        classification["keywords"] = found_keywords
        
        return classification
    
    def similarity_search(self, 
                         query: str, 
                         k: int = 5,
                         filter_dict: Dict = None,
                         include_embeddings: bool = False) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = self.collection.query(
                query_texts=[query],
                n_results=k,
                where=filter_dict,
                include=["documents", "metadatas", "distances"] + 
                       (["embeddings"] if include_embeddings else [])
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for i in range(len(results['documents'][0])):
                result = {
                    'document': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i],
                    'similarity_score': 1 - results['distances'][0][i]  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                }
                
                if include_embeddings and 'embeddings' in results:
                    result['embedding'] = results['embeddings'][0][i]
                
                formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_metadata(self, 
                          filter_dict: Dict, 
                          limit: int = 10) -> List[Dict]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            results = self.collection.get(
                where=filter_dict,
                limit=limit,
                include=["documents", "metadatas"]
            )
            
            formatted_results = []
            for i in range(len(results['documents'])):
                formatted_results.append({
                    'document': results['documents'][i],
                    'metadata': results['metadatas'][i],
                    'id': results['ids'][i]
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        try:
            count = self.collection.count()
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            support_type_stats = {}
            region_stats = {}
            
            # ì „ì²´ ë¬¸ì„œ ì¡°íšŒ (ìƒ˜í”Œë§)
            sample_size = min(1000, count)
            if count > 0:
                results = self.collection.get(
                    limit=sample_size,
                    include=["metadatas"]
                )
                
                for metadata in results['metadatas']:
                    # ì¹´í…Œê³ ë¦¬ í†µê³„
                    category = metadata.get('welfare_category', 'ê¸°íƒ€')
                    category_stats[category] = category_stats.get(category, 0) + 1
                    
                    # ì§€ì› ìœ í˜• í†µê³„
                    support_types = metadata.get('support_type', [])
                    for support_type in support_types:
                        support_type_stats[support_type] = support_type_stats.get(support_type, 0) + 1
                    
                    # ì§€ì—­ í†µê³„
                    region = metadata.get('region', 'ì „êµ­')
                    region_stats[region] = region_stats.get(region, 0) + 1
            
            return {
                'total_documents': count,
                'collection_name': self.collection_name,
                'category_distribution': category_stats,
                'support_type_distribution': support_type_stats,
                'region_distribution': region_stats,
                'last_updated': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def delete_documents(self, ids: List[str]) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        try:
            self.collection.delete(ids=ids)
            
            # ë¡œì»¬ ë©”íƒ€ë°ì´í„°ì—ì„œë„ ì‚­ì œ
            for doc_id in ids:
                if doc_id in self.document_metadata:
                    del self.document_metadata[doc_id]
            
            self._save_metadata()
            
            logger.info(f"{len(ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def update_document(self, doc_id: str, text: str = None, metadata: Dict = None) -> bool:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        try:
            update_data = {"ids": [doc_id]}
            
            if text is not None:
                update_data["documents"] = [text]
                
            if metadata is not None:
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ë¶„ë¥˜ ì¬ìˆ˜í–‰
                if text:
                    metadata.update(self._classify_welfare_document(text))
                metadata["updated_at"] = datetime.now().isoformat()
                update_data["metadatas"] = [metadata]
            
            self.collection.update(**update_data)
            
            # ë¡œì»¬ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if doc_id in self.document_metadata:
                if text is not None:
                    self.document_metadata[doc_id]["text"] = text
                if metadata is not None:
                    self.document_metadata[doc_id]["metadata"].update(metadata)
            
            self._save_metadata()
            
            logger.info(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {doc_id}")
            return True
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    def export_collection(self, output_path: str = None) -> str:
        """ì»¬ë ‰ì…˜ ë‚´ìš©ì„ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        if output_path is None:
            output_path = f"{self.collection_name}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        try:
            # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
            results = self.collection.get(include=["documents", "metadatas"])
            
            # DataFrame ìƒì„±
            data = []
            for i, doc in enumerate(results['documents']):
                metadata = results['metadatas'][i]
                row = {
                    'id': results['ids'][i],
                    'document': doc,
                    'text_length': len(doc),
                    'welfare_category': metadata.get('welfare_category', ''),
                    'target_age': metadata.get('target_age', ''),
                    'support_type': ', '.join(metadata.get('support_type', [])),
                    'keywords': ', '.join(metadata.get('keywords', [])),
                    'region': metadata.get('region', ''),
                    'created_at': metadata.get('created_at', ''),
                    'updated_at': metadata.get('updated_at', '')
                }
                data.append(row)
            
            df = pd.DataFrame(data)
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"ì»¬ë ‰ì…˜ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            raise
    
    def backup_collection(self, backup_path: str = None) -> str:
        """ì»¬ë ‰ì…˜ ë°±ì—…"""
        if backup_path is None:
            backup_path = f"backup_{self.collection_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            import shutil
            
            # ì „ì²´ ë””ë ‰í† ë¦¬ ë°±ì—…
            shutil.copytree(self.persist_directory, backup_path)
            
            logger.info(f"ì»¬ë ‰ì…˜ ë°±ì—… ì™„ë£Œ: {backup_path}")
            return backup_path
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ë°±ì—… ì‹¤íŒ¨: {e}")
            raise


class LangChainChromaWrapper:
    """LangChainê³¼ í˜¸í™˜ë˜ëŠ” ChromaDB ë˜í¼"""
    
    def __init__(self, welfare_vector_store: WelfareVectorStore, embedding_function):
        """
        Args:
            welfare_vector_store: WelfareVectorStore ì¸ìŠ¤í„´ìŠ¤
            embedding_function: LangChain ì„ë² ë”© í•¨ìˆ˜
        """
        if not LANGCHAIN_CHROMA_AVAILABLE:
            logger.warning("LangChain Chroma ì§€ì›ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        self.welfare_store = welfare_vector_store
        self.embedding_function = embedding_function
        self._langchain_store = None
        
        # LangChain Chroma ì´ˆê¸°í™” ì‹œë„
        if LANGCHAIN_CHROMA_AVAILABLE:
            try:
                self._langchain_store = Chroma(
                    collection_name=welfare_vector_store.collection_name,
                    persist_directory=welfare_vector_store.persist_directory,
                    embedding_function=embedding_function
                )
            except Exception as e:
                logger.warning(f"LangChain Chroma ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def add_documents(self, documents: List[Document]) -> List[str]:
        """LangChain Document ê°ì²´ë“¤ì„ ì¶”ê°€"""
        texts = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]
        
        return self.welfare_store.add_documents(texts, metadatas)
    
    def similarity_search(self, query: str, k: int = 4, **kwargs) -> List[Document]:
        """LangChain í˜¸í™˜ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        results = self.welfare_store.similarity_search(query, k, **kwargs)
        
        documents = []
        for result in results:
            doc = Document(
                page_content=result['document'],
                metadata=result['metadata']
            )
            documents.append(doc)
        
        return documents
    
    def as_retriever(self, **kwargs):
        """LangChain Retrieverë¡œ ë³€í™˜"""
        if self._langchain_store:
            return self._langchain_store.as_retriever(**kwargs)
        else:
            # ì‚¬ìš©ì ì •ì˜ ë¦¬íŠ¸ë¦¬ë²„ êµ¬í˜„
            from langchain.schema.retriever import BaseRetriever
            
            class CustomWelfareRetriever(BaseRetriever):
                def __init__(self, wrapper):
                    self.wrapper = wrapper
                
                def _get_relevant_documents(self, query: str) -> List[Document]:
                    return self.wrapper.similarity_search(query)
            
            return CustomWelfareRetriever(self)


def main():
    """ë²¡í„°ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ—„ï¸ ChromaDB ë²¡í„°ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_documents = [
        {
            "text": "ë§Œ 65ì„¸ ì´ìƒ ë…¸ì¸ì„ ëŒ€ìƒìœ¼ë¡œ ì˜ë£Œë¹„ë¥¼ ì›” ìµœëŒ€ 30ë§Œì›ê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤.",
            "metadata": {"region": "ì „êµ­", "file_format": "pdf", "source": "ì˜ë£Œë¹„ì§€ì›ì•ˆë‚´.pdf"}
        },
        {
            "text": "ê¸°ì´ˆìƒí™œìˆ˜ê¸‰ì ì–´ë¥´ì‹ ì—ê²Œ ìƒí™œë¹„ë¥¼ ì›” 50ë§Œì›ì”© ì§€ì›í•˜ëŠ” ì œë„ì…ë‹ˆë‹¤.",
            "metadata": {"region": "ì„œìš¸", "file_format": "hwp", "source": "ìƒí™œë¹„ì§€ì›.hwp"}
        },
        {
            "text": "ìš”ì–‘ë³´í˜¸ì‚¬ê°€ ì£¼ 3íšŒ ë°©ë¬¸í•˜ì—¬ ëŒë´„ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            "metadata": {"region": "ë¶€ì‚°", "file_format": "pdf", "source": "ëŒë´„ì„œë¹„ìŠ¤.pdf"}
        },
        {
            "text": "ì¹˜ë§¤ ì–´ë¥´ì‹ ì„ ìœ„í•œ ì¸ì§€ê¸°ëŠ¥ í–¥ìƒ í”„ë¡œê·¸ë¨ì„ ìš´ì˜í•©ë‹ˆë‹¤.",
            "metadata": {"region": "ëŒ€êµ¬", "file_format": "txt", "source": "ì¹˜ë§¤ì˜ˆë°©.txt"}
        }
    ]
    
    try:
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        vector_store = WelfareVectorStore(
            collection_name="test_welfare_collection",
            persist_directory="./test_chroma_db"
        )
        
        print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë¬¸ì„œ ì¶”ê°€
        texts = [doc["text"] for doc in test_documents]
        metadatas = [doc["metadata"] for doc in test_documents]
        
        doc_ids = vector_store.add_documents(texts, metadatas)
        print(f"ğŸ“ {len(doc_ids)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
        
        # í†µê³„ í™•ì¸
        stats = vector_store.get_collection_stats()
        print(f"\nğŸ“Š ì»¬ë ‰ì…˜ í†µê³„:")
        print(f"  ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
        print(f"  ì¹´í…Œê³ ë¦¬ ë¶„í¬: {stats['category_distribution']}")
        print(f"  ì§€ì—­ ë¶„í¬: {stats['region_distribution']}")
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
        query = "ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì›"
        results = vector_store.similarity_search(query, k=3)
        
        for i, result in enumerate(results, 1):
            print(f"  [{i}] ìœ ì‚¬ë„: {result['similarity_score']:.3f}")
            print(f"      ë‚´ìš©: {result['document'][:50]}...")
            print(f"      ì¹´í…Œê³ ë¦¬: {result['metadata'].get('welfare_category', 'N/A')}")
        
        # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ·ï¸ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
        filter_results = vector_store.search_by_metadata(
            {"welfare_category": "ì˜ë£Œì§€ì›"}
        )
        
        for result in filter_results:
            print(f"  ì˜ë£Œì§€ì› ë¬¸ì„œ: {result['document'][:50]}...")
        
        # ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
        export_path = vector_store.export_collection("test_export.csv")
        print(f"\nğŸ’¾ ì»¬ë ‰ì…˜ ë‚´ë³´ë‚´ê¸°: {export_path}")
        
        print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()