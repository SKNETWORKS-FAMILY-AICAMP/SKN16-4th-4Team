# =========================================
# ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° ëª¨ë“ˆ
# =========================================

import os
import logging
from typing import List, Dict, Any, Optional, TypedDict, Annotated
from datetime import datetime
import json

# LangGraph ì„í¬íŠ¸ (ìµœì‹  API)
try:
    from langgraph.graph import StateGraph, END
    try:
        from langgraph.checkpoint.memory import MemorySaver
    except ImportError:
        from langgraph.checkpoint import MemorySaver
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    
    # ê¸°ë³¸ í´ë˜ìŠ¤ë“¤ ì •ì˜ (LangGraphê°€ ì—†ì„ ë•Œ)
    class StateGraph:
        def __init__(self, *args, **kwargs):
            self.nodes = {}
            self.edges = []
        
        def add_node(self, name, func):
            self.nodes[name] = func
            return self
        
        def add_edge(self, from_node, to_node):
            self.edges.append((from_node, to_node))
            return self
            
        def add_conditional_edges(self, *args, **kwargs):
            return self
        
        def compile(self, *args, **kwargs):
            return MockWorkflow(self.nodes)
    
    class END:
        pass
    
    class MemorySaver:
        def __init__(self):
            pass
    
    class MockWorkflow:
        def __init__(self, nodes):
            self.nodes = nodes
        
        def invoke(self, state, config=None):
            return state
        
        def stream(self, state, config=None):
            yield state
    
    class MemorySaver:
        def __init__(self, *args, **kwargs):
            pass

# LangChain ì„í¬íŠ¸
try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.schema import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

logger = logging.getLogger(__name__)


# ìƒíƒœ íƒ€ì… ì •ì˜
class WelfareQueryState(TypedDict):
    """ë³µì§€ ì •ì±… ì§ˆì˜ ìƒíƒœ"""
    question: str
    processed_question: str
    user_intent: str
    retrieved_documents: List[Dict]
    answer: str
    confidence_score: float
    sources: List[Dict]
    follow_up_questions: List[str]
    error_message: str
    step_history: List[str]


class ElderlyWelfareWorkflow:
    """ë…¸ì¸ë³µì§€ ì •ì±… ì „ìš© LangGraph ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, 
                 retriever=None, 
                 llm=None,
                 enable_checkpointing: bool = True):
        """
        Args:
            retriever: ë¬¸ì„œ ê²€ìƒ‰ê¸°
            llm: ì–¸ì–´ ëª¨ë¸
            enable_checkpointing: ì²´í¬í¬ì¸íŠ¸ í™œì„±í™” ì—¬ë¶€
        """
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraph")
        
        self.retriever = retriever
        self.enable_checkpointing = enable_checkpointing
        
        # LLM ì„¤ì •
        if llm is None and LANGCHAIN_AVAILABLE:
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.1,
                    openai_api_key=api_key
                )
            else:
                self.llm = None
                logger.warning("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            self.llm = llm
        
        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±
        self.graph = self._create_workflow()
        
        # ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° (ì„ íƒì )
        self.checkpointer = MemorySaver() if enable_checkpointing else None
        
        # ì»´íŒŒì¼ëœ ì›Œí¬í”Œë¡œìš°
        self.compiled_workflow = self.graph.compile(checkpointer=self.checkpointer)
        
        logger.info("ë…¸ì¸ë³µì§€ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        
        # ìƒíƒœ ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(WelfareQueryState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("retrieve_documents", self._retrieve_documents)
        workflow.add_node("evaluate_relevance", self._evaluate_relevance)
        workflow.add_node("generate_answer", self._generate_answer)
        workflow.add_node("enhance_answer", self._enhance_answer)
        workflow.add_node("generate_followup", self._generate_followup)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("analyze_query")
        
        workflow.add_edge("analyze_query", "retrieve_documents")
        workflow.add_edge("retrieve_documents", "evaluate_relevance")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ (ê´€ë ¨ì„±ì— ë”°ë¥¸ ë¶„ê¸°)
        workflow.add_conditional_edges(
            "evaluate_relevance",
            self._decide_next_step,
            {
                "generate": "generate_answer",
                "retry": "retrieve_documents",
                "end": END
            }
        )
        
        workflow.add_edge("generate_answer", "enhance_answer")
        workflow.add_edge("enhance_answer", "generate_followup")
        workflow.add_edge("generate_followup", END)
        
        return workflow
    
    def _analyze_query(self, state: WelfareQueryState) -> WelfareQueryState:
        """ì§ˆì˜ ë¶„ì„ ë° ì „ì²˜ë¦¬"""
        
        question = state["question"]
        
        # ì§ˆì˜ ì „ì²˜ë¦¬
        processed_question = question.strip()
        
        # ì˜ë„ ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        intent = self._classify_intent(processed_question)
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["processed_question"] = processed_question
        state["user_intent"] = intent
        state["step_history"] = state.get("step_history", []) + ["analyze_query"]
        
        logger.debug(f"ì§ˆì˜ ë¶„ì„ ì™„ë£Œ: ì˜ë„={intent}")
        
        return state
    
    def _classify_intent(self, question: str) -> str:
        """ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜"""
        
        question_lower = question.lower()
        
        # ì˜ë„ ë¶„ë¥˜ í‚¤ì›Œë“œ
        intent_keywords = {
            "medical_support": ["ì˜ë£Œë¹„", "ì§„ë£Œë¹„", "ì¹˜ë£Œ", "ë³‘ì›", "ì•½", "ìˆ˜ìˆ "],
            "care_service": ["ëŒë´„", "ê°„ë³‘", "ìš”ì–‘", "ì¼€ì–´", "ë°©ë¬¸"],
            "living_support": ["ìƒí™œë¹„", "ìˆ˜ë‹¹", "ì—°ê¸ˆ", "ê¸‰ì—¬", "ë³´ì¡°ê¸ˆ"],
            "application_process": ["ì‹ ì²­", "ë°©ë²•", "ì ˆì°¨", "ì„œë¥˜", "ì¡°ê±´"],
            "housing_support": ["ì£¼ê±°", "ì„ëŒ€", "ì£¼íƒ", "ê±°ì£¼"],
            "transportation": ["êµí†µ", "ì´ë™", "ë²„ìŠ¤", "ì§€í•˜ì² ", "í• ì¸"],
            "general_inquiry": ["ì •ë³´", "ì•ˆë‚´", "ë¬¸ì˜", "ì•Œë ¤", "ì„¤ëª…"]
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì˜ë„ ë¶„ë¥˜
        for intent, keywords in intent_keywords.items():
            if any(keyword in question_lower for keyword in keywords):
                return intent
        
        return "general_inquiry"
    
    def _retrieve_documents(self, state: WelfareQueryState) -> WelfareQueryState:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        
        processed_question = state["processed_question"]
        
        try:
            if self.retriever:
                # ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©
                docs = self.retriever.get_relevant_documents(processed_question)
                
                # Document ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                retrieved_docs = []
                for doc in docs:
                    doc_dict = {
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "source": doc.metadata.get("source", "Unknown")
                    }
                    retrieved_docs.append(doc_dict)
                
            else:
                # ë”ë¯¸ ë¬¸ì„œ (í…ŒìŠ¤íŠ¸ìš©)
                retrieved_docs = [
                    {
                        "content": "ë…¸ì¸ë³µì§€ ê´€ë ¨ ê¸°ë³¸ ì •ë³´ì…ë‹ˆë‹¤.",
                        "metadata": {"source": "default"},
                        "source": "default"
                    }
                ]
            
            state["retrieved_documents"] = retrieved_docs
            state["step_history"].append("retrieve_documents")
            
            logger.debug(f"ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ: {len(retrieved_docs)}ê°œ ë¬¸ì„œ")
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            state["error_message"] = f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            state["retrieved_documents"] = []
        
        return state
    
    def _evaluate_relevance(self, state: WelfareQueryState) -> WelfareQueryState:
        """ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€"""
        
        retrieved_docs = state["retrieved_documents"]
        processed_question = state["processed_question"]
        
        if not retrieved_docs:
            state["confidence_score"] = 0.0
            state["step_history"].append("evaluate_relevance")
            return state
        
        # ê°„ë‹¨í•œ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ ê¸°ë°˜)
        question_words = set(processed_question.lower().split())
        
        relevance_scores = []
        for doc in retrieved_docs:
            content_words = set(doc["content"].lower().split())
            
            # ìì¹´ë“œ ìœ ì‚¬ë„
            intersection = len(question_words.intersection(content_words))
            union = len(question_words.union(content_words))
            jaccard_score = intersection / union if union > 0 else 0
            
            relevance_scores.append(jaccard_score)
            doc["relevance_score"] = jaccard_score
        
        # ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜
        confidence_score = max(relevance_scores) if relevance_scores else 0.0
        
        state["confidence_score"] = confidence_score
        state["step_history"].append("evaluate_relevance")
        
        logger.debug(f"ê´€ë ¨ì„± í‰ê°€ ì™„ë£Œ: ì‹ ë¢°ë„={confidence_score:.3f}")
        
        return state
    
    def _decide_next_step(self, state: WelfareQueryState) -> str:
        """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        
        confidence = state["confidence_score"]
        retrieved_docs = state["retrieved_documents"]
        
        # ê²°ì • ë¡œì§
        if not retrieved_docs:
            return "end"  # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        elif confidence < 0.1:
            return "retry"  # ê´€ë ¨ì„±ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¬ê²€ìƒ‰
        else:
            return "generate"  # ë‹µë³€ ìƒì„±
    
    def _generate_answer(self, state: WelfareQueryState) -> WelfareQueryState:
        """ë‹µë³€ ìƒì„±"""
        
        processed_question = state["processed_question"]
        retrieved_docs = state["retrieved_documents"]
        user_intent = state["user_intent"]
        
        try:
            if self.llm and retrieved_docs:
                # LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
                answer = self._generate_llm_answer(processed_question, retrieved_docs, user_intent)
            else:
                # ê¸°ë³¸ ë‹µë³€ ìƒì„±
                answer = self._generate_basic_answer(processed_question, retrieved_docs)
            
            state["answer"] = answer
            state["step_history"].append("generate_answer")
            
            logger.debug("ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            state["answer"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            state["error_message"] = f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}"
        
        return state
    
    def _generate_llm_answer(self, question: str, docs: List[Dict], intent: str) -> str:
        """LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] {doc['content']}" 
            for i, doc in enumerate(docs[:3])
        ])
        
        # ì˜ë„ë³„ ë§ì¶¤ í”„ë¡¬í”„íŠ¸
        intent_instructions = {
            "medical_support": "ì˜ë£Œë¹„ ì§€ì› ê´€ë ¨ ì •í™•í•œ ê¸ˆì•¡, ëŒ€ìƒì ì¡°ê±´, ì‹ ì²­ ë°©ë²•ì„ í¬í•¨í•´ ë‹µë³€í•˜ì„¸ìš”.",
            "care_service": "ëŒë´„ ì„œë¹„ìŠ¤ì˜ ì¢…ë¥˜, ì´ìš© ë°©ë²•, ë¹„ìš© ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.",
            "living_support": "ìƒí™œë¹„ ì§€ì›ì˜ ê¸ˆì•¡, ì§€ê¸‰ ì¡°ê±´, ì‹ ì²­ ì ˆì°¨ë¥¼ ëª…í™•íˆ ì•ˆë‚´í•˜ì„¸ìš”.",
            "application_process": "ì‹ ì²­ì— í•„ìš”í•œ ì„œë¥˜, ì ˆì°¨, ì ‘ìˆ˜ì²˜ë¥¼ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.",
            "general_inquiry": "ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ì•ˆë‚´í•˜ì„¸ìš”."
        }
        
        instruction = intent_instructions.get(intent, intent_instructions["general_inquiry"])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""ë‹¹ì‹ ì€ ë…¸ì¸ë³µì§€ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. {instruction}
2. ì œê³µëœ ë¬¸ì„œ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì œê³µëœ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
5. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì¡°ê±´ì´ ìˆìœ¼ë©´ ì •í™•íˆ ì–¸ê¸‰í•˜ì„¸ìš”

ì°¸ê³  ë¬¸ì„œ:
{context}"""),
            ("human", question)
        ])
        
        # LLM í˜¸ì¶œ
        response = self.llm.invoke(prompt.format_messages())
        return response.content
    
    def _generate_basic_answer(self, question: str, docs: List[Dict]) -> str:
        """ê¸°ë³¸ ë‹µë³€ ìƒì„± (LLM ì—†ì´)"""
        
        if not docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
        best_doc = max(docs, key=lambda d: d.get("relevance_score", 0))
        
        return f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì„¸ìš”:

{best_doc['content'][:500]}

ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
    
    def _enhance_answer(self, state: WelfareQueryState) -> WelfareQueryState:
        """ë‹µë³€ í–¥ìƒ ë° ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€"""
        
        retrieved_docs = state["retrieved_documents"]
        
        # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
        sources = []
        for i, doc in enumerate(retrieved_docs[:3]):
            source_info = {
                "index": i + 1,
                "content": doc["content"][:200] + "...",
                "source": doc["source"],
                "relevance_score": doc.get("relevance_score", 0)
            }
            sources.append(source_info)
        
        state["sources"] = sources
        state["step_history"].append("enhance_answer")
        
        logger.debug(f"ë‹µë³€ í–¥ìƒ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")
        
        return state
    
    def _generate_followup(self, state: WelfareQueryState) -> WelfareQueryState:
        """í›„ì† ì§ˆë¬¸ ìƒì„±"""
        
        user_intent = state["user_intent"]
        answer = state["answer"]
        
        # ì˜ë„ë³„ í›„ì† ì§ˆë¬¸ í…œí”Œë¦¿
        followup_templates = {
            "medical_support": [
                "ì˜ë£Œë¹„ ì§€ì› ì‹ ì²­ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                "ë‹¤ë¥¸ ì˜ë£Œ ê´€ë ¨ ë³µì§€ ì œë„ë„ ì•Œì•„ë³´ì‹œê² ì–´ìš”?",
                "ì˜ë£Œë¹„ ì§€ì› ëŒ€ìƒì ì¡°ê±´ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
            ],
            "care_service": [
                "ëŒë´„ ì„œë¹„ìŠ¤ ì‹ ì²­ ì ˆì°¨ê°€ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                "ìš”ì–‘ë“±ê¸‰ íŒì •ì— ëŒ€í•´ ì•Œì•„ë³´ì‹œê² ì–´ìš”?",
                "ë‹¤ë¥¸ ëŒë´„ ê´€ë ¨ ì„œë¹„ìŠ¤ë„ í™•ì¸í•˜ì‹œê² ì–´ìš”?"
            ],
            "living_support": [
                "ìƒí™œë¹„ ì§€ì› ì‹ ì²­ ë°©ë²•ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                "ë‹¤ë¥¸ ê²½ì œì  ì§€ì› ì œë„ë„ ì•Œì•„ë³´ì‹œê² ì–´ìš”?",
                "ìˆ˜ê¸‰ ì¡°ê±´ì„ ìì„¸íˆ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
            ],
            "general_inquiry": [
                "ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?",
                "ë‹¤ë¥¸ ë³µì§€ ì œë„ë„ ì•Œì•„ë³´ì‹œê² ì–´ìš”?",
                "ì‹ ì²­ ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"
            ]
        }
        
        followup_questions = followup_templates.get(
            user_intent, 
            followup_templates["general_inquiry"]
        )
        
        state["follow_up_questions"] = followup_questions
        state["step_history"].append("generate_followup")
        
        logger.debug("í›„ì† ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
        
        return state
    
    def process_query(self, question: str, config: Dict = None) -> Dict[str, Any]:
        """ì§ˆì˜ ì²˜ë¦¬ (ì›Œí¬í”Œë¡œìš° ì‹¤í–‰)"""
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = WelfareQueryState(
            question=question,
            processed_question="",
            user_intent="",
            retrieved_documents=[],
            answer="",
            confidence_score=0.0,
            sources=[],
            follow_up_questions=[],
            error_message="",
            step_history=[]
        )
        
        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            if config and self.checkpointer:
                result = self.compiled_workflow.invoke(initial_state, config)
            else:
                result = self.compiled_workflow.invoke(initial_state)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            response = {
                "question": result["question"],
                "answer": result["answer"],
                "confidence_score": result["confidence_score"],
                "sources": result["sources"],
                "follow_up_questions": result["follow_up_questions"],
                "user_intent": result["user_intent"],
                "step_history": result["step_history"],
                "success": True,
                "timestamp": datetime.now().isoformat()
            }
            
            if result["error_message"]:
                response["error_message"] = result["error_message"]
                response["success"] = False
            
            return response
            
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {
                "question": question,
                "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def get_workflow_visualization(self) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‹œê°í™” (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë²„ì „)"""
        
        workflow_steps = [
            "1. ì§ˆì˜ ë¶„ì„ (analyze_query)",
            "2. ë¬¸ì„œ ê²€ìƒ‰ (retrieve_documents)",
            "3. ê´€ë ¨ì„± í‰ê°€ (evaluate_relevance)",
            "4. ë‹µë³€ ìƒì„± (generate_answer)",
            "5. ë‹µë³€ í–¥ìƒ (enhance_answer)",
            "6. í›„ì† ì§ˆë¬¸ ìƒì„± (generate_followup)"
        ]
        
        return "\n".join(workflow_steps)


def main():
    """LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    if not LANGGRAPH_AVAILABLE:
        print("âŒ LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install langgraph")
        return
    
    try:
        # ì›Œí¬í”Œë¡œìš° ìƒì„±
        print("ğŸ“ ì›Œí¬í”Œë¡œìš° ìƒì„±...")
        workflow = ElderlyWelfareWorkflow(
            retriever=None,  # í…ŒìŠ¤íŠ¸ìš© None
            llm=None,        # OpenAI API ì—†ì´ í…ŒìŠ¤íŠ¸
            enable_checkpointing=False
        )
        
        # ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
        print(f"\nğŸ“Š ì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
        print(workflow.get_workflow_visualization())
        
        # ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print(f"\nâ“ ì§ˆì˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        question = "65ì„¸ ì´ìƒ ë…¸ì¸ ì˜ë£Œë¹„ ì§€ì›ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"
        
        result = workflow.process_query(question)
        
        print(f"\nğŸ“‹ ì²˜ë¦¬ ê²°ê³¼:")
        print(f"  ì§ˆë¬¸: {result['question']}")
        print(f"  ë‹µë³€: {result['answer'][:200]}...")
        print(f"  ì„±ê³µ: {result['success']}")
        print(f"  ì˜ë„: {result['user_intent']}")
        print(f"  ì‹ ë¢°ë„: {result['confidence_score']:.3f}")
        print(f"  ì²˜ë¦¬ ë‹¨ê³„: {' -> '.join(result['step_history'])}")
        
        if result['follow_up_questions']:
            print(f"  í›„ì† ì§ˆë¬¸: {result['follow_up_questions'][0]}")
        
        print(f"\nâœ… LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()