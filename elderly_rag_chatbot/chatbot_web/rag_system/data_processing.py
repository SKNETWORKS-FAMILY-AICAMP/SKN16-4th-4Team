"""
ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ

ë³µì§€ ì •ì±… ë¬¸ì„œ ë¡œë”©, ì„ë² ë”©, ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬
"""

import os

# ChromaDB í…”ë ˆë©”íŠ¸ë¦¬ ì™„ì „ ë¹„í™œì„±í™” (import ì „ì— ë¨¼ì € ì„¤ì •)
os.environ['CHROMA_TELEMETRY'] = '0'
os.environ['ANONYMIZED_TELEMETRY'] = 'False'
os.environ['CHROMA_ANONYMIZED_TELEMETRY'] = 'False'

import logging
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path
import pickle

try:
    import PyPDF2
    from PyPDF2 import PdfReader
except ImportError:
    PyPDF2 = None
    PdfReader = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

try:
    import pdfplumber
except ImportError:
    pdfplumber = None

try:
    from PIL import Image
    import io
except ImportError:
    Image = None
    io = None

try:
    import pytesseract
    # Windowsì—ì„œ Tesseract ê²½ë¡œ ì„¤ì •
    import platform
    if platform.system() == 'Windows':
        tesseract_paths = [
            r'C:\Program Files\Tesseract-OCR\tesseract.exe',
            r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
        ]
        for path in tesseract_paths:
            if os.path.exists(path):
                pytesseract.pytesseract.tesseract_cmd = path
                print(f"[INFO] Tesseract ê²½ë¡œ ì„¤ì •: {path}")
                break
except ImportError:
    pytesseract = None

try:
    import chromadb
    from chromadb.config import Settings
except ImportError:
    chromadb = None

try:
    import openai
except ImportError:
    openai = None

# Django í˜¸í™˜ì„ ìœ„í•´ rag_config ì‚¬ìš©
from . import rag_config

DATA_DIR = str(rag_config.MAIN_DATA_DIR.parent)  # data í´ë”
EMBEDDING_CACHE_DIR = str(rag_config.BASE_DIR / 'embeddings_cache')
CHROMA_DB_DIR = str(rag_config.MAIN_CHROMA_DB_DIR)
CHUNK_SIZE = rag_config.CHUNK_SIZE
CHUNK_OVERLAP = rag_config.CHUNK_OVERLAP
OPENAI_EMB_MODEL = rag_config.OPENAI_EMB_MODEL
get_openai_api_key = rag_config.get_openai_api_key

logger = logging.getLogger(__name__)

class WelfareDocumentLoader:
    """ë³µì§€ ì •ì±… ë¬¸ì„œ ë¡œë”"""

    def __init__(self, welfare_dir: str = None):
        """
        Args:
            welfare_dir: ë³µì§€ ë°ì´í„° ë””ë ‰í† ë¦¬ (ì˜ˆ: data/ë³µì§€ë¡œ ë˜ëŠ” data/ë³µì§€ë¡œ - ë³µì‚¬ë³¸)
        """
        if welfare_dir:
            self.welfare_dir = welfare_dir
        else:
            self.welfare_dir = os.path.join(DATA_DIR, "ë³µì§€ë¡œ")
        self.data_dir = os.path.dirname(self.welfare_dir)

        # ì‚¬ìš©í•  ë°ì´í„° ë””ë ‰í† ë¦¬ ë¡œê·¸
        logger.info(f"WelfareDocumentLoader ì´ˆê¸°í™”: {self.welfare_dir}")
        
    def load_welfare_documents(self) -> List[Dict[str, Any]]:
        """ë³µì§€ ì •ì±… ë¬¸ì„œ ë¡œë“œ"""
        logger.info("ë³µì§€ ì •ì±… ë¬¸ì„œ ë¡œë”© ì‹œì‘")
        documents = []
        
        if not os.path.exists(self.welfare_dir):
            logger.error(f"ë³µì§€ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.welfare_dir}")
            return documents
        
        # ì§€ì—­ë³„ ë¬¸ì„œ ë¡œë”©
        regions = ['ê²½ë‚¨', 'ê²½ë¶', 'ëŒ€êµ¬', 'ë¶€ì‚°', 'ì „êµ­', 'ì „ë‚¨', 'ì „ë¶']
        
        for region in regions:
            region_path = os.path.join(self.welfare_dir, region)
            if os.path.exists(region_path):
                logger.info(f"ì§€ì—­ '{region}' ë¬¸ì„œ ë¡œë”© ì¤‘...")
                region_docs = self._load_region_documents(region, region_path)
                documents.extend(region_docs)
                logger.info(f"ì§€ì—­ '{region}': {len(region_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")
        
        logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return documents
    
    def _load_region_documents(self, region: str, region_path: str) -> List[Dict[str, Any]]:
        """ì§€ì—­ë³„ ë¬¸ì„œ ë¡œë”© (PDFë§Œ)"""
        documents = []

        # PDF íŒŒì¼ ë¡œë”©
        pdf_files = list(Path(region_path).glob("*.pdf"))
        # pdf í•˜ìœ„í´ë”ë„ í™•ì¸
        pdf_dir = os.path.join(region_path, "pdf")
        if os.path.exists(pdf_dir):
            pdf_files.extend(list(Path(pdf_dir).glob("*.pdf")))

        if pdf_files:
            pdf_docs = self._load_pdf_files(region, pdf_files)
            documents.extend(pdf_docs)

        return documents
    
    def _load_pdf_files(self, region: str, pdf_files: List[Path]) -> List[Dict[str, Any]]:
        """PDF íŒŒì¼ ë¡œë”© (OCR í¬í•¨)"""
        documents = []

        if not fitz and not pdfplumber and not PdfReader:
            logger.warning("PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - PDF íŒŒì¼ ê±´ë„ˆëœ€")
            return documents

        for file_path in pdf_files:
            try:
                # íŒŒì¼ í¬ê¸° í™•ì¸ (0 ë°”ì´íŠ¸ ë˜ëŠ” ë„ˆë¬´ ì‘ì€ íŒŒì¼ ìŠ¤í‚µ)
                file_size = file_path.stat().st_size
                if file_size < 100:
                    logger.warning(f"âŠ˜ PDF íŒŒì¼ ë„ˆë¬´ ì‘ìŒ: {file_path.name} ({file_size} bytes) - ê±´ë„ˆëœ€")
                    continue

                text = self._extract_pdf_text_with_ocr(str(file_path))
                if text and len(text.strip()) > 50:
                    doc = {
                        'content': text,
                        'metadata': {
                            'source': str(file_path),
                            'filename': file_path.name,
                            'region': region,
                            'file_type': 'pdf'
                        }
                    }
                    documents.append(doc)
                    logger.info(f"âœ“ PDF ë¡œë“œ ì„±ê³µ: {file_path.name} ({len(text)} ë¬¸ì)")
                else:
                    logger.warning(f"âŠ˜ PDF í…ìŠ¤íŠ¸ ë¶€ì¡± (ì†ìƒ ê°€ëŠ¥): {file_path.name} ({len(text) if text else 0} ë¬¸ì) - ê±´ë„ˆëœ€")
            except Exception as e:
                logger.warning(f"âŠ˜ PDF íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {file_path.name} - {str(e)[:100]}")
                continue  # ì—ëŸ¬ ë°œìƒ ì‹œ ë‹¤ìŒ íŒŒì¼ë¡œ ê³„ì†

        return documents

    def _extract_pdf_text_with_ocr(self, pdf_path: str) -> str:
        """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ OCR í´ë°±)"""

        # 1ë‹¨ê³„: ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (PyMuPDF â†’ pdfplumber â†’ PyPDF2)
        text = self._extract_pdf_text(pdf_path)

        # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ì¶”ì¶œë˜ì—ˆìœ¼ë©´ ë°˜í™˜
        if text and len(text.strip()) > 100:
            return text

        # 2ë‹¨ê³„: OCR ì‹œë„ (í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ìœ¼ë©´)
        logger.info(f"ì¼ë°˜ ì¶”ì¶œ ì‹¤íŒ¨ ({len(text) if text else 0}ì), OCR ì‹œë„: {Path(pdf_path).name}")
        ocr_text = self._extract_pdf_with_ocr(pdf_path)

        if ocr_text and len(ocr_text.strip()) > len(text if text else ""):
            return ocr_text

        return text if text else ""

    def _extract_pdf_text(self, pdf_path: str) -> str:
        """PDF ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF â†’ pdfplumber â†’ PyPDF2)"""

        # ë°©ë²• 1: PyMuPDF (fitz) - ê°€ì¥ ê°•ë ¥í•˜ê³  ë¹ ë¦„
        if fitz:
            try:
                text = ""
                doc = fitz.open(pdf_path)
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text and page_text.strip():
                        text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n{page_text}"
                doc.close()

                if text.strip():
                    return text.strip()
            except Exception as e:
                logger.warning(f"PyMuPDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ë°©ë²• 2: pdfplumber - í‘œ ì¶”ì¶œì— ê°•í•¨
        if pdfplumber:
            try:
                text = ""
                with pdfplumber.open(pdf_path) as pdf:
                    for page_num, page in enumerate(pdf.pages):
                        page_text = page.extract_text()
                        if page_text and page_text.strip():
                            text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n{page_text}"

                if text.strip():
                    return text.strip()
            except Exception as e:
                logger.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        # ë°©ë²• 3: PyPDF2 - í´ë°±
        if PdfReader:
            try:
                with open(pdf_path, 'rb') as file:
                    reader = PdfReader(file, strict=False)
                    text = ""

                    for page_num, page in enumerate(reader.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                text += f"\n--- í˜ì´ì§€ {page_num + 1} ---\n{page_text}"
                        except Exception as e:
                            logger.warning(f"í˜ì´ì§€ {page_num + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            continue

                    if text.strip():
                        return text.strip()
            except Exception as e:
                logger.warning(f"PyPDF2 ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return ""

    def _extract_pdf_with_ocr(self, pdf_path: str) -> str:
        """PDF OCR ì¶”ì¶œ (ì´ë¯¸ì§€ ê¸°ë°˜ PDFìš©)"""

        if not fitz or not pytesseract or not Image:
            logger.warning("OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (pymupdf, pytesseract, Pillow í•„ìš”)")
            return ""

        try:
            text = ""
            doc = fitz.open(pdf_path)

            for page_num in range(len(doc)):
                page = doc[page_num]

                # í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (200 DPIë¡œ ë‚®ì¶¤ - ì†ë„ í–¥ìƒ)
                pix = page.get_pixmap(dpi=200)
                img_data = pix.tobytes("png")

                # PIL Imageë¡œ ë³€í™˜
                image = Image.open(io.BytesIO(img_data))

                # Tesseract OCR (í•œêµ­ì–´ + ì˜ì–´)
                try:
                    # timeout ì„¤ì • ë° ì„ì‹œ íŒŒì¼ ì²˜ë¦¬ ê°œì„ 
                    page_text = pytesseract.image_to_string(
                        image,
                        lang='kor+eng',
                        timeout=30  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                    )
                    if page_text and page_text.strip():
                        text += f"\n--- í˜ì´ì§€ {page_num + 1} (OCR) ---\n{page_text}"
                        logger.info(f"  OCR ì„±ê³µ: í˜ì´ì§€ {page_num + 1} ({len(page_text)}ì)")
                except Exception as e:
                    logger.warning(f"  OCR ì‹¤íŒ¨: í˜ì´ì§€ {page_num + 1} - {e}")
                finally:
                    # ì´ë¯¸ì§€ ë©”ëª¨ë¦¬ í•´ì œ
                    image.close()
                    del image
                    del pix

            doc.close()
            return text.strip()

        except Exception as e:
            logger.error(f"OCR ì „ì²´ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_document(self, document: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì²­í‚¹ - ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„í• """
        content = document['content']
        metadata = document['metadata']

        chunks = []

        # 1ë‹¨ê³„: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
        import re
        sentences = re.split(r'[\n]+', content)

        current_chunk = ""
        chunk_sentences = []

        for sentence in sentences:
            sentence = sentence.strip()

            # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
            if len(sentence) < 10:
                continue

            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€
            potential_chunk = current_chunk + "\n" + sentence if current_chunk else sentence

            # ì²­í¬ í¬ê¸° í™•ì¸
            if len(potential_chunk) <= CHUNK_SIZE:
                # ì•„ì§ ì—¬ìœ  ìˆìŒ - ë¬¸ì¥ ì¶”ê°€
                current_chunk = potential_chunk
                chunk_sentences.append(sentence)
            else:
                # ì²­í¬ê°€ ë„ˆë¬´ ì»¤ì§ - í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk and len(current_chunk.strip()) >= 50:
                    chunk = {
                        'content': current_chunk.strip(),
                        'metadata': {
                            **metadata,
                            'chunk_id': len(chunks),
                            'sentence_count': len(chunk_sentences)
                        }
                    }
                    chunks.append(chunk)

                # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
                current_chunk = sentence
                chunk_sentences = [sentence]

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk and len(current_chunk.strip()) >= 50:
            chunk = {
                'content': current_chunk.strip(),
                'metadata': {
                    **metadata,
                    'chunk_id': len(chunks),
                    'sentence_count': len(chunk_sentences)
                }
            }
            chunks.append(chunk)

        return chunks

class WelfareEmbedder:
    """ì„ë² ë”© ìƒì„±ê¸°"""
    
    def __init__(self):
        self.model_name = OPENAI_EMB_MODEL
        self.api_key = get_openai_api_key()
        self.client = None
        
        if self.api_key and openai:
            try:
                # ì•ˆì „í•œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    timeout=30.0,
                    max_retries=3
                )
            except Exception as e:
                logger.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None
        
        # ì„ë² ë”© ìºì‹œ
        self.cache_dir = EMBEDDING_CACHE_DIR
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def embed_text(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        if not self.client:
            logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return None
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key(text)
        cached_embedding = self._load_from_cache(cache_key)
        if cached_embedding is not None:
            return cached_embedding
        
        try:
            response = self.client.embeddings.create(
                model=self.model_name,
                input=text
            )
            embedding = response.data[0].embedding
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache(cache_key, embedding)
            
            return embedding
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def embed_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ëª©ë¡ ì„ë² ë”©"""
        embedded_docs = []
        
        for doc in documents:
            embedding = self.embed_text(doc['content'])
            if embedding:
                doc['embedding'] = embedding
                embedded_docs.append(doc)
        
        return embedded_docs
    
    def _get_cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(f"{self.model_name}:{text}".encode()).hexdigest()
    
    def _load_from_cache(self, cache_key: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ë¡œë“œ"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ {cache_key}: {e}")
        return None
    
    def _save_to_cache(self, cache_key: str, embedding: List[float]):
        """ìºì‹œì— ì €ì¥"""
        cache_file = os.path.join(self.cache_dir, f"{cache_key}.pkl")
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {cache_key}: {e}")

class WelfareVectorStore:
    """ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(self, chroma_db_dir: str = None):
        """
        Args:
            chroma_db_dir: ChromaDB ë””ë ‰í† ë¦¬ (ì˜ˆ: chroma_db/main ë˜ëŠ” chroma_db/validation)
        """
        self.db_dir = chroma_db_dir if chroma_db_dir else CHROMA_DB_DIR
        self.collection_name = "welfare_documents"
        self.client = None
        self.collection = None

        logger.info(f"WelfareVectorStore ì´ˆê¸°í™”: {self.db_dir}")

        if chromadb:
            self._initialize_chroma()
    
    def _initialize_chroma(self):
        """ChromaDB ì´ˆê¸°í™” (ìˆœìˆ˜ SQLite ë²¡í„° ì €ì¥ì†Œë¡œ ëŒ€ì²´)"""
        try:
            # ìˆœìˆ˜ SQLiteë¡œ ìì²´ ë²¡í„° DB êµ¬í˜„
            os.makedirs(self.db_dir, exist_ok=True)
            self.db_path = os.path.join(self.db_dir, 'vector_store.db')

            import sqlite3
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS documents (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    metadata TEXT,
                    embedding BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON documents(id)')

            conn.commit()
            conn.close()

            logger.info(f"âœ… ìˆœìˆ˜ SQLite ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”: {self.db_path}")
            self.collection = True  # Flagë¡œ ì‚¬ìš©

        except Exception as e:
            logger.error(f"ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def add_documents(self, documents: List[Dict[str, Any]], embedder: WelfareEmbedder):
        """ë¬¸ì„œ ì¶”ê°€"""
        if not self.collection:
            logger.error("ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return

        logger.info("ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€ ì¤‘...")

        # ë¬¸ì„œ ì²­í‚¹
        all_chunks = []
        # ì„ì‹œ ë¡œë” ìƒì„± (ì²­í‚¹ì€ ê²½ë¡œ ë¬´ê´€)
        temp_loader = WelfareDocumentLoader()

        for doc in documents:
            chunks = temp_loader.chunk_document(doc)
            all_chunks.extend(chunks)
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = 100
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i + batch_size]
            self._add_batch(batch, embedder)
        
        logger.info(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ")
    
    def _add_batch(self, batch: List[Dict[str, Any]], embedder: WelfareEmbedder):
        """ë°°ì¹˜ ì¶”ê°€ (ìˆœìˆ˜ SQLite)"""
        import sqlite3
        import json

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            for chunk in batch:
                # ê³ ìœ  ID ìƒì„±
                filename = chunk['metadata'].get('filename', 'unknown')
                chunk_id = chunk['metadata'].get('chunk_id', 0)
                content_hash = hashlib.md5(chunk['content'].encode()).hexdigest()[:8]
                unique_id = f"{filename}_{chunk_id}_{content_hash}"

                # ì„ë² ë”© ìƒì„±
                embedding = embedder.embed_text(chunk['content'])
                if not embedding:
                    logger.warning(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {unique_id}")
                    continue

                # ì„ë² ë”©ì„ pickleë¡œ ì§ë ¬í™”
                embedding_blob = pickle.dumps(embedding)

                # ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
                metadata_json = json.dumps(chunk['metadata'], ensure_ascii=False)

                # INSERT OR REPLACE
                cursor.execute('''
                    INSERT OR REPLACE INTO documents (id, content, metadata, embedding)
                    VALUES (?, ?, ?, ?)
                ''', (unique_id, chunk['content'], metadata_json, embedding_blob))

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì¶”ê°€ ì‹¤íŒ¨: {e}", exc_info=True)
    
    def similarity_search(self, query: str, embedder: WelfareEmbedder, k: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ (ìˆœìˆ˜ SQLite ë²¡í„° ì €ì¥ì†Œ)"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        logger.info(f"ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘: '{query}'")
        query_embedding = embedder.embed_text(query)

        if not query_embedding:
            logger.error("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return []

        logger.info(f"âœ… ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")

        try:
            import sqlite3
            import json
            import numpy as np

            logger.info(f"ğŸ” SQLiteì—ì„œ ë°ì´í„° ì½ê¸°: {self.db_path}")

            if not os.path.exists(self.db_path):
                logger.error(f"âŒ SQLite íŒŒì¼ ì—†ìŒ: {self.db_path}")
                return []

            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            cursor.execute("SELECT id, content, metadata, embedding FROM documents")
            rows = cursor.fetchall()
            conn.close()

            logger.info(f"ğŸ’¾ ê°€ì ¸ì˜¨ ë¬¸ì„œ ìˆ˜: {len(rows)}")

            if not rows:
                logger.warning("âš ï¸ ë¬¸ì„œ ë°ì´í„° ì—†ìŒ")
                return []

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            query_vec = np.array(query_embedding)
            similarities = []

            for row in rows:
                doc_id, content, metadata_json, embedding_blob = row

                if embedding_blob:
                    try:
                        # pickleì—ì„œ ì„ë² ë”© ë³µì›
                        doc_embedding = pickle.loads(embedding_blob)
                        doc_vec = np.array(doc_embedding)

                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                        cosine_sim = np.dot(query_vec, doc_vec) / (
                            np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)
                        )

                        # ë©”íƒ€ë°ì´í„° íŒŒì‹±
                        try:
                            metadata = json.loads(metadata_json) if metadata_json else {}
                        except:
                            metadata = {}

                        similarities.append({
                            'content': content if content else '',
                            'metadata': metadata,
                            'distance': 1.0 - cosine_sim,
                            'similarity': cosine_sim
                        })
                    except Exception as parse_error:
                        logger.warning(f"ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨ ({doc_id}): {parse_error}")
                        continue

            # ìœ ì‚¬ë„ìˆœ ì •ë ¬
            similarities.sort(key=lambda x: x['similarity'], reverse=True)

            # ìƒìœ„ kê°œ
            top_results = similarities[:k]

            logger.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(top_results)}ê°œ")

            # ìƒìœ„ 3ê°œ ë¡œê¹…
            for i, result in enumerate(top_results[:3]):
                filename = result['metadata'].get('filename', 'unknown')
                region = result['metadata'].get('region', 'unknown')
                sim = result['similarity']
                logger.info(f"  ê²°ê³¼ {i+1}: {filename} (ì§€ì—­: {region}, ìœ ì‚¬ë„: {sim:.4f})")

            logger.info(f"âœ… ì´ {len(top_results)}ê°œ ë¬¸ì„œ ë°˜í™˜")
            return top_results

        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
            return []
    
    def get_document_count(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ ë°˜í™˜"""
        try:
            if not os.path.exists(self.db_path):
                return 0

            import sqlite3
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM documents")
            count = cursor.fetchone()[0]
            conn.close()
            return count
        except:
            return 0


class DataProcessor:
    """ë°ì´í„° í”„ë¡œì„¸ì„œ - ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

    def __init__(self, data_dir: str, chroma_db_dir: str = None):
        """
        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ (ì˜ˆ: data/ë³µì§€ë¡œ ë˜ëŠ” data/ë³µì§€ë¡œ - ë³µì‚¬ë³¸)
            chroma_db_dir: ChromaDB ë””ë ‰í† ë¦¬ (ì˜ˆ: chroma_db/main ë˜ëŠ” chroma_db/validation)
        """
        self.data_dir = data_dir
        self.loader = WelfareDocumentLoader(welfare_dir=data_dir)  # ê²½ë¡œ ì „ë‹¬
        self.embedder = WelfareEmbedder()
        self.vector_store = WelfareVectorStore(chroma_db_dir=chroma_db_dir)  # ChromaDB ê²½ë¡œ ì „ë‹¬

        logger.info(f"DataProcessor initialized with data_dir: {data_dir}, chroma_db_dir: {chroma_db_dir}")

    def build_vector_store(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•"""
        logger.info("=== ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì‹œì‘ ===")

        # ë¬¸ì„œ ë¡œë“œ
        documents = self.loader.load_welfare_documents()
        if not documents:
            logger.error("ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return {
                "status": "error",
                "message": "ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨"
            }

        logger.info(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        self.vector_store.add_documents(documents, self.embedder)

        final_count = self.vector_store.get_document_count()
        logger.info(f"=== ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶• ì™„ë£Œ: {final_count}ê°œ ì²­í¬ ===")

        return {
            "status": "success",
            "documents_loaded": len(documents),
            "chunks_created": final_count,
            "message": "ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ"
        }