"""
RAG ê¸°ëŠ¥ ëª¨ë“ˆ

í•µì‹¬ RAG ë¡œì§ê³¼ AI ì—ì´ì „íŠ¸ êµ¬í˜„
"""

import logging
from typing import List, Dict, Any, Optional
import re
import random
import csv
from pathlib import Path

try:
    import openai
except ImportError:
    openai = None

# Django í˜¸í™˜ì„ ìœ„í•´ rag_config ì‚¬ìš©
from . import rag_config
from .advanced_rag import AdvancedRAGPipeline
from .policy_extractor import EnhancedPolicyFormatter

OPENAI_MODEL = rag_config.OPENAI_MODEL
TEMPERATURE = rag_config.TEMPERATURE
MAX_TOKENS = rag_config.MAX_TOKENS
TOP_K_RESULTS = rag_config.TOP_K_RESULTS
get_openai_api_key = rag_config.get_openai_api_key

logger = logging.getLogger(__name__)


# 17ê°œ ì‹œë„ ì§€ì—­ ë§¤í•‘ (UserProfile ì§€ì—­ëª… â†’ ë°ì´í„° í´ë” ì§€ì—­ëª…)
REGION_MAPPING = {
    'ì„œìš¸íŠ¹ë³„ì‹œ': 'ì„œìš¸',
    'ë¶€ì‚°ê´‘ì—­ì‹œ': 'ë¶€ì‚°',
    'ëŒ€êµ¬ê´‘ì—­ì‹œ': 'ëŒ€êµ¬',
    'ì¸ì²œê´‘ì—­ì‹œ': 'ì¸ì²œ',
    'ê´‘ì£¼ê´‘ì—­ì‹œ': 'ê´‘ì£¼',
    'ëŒ€ì „ê´‘ì—­ì‹œ': 'ëŒ€ì „',
    'ìš¸ì‚°ê´‘ì—­ì‹œ': 'ìš¸ì‚°',
    'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ': 'ì„¸ì¢…',
    'ê²½ê¸°ë„': 'ê²½ê¸°',
    'ê°•ì›íŠ¹ë³„ìì¹˜ë„': 'ê°•ì›',
    'ì¶©ì²­ë¶ë„': 'ì¶©ë¶',
    'ì¶©ì²­ë‚¨ë„': 'ì¶©ë‚¨',
    'ì „ë¶íŠ¹ë³„ìì¹˜ë„': 'ì „ë¶',
    'ì „ë¼ë‚¨ë„': 'ì „ë‚¨',
    'ê²½ìƒë¶ë„': 'ê²½ë¶',
    'ê²½ìƒë‚¨ë„': 'ê²½ë‚¨',
    'ì œì£¼íŠ¹ë³„ìì¹˜ë„': 'ì œì£¼',
}

# ë°ì´í„° í´ë” ì§€ì—­ëª… â†’ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì§ˆë¬¸ì—ì„œ ì§€ì—­ ì¶”ì¶œìš©)
REGION_KEYWORDS = {
    'ì„œìš¸': ['ì„œìš¸'],
    'ë¶€ì‚°': ['ë¶€ì‚°'],
    'ëŒ€êµ¬': ['ëŒ€êµ¬'],
    'ì¸ì²œ': ['ì¸ì²œ'],
    'ê´‘ì£¼': ['ê´‘ì£¼'],
    'ëŒ€ì „': ['ëŒ€ì „'],
    'ìš¸ì‚°': ['ìš¸ì‚°'],
    'ì„¸ì¢…': ['ì„¸ì¢…'],
    'ê²½ê¸°': ['ê²½ê¸°'],
    'ê°•ì›': ['ê°•ì›'],
    'ì¶©ë¶': ['ì¶©ë¶', 'ì¶©ì²­ë¶ë„'],
    'ì¶©ë‚¨': ['ì¶©ë‚¨', 'ì¶©ì²­ë‚¨ë„'],
    'ì „ë¶': ['ì „ë¶', 'ì „ë¶íŠ¹ë³„ìì¹˜ë„'],
    'ì „ë‚¨': ['ì „ë‚¨', 'ì „ë¼ë‚¨ë„'],
    'ê²½ë¶': ['ê²½ë¶', 'ê²½ìƒë¶ë„'],
    'ê²½ë‚¨': ['ê²½ë‚¨', 'ê²½ìƒë‚¨ë„'],
    'ì œì£¼': ['ì œì£¼'],
}


def map_user_region_to_data_folder(user_region: str) -> Optional[str]:
    """
    ì‚¬ìš©ì í”„ë¡œí•„ì˜ ì§€ì—­ëª…ì„ ë°ì´í„° í´ë” ì§€ì—­ëª…ìœ¼ë¡œ ë³€í™˜

    Args:
        user_region: UserProfileì˜ ì§€ì—­ (ì˜ˆ: 'ê²½ìƒë¶ë„', 'ë¶€ì‚°ê´‘ì—­ì‹œ')

    Returns:
        ë°ì´í„° í´ë” ì§€ì—­ëª… (ì˜ˆ: 'ê²½ë¶', 'ë¶€ì‚°') ë˜ëŠ” None
    """
    return REGION_MAPPING.get(user_region)


def extract_region_from_question(question: str) -> Optional[str]:
    """
    ì§ˆë¬¸ì—ì„œ ì§€ì—­ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸

    Returns:
        ì¶”ì¶œëœ ì§€ì—­ëª… (ë°ì´í„° í´ë” í˜•ì‹) ë˜ëŠ” None
    """
    for region, keywords in REGION_KEYWORDS.items():
        for keyword in keywords:
            if keyword in question:
                return region
    return None


class WelfareRAGChain:
    """ë…¸ì¸ë³µì§€ RAG ì²´ì¸"""

    def __init__(self, vector_store, embedder):
        self.vector_store = vector_store
        self.embedder = embedder
        self.api_key = get_openai_api_key()
        self.client = None

        # Advanced RAG Pipeline ì´ˆê¸°í™”
        self.advanced_rag = AdvancedRAGPipeline()
        logger.info("ğŸš€ Advanced RAG Pipeline í™œì„±í™”")

        # Enhanced Policy Formatter ì´ˆê¸°í™”
        self.policy_formatter = EnhancedPolicyFormatter()
        logger.info("âœ¨ Enhanced Policy Formatter í™œì„±í™”")

        # Policy URL ë§¤í•‘ ë¡œë“œ
        self.policy_url_mapping = self._load_policy_url_mapping()
        logger.info(f"ğŸ“‹ Policy URL ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.policy_url_mapping)}ê°œ")

        if self.api_key and openai:
            try:
                # ì•ˆì „í•œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    timeout=30.0,
                    max_retries=3
                )
            except Exception as e:
                logger.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.client = None

    def _load_policy_url_mapping(self) -> Dict[str, str]:
        """policy_mapping.csv íŒŒì¼ì—ì„œ ì •ì±…ëª… -> URL ë§¤í•‘ ë¡œë“œ (ì§€ì—­+ì •ì±…ëª… ì¡°í•©ë§Œ ì‚¬ìš©)"""
        mapping = {}
        csv_path = Path(__file__).parent.parent.parent / 'policy_mapping.csv'

        try:
            with open(csv_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # xlsx í˜•ì‹: ì‹œë„ëª…, ê´€ì‹¬ì£¼ì œ, ì •ì±…ëª…, ë§í¬
                    policy_name = row.get('ì •ì±…ëª…', '').strip()
                    url = row.get('ë§í¬', '').strip()
                    region = row.get('ì‹œë„ëª…', '').strip()

                    # ì§€ì—­+ì •ì±…ëª… ì¡°í•©ìœ¼ë¡œë§Œ ë§¤í•‘ (ì •í™•í•œ ë§¤ì¹­ì„ ìœ„í•´)
                    if policy_name and url and region:
                        mapping[f"{region}_{policy_name}"] = url

            logger.info(f"âœ… CSV ë¡œë“œ ì„±ê³µ: {csv_path} ({len(mapping)}ê°œ ë§¤í•‘)")
            logger.info(f"ğŸ“‹ ë§¤í•‘ ì˜ˆì‹œ: {list(mapping.keys())[:3]}")
        except FileNotFoundError:
            logger.warning(f"âš ï¸ policy_mapping.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        except Exception as e:
            logger.error(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")

        return mapping

    def process_question(self, question: str, user_region: Optional[str] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”” ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{question}'")
        logger.info(f"{'='*60}")

        # 1ë‹¨ê³„: ì˜ë„ ë¶„ì„
        intent = self._analyze_intent(question)
        logger.info(f"ğŸ¯ ì˜ë„ ë¶„ì„ ê²°ê³¼: '{intent}'")

        if intent == "casual_conversation":
            logger.info("ğŸ’¬ ìºì£¼ì–¼ ëŒ€í™”ë¡œ ì²˜ë¦¬ ì¤‘...")
            return self._generate_casual_response(question)
        elif intent == "irrelevant":
            logger.info("ğŸš« ë¬´ê´€í•œ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ ì¤‘...")
            return self._generate_irrelevant_response(question)

        logger.info("ğŸ“‹ ë³µì§€ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ - Advanced RAG ì‹œì‘")

        # 2ë‹¨ê³„: ì§€ì—­ ìš°ì„ ìˆœìœ„ ê²°ì •
        # ìš°ì„ ìˆœìœ„ 1: ì‚¬ìš©ì í”„ë¡œí•„ì˜ 'ì‚¬ëŠ” ì§€ì—­' (ìµœìš°ì„ !)
        # ìš°ì„ ìˆœìœ„ 2: ì§ˆë¬¸ì— ëª…ì‹œëœ ì§€ì—­ (í”„ë¡œí•„ ì—†ì„ ë•Œë§Œ)
        # ìš°ì„ ìˆœìœ„ 3: ì „êµ­ + ëœë¤ ì§€ì—­ (ë‘˜ ë‹¤ ì—†ì„ ë•Œ)

        target_region = None
        question_region = extract_region_from_question(question)

        if user_region:
            # 1ìˆœìœ„: ì‚¬ìš©ì í”„ë¡œí•„ì˜ ì§€ì—­ (ì§ˆë¬¸ì— ì§€ì—­ì´ ìˆì–´ë„ ë¬´ì‹œ!)
            target_region = map_user_region_to_data_folder(user_region)
            if question_region and question_region != target_region:
                logger.info(f"âš ï¸ ì§ˆë¬¸ì— '{question_region}' ì§€ì—­ì´ ìˆì§€ë§Œ, ì‚¬ìš©ì í”„ë¡œí•„ '{target_region}' ìš°ì„  ì ìš©")
            else:
                logger.info(f"âœ… ì‚¬ìš©ì í”„ë¡œí•„ ì§€ì—­: {user_region} â†’ {target_region}")
        elif question_region:
            # 2ìˆœìœ„: ì§ˆë¬¸ì—ì„œ ì§€ì—­ ì¶”ì¶œ (í”„ë¡œí•„ ì—†ì„ ë•Œë§Œ)
            target_region = question_region
            logger.info(f"âœ… ì§ˆë¬¸ì—ì„œ ì§€ì—­ ì¶”ì¶œ: {target_region}")
        else:
            # 3ìˆœìœ„: ë‘˜ ë‹¤ ì—†ìŒ - Noneìœ¼ë¡œ ìœ ì§€ (ë‚˜ì¤‘ì— ëœë¤ ì§€ì—­ ì²˜ë¦¬)
            logger.info(f"â„¹ï¸ ì§€ì—­ ì •ë³´ ì—†ìŒ - ì „êµ­ + ëœë¤ ì§€ì—­ìœ¼ë¡œ ë‹µë³€")

        # 3-5ë‹¨ê³„: Advanced RAG Pipeline ì‹¤í–‰
        rag_result = self.advanced_rag.process(
            query=question,
            vector_store=self.vector_store,
            embedder=self.embedder,
            top_k=TOP_K_RESULTS
        )

        if rag_result['documents']:
            # ì§€ì—­ ê¸°ë°˜ ë¬¸ì„œ ì •ë ¬ (Advanced RAG í›„)
            relevant_docs = rag_result['documents']
            if target_region:
                relevant_docs = self._sort_docs_by_region(relevant_docs, target_region)
                logger.info(f"ì§€ì—­ '{target_region}' ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬ ì™„ë£Œ")

            # í…ìŠ¤íŠ¸ ì •ì œ ë° ìµœì¢… ë‹µë³€ ìƒì„±
            return self._generate_welfare_response(question, relevant_docs, target_region)
        else:
            logger.warning(f"ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ - ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
            return self._generate_no_docs_response(question)

    def _analyze_intent(self, question: str) -> str:
        """ì˜ë„ ë¶„ì„ - ë³µì§€ ì •ì±… ê´€ë ¨ ì§ˆë¬¸ë§Œ ì²˜ë¦¬"""
        # 1ë‹¨ê³„: ë³µì§€ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ê°€ì¥ ì¤‘ìš”!)
        welfare_keywords = [
            'ë³µì§€', 'ë…¸ì¸', 'ì–´ë¥´ì‹ ', 'ê³ ë ¹ì', 'ì—°ê¸ˆ', 'ì˜ë£Œë¹„', 'ê±´ê°•ë³´í—˜',
            'ì¥ê¸°ìš”ì–‘', 'ëŒë´„', 'ê²½ë¡œë‹¹', 'í‹€ë‹ˆ', 'ë³´ì²­ê¸°', 'ëª©ìš•', 'ì´ë¯¸ìš©',
            'ì‹ì‚¬ë°°ë‹¬', 'íš¨ë„ìˆ˜ë‹¹', 'ì¥ìˆ˜ì¶•í•˜', 'ì°¸ì „ìœ ê³µì', 'ë³´í›ˆ',
            'ê¸°ì´ˆìƒí™œìˆ˜ê¸‰', 'ì˜ë£Œê¸‰ì—¬', 'êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥', 'ì°¨ìƒìœ„',
            'ê¸´ê¸‰ë³µì§€', 'ì§€ì›', 'ìˆ˜ë‹¹', 'ì„œë¹„ìŠ¤', 'ì •ì±…', 'í˜œíƒ', 'ì‹ ì²­',
            'ìš”ì–‘', 'ê°„ë³‘', 'ì¬í™œ', 'ê±´ê°•', 'ì˜ë£Œ', 'ë³‘ì›', 'ì•½', 'ì¹˜ë£Œ',
            'ê²½ë¡œìš°ëŒ€', 'í• ì¸', 'êµí†µ', 'ë²„ìŠ¤', 'ì§€í•˜ì² ', 'ë³µì§€ê´€',
            'ë…¸ì¸íšŒê´€', 'ì–‘ë¡œì›', 'ìš”ì–‘ì›', 'ê¸‰ì—¬', 'ë³´ì¡°ê¸ˆ'
        ]

        # ë³µì§€ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ welfare_inquiryë¡œ ì²˜ë¦¬
        for keyword in welfare_keywords:
            if keyword in question:
                return "welfare_inquiry"

        # 2ë‹¨ê³„: ì¸ì‚¬/ê°ì‚¬ íŒ¨í„´ (ê°„ë‹¨í•œ ì‘ë‹µ)
        greeting_patterns = [
            r'^ì•ˆë…•$|^ì•ˆë…•í•˜ì„¸ìš”$|^ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ$',
            r'^hi$|^hello$|^í•˜ì´$',
            r'ê³ ë§ˆì›Œ|ê°ì‚¬í•©ë‹ˆë‹¤|ê³ ë§™ìŠµë‹ˆë‹¤'
        ]

        for pattern in greeting_patterns:
            if re.search(pattern, question.strip(), re.IGNORECASE):
                return "casual_conversation"

        # 3ë‹¨ê³„: ë¬´ê´€í•œ/ìš•ì„¤/ê³µê²©ì  ì§ˆë¬¸ íŒ¨í„´
        irrelevant_patterns = [
            # ìš•ì„¤ ë° ë¹„í•˜ í‘œí˜„
            r'ë°”ë³´|ë©ì²­|ë˜¥|ì“°ë ˆê¸°|êº¼ì ¸|ë‹¥ì³|ì‹œë„|ì£½ì–´',
            r'fuck|shit|damn|stupid|idiot',

            # ì „í˜€ ë¬´ê´€í•œ ì£¼ì œ
            r'ìë™ì°¨|ì—¬í–‰|ì½”ë”©|í”„ë¡œê·¸ë˜ë°|ìŠ¤í¬ì¸ |ì¶•êµ¬|ì•¼êµ¬',
            r'ì£¼ì‹|íˆ¬ì|ë¶€ë™ì‚°|ì‡¼í•‘|íŒ¨ì…˜|ë·°í‹°',
            r'ìˆ˜í•™|ë¬¼ë¦¬|í™”í•™|ì˜ì–´|ì¼ë³¸ì–´|ì¤‘êµ­ì–´',
            r'ì•„ì´ìŠ¤í¬ë¦¼|í”¼ì|ì¹˜í‚¨|ë§¥ì£¼|ìŒë£Œ|ì¹´í˜',
            r'ê²Œì„|ì˜í™”|ë“œë¼ë§ˆ|ìŒì•…|ì—°ì˜ˆì¸|ì•„ì´ëŒ',
            r'ì»´í“¨í„°|í•¸ë“œí°|ìŠ¤ë§ˆíŠ¸í°|íƒœë¸”ë¦¿|ë…¸íŠ¸ë¶',
            r'ë‚ ì”¨|ê¸°ì˜¨|ë¹„|ëˆˆ|íƒœí’',
            r'ì •ì¹˜|ì„ ê±°|ëŒ€í†µë ¹|êµ­íšŒì˜ì›',

            # í…ŒìŠ¤íŠ¸ì„± ì§ˆë¬¸
            r'í…ŒìŠ¤íŠ¸|test|ã…‹|ã…|ã„±|ã„´|123|abc'
        ]

        for pattern in irrelevant_patterns:
            if re.search(pattern, question, re.IGNORECASE):
                return "irrelevant"

        # 4ë‹¨ê³„: ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš°
        if len(question.strip()) < 2:
            return "irrelevant"

        # 5ë‹¨ê³„: ê¸°ë³¸ê°’ - ë¬´ê´€í•œ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ (ë³µì§€ í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê±°ë¶€)
        # ì´ì „ì—ëŠ” welfare_inquiryë¡œ ì²˜ë¦¬í–ˆì§€ë§Œ, ì´ì œëŠ” irrelevantë¡œ ì²˜ë¦¬
        return "irrelevant"

    def _generate_casual_response(self, question: str) -> Dict[str, Any]:
        """ìºì£¼ì–¼ ëŒ€í™” ì‘ë‹µ"""
        responses = {
            "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ë…¸ì¸ë³µì§€ ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.",
            "thanks": "ì²œë§Œì—ìš”! ë” ê¶ê¸ˆí•œ ë³µì§€ ì •ì±…ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.",
            "general": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë…¸ì¸ë³µì§€ ì •ì±… ìƒë‹´ì„ ë„ì™€ë“œë¦¬ëŠ” AIì…ë‹ˆë‹¤. ë³µì§€ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ê²Œìš”!"
        }

        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        if any(word in question for word in ['ì•ˆë…•', 'í•˜ì´', 'hi', 'hello']):
            response_text = responses["greeting"]
        elif any(word in question for word in ['ê³ ë§ˆì›Œ', 'ê°ì‚¬', 'ì˜í–ˆì–´']):
            response_text = responses["thanks"]
        else:
            response_text = responses["general"]

        return {
            "answer": response_text,
            "intent": "casual_conversation",
            "sources": [],
            "confidence": "high"
        }

    def _generate_irrelevant_response(self, question: str) -> Dict[str, Any]:
        """ë¬´ê´€í•œ ì§ˆë¬¸ ì‘ë‹µ"""
        response = """ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” ë…¸ì¸ë³µì§€ ì •ì±… ì „ë¬¸ ìƒë‹´ AIì…ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë…¸ì¸ë³µì§€ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
â€¢ ê¸°ì´ˆì—°ê¸ˆ ë° ë…¸ì¸ìˆ˜ë‹¹
â€¢ ì˜ë£Œë¹„ ì§€ì› (ê±´ê°•ë³´í—˜ë£Œ, ì¥ê¸°ìš”ì–‘ë³´í—˜ë£Œ)
â€¢ ë…¸ì¸ëŒë´„ì„œë¹„ìŠ¤ (ì‹ì‚¬ë°°ë‹¬, ëª©ìš•ì„œë¹„ìŠ¤ ë“±)
â€¢ ë³´ê±´ì˜ë£Œ ì§€ì› (í‹€ë‹ˆ, ë³´ì²­ê¸°, ê±´ê°•ê²€ì§„ ë“±)
â€¢ ì°¸ì „ìœ ê³µì ì˜ˆìš°
â€¢ ì§€ì—­ë³„ ë…¸ì¸ë³µì§€ ì •ì±…

ë³µì§€ ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!"""

        return {
            "answer": response,
            "intent": "irrelevant",
            "sources": [],
            "confidence": "high"
        }

    def _sort_docs_by_region(self, docs: List[Dict], target_region: str) -> List[Dict]:
        """
        ì§€ì—­ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ ì •ë ¬

        ìš°ì„ ìˆœìœ„:
        1. 'ì „êµ­' ë¬¸ì„œ (ìµœìš°ì„ ! - ëª¨ë“  ì§€ì—­ì— ì ìš©ë˜ëŠ” ì •ì±…)
        2. target_regionê³¼ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ
        3. ë‹¤ë¥¸ ì§€ì—­ ë¬¸ì„œ
        """
        nationwide_docs = []
        target_docs = []
        other_docs = []

        for doc in docs:
            region = doc['metadata'].get('region', '')
            if region == 'ì „êµ­':
                nationwide_docs.append(doc)
            elif region == target_region:
                target_docs.append(doc)
            else:
                other_docs.append(doc)

        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬: ì „êµ­ â†’ í•´ë‹¹ ì§€ì—­ â†’ ë‹¤ë¥¸ ì§€ì—­
        return nationwide_docs + target_docs + other_docs

    def _clean_pdf_text(self, text: str) -> str:
        """PDF í…ìŠ¤íŠ¸ ì •ì œ - ë©”íƒ€ë°ì´í„° ì œê±° ë° ì •ë¦¬"""
        # 1. í˜ì´ì§€ êµ¬ë¶„ì„  ì œê±°
        text = re.sub(r'---\s*í˜ì´ì§€\s*\d+\s*---', '', text)

        # 2. ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)  # 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œë¡œ
        text = re.sub(r'[ \t]{2,}', ' ', text)  # 2ê°œ ì´ìƒ ê³µë°± â†’ 1ê°œë¡œ

        # 3. ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = text.replace('ï½¥', 'Â·')
        text = text.replace('â€‘', '-')

        # 4. ë²•ë ¹/ë¬¸ì„œ ë©”íƒ€ì •ë³´ íŒ¨í„´ ì œê±°
        text = re.sub(r'ë²•ì œì²˜\s*\d+', '', text)
        text = re.sub(r'êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°', '', text)
        text = re.sub(r'\[ì‹œí–‰\s*[\d\.\s]+\]', '', text)
        text = re.sub(r'\[ë²•ë¥ \s*ì œ\d+í˜¸.*?\]', '', text)
        text = re.sub(r'ë°œ\s*ê°„\s*ë“±\s*ë¡\s*ë²ˆ\s*í˜¸', '', text)
        text = re.sub(r'\d{2}-\d{7}-\d{6}-\d{2}', '', text)  # ë“±ë¡ë²ˆí˜¸

        # 5. ì—°ë½ì²˜ íŒ¨í„´ ê°„ì†Œí™”
        text = re.sub(r'ë³´ê±´ë³µì§€ë¶€\s*\(.*?\)\s*\d{3}-\d{3}-\d{4}', 'ë³´ê±´ë³µì§€ë¶€', text)

        # 6. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì •ë¦¬
        text = re.sub(r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', 'â€¢', text)

        # 7. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        return text

    def _extract_key_sentences(self, text: str, max_sentences: int = 3, question: str = "") -> str:
        """í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ - ë³µì§€ ê´€ë ¨ ë‚´ìš© + ì§ˆë¬¸ ê´€ë ¨ì„± ì²´í¬"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = re.split(r'[\n]+', text)

        # í•„í„°ë§: ì •ì±…/ë³µì§€ ê´€ë ¨ í•µì‹¬ ë¬¸ì¥ë§Œ
        key_sentences = []
        priority_keywords = [
            'ì§€ì›', 'ê¸‰ì—¬', 'í˜œíƒ', 'ì‹ ì²­', 'ëŒ€ìƒ', 'ì¡°ê±´', 'ê¸°ì¤€',
            'ì„œë¹„ìŠ¤', 'ì •ì±…', 'ë³µì§€', 'ë…¸ì¸', 'ì–´ë¥´ì‹ ', 'ìˆ˜ë‹¹', 'ì—°ê¸ˆ',
            'ì˜ë£Œ', 'ê±´ê°•', 'ìš”ì–‘', 'ëŒë´„', 'ë³´ì¡°', 'í• ì¸', 'ì œê³µ',
            'ì‚¬ì—…', 'í”„ë¡œê·¸ë¨', 'êµìœ¡', 'í™œë™', 'ë³´í—˜', 'ì¥ê¸°ìš”ì–‘',
            'ë¬´ë£Œ', 'ê°ë©´', 'ìš”ê¸ˆ', 'êµí†µ', 'ì‹œì„¤', 'ì´ìš©', 'ê²½ë¡œ'
        ]

        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = []
        if question:
            # 2ê¸€ì ì´ìƒ í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£a-zA-Z]{2,}', question)
            question_keywords = [w for w in words if w not in ['ëˆ„ê°€', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ']]

        # ëª…í™•í•˜ê²Œ ì œì™¸í•  ë©”íƒ€ì •ë³´ì™€ ë²•ë¥  ì œëª©
        exclude_keywords = [
            'í˜ì´ì§€', 'ë²•ì œì²˜', 'êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°', 'ë°œê°„ë“±ë¡ë²ˆí˜¸',
            'Ministry', 'www.', 'http://', 'https://',
            'ì œ', 'ì¡°', 'í•­', 'í˜¸'  # ë²•ë¥  ì¡°í•­
        ]

        # ì œì™¸í•  íŒ¨í„´
        exclude_patterns = [
            r'^ì œ\d+ì¡°',  # "ì œ30ì¡°"ë¡œ ì‹œì‘
            r'^\d+\.',  # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¡°í•­
            r'ë²•ë¥ .*?ì œ\d+í˜¸',  # ë²•ë¥  ì œxxí˜¸
            r'ì´\s*ë²•ì€.*?ì‹œí–‰í•œë‹¤',  # ë²•ë¥  ì‹œí–‰ì¼
            r'ë³´ìƒì²­êµ¬ì„œ.*?ì¦ëª…ì„œë¥˜',  # ë²•ë¥  ì ˆì°¨
            r'ê·€í•˜ì˜.*?ì§€ì›ë‚´ìš©',  # ì„œì‹ ì œëª©
        ]

        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ íŒ¨í„´
        incomplete_patterns = [
            r'^[ê°€-í£]{1,2}ì˜\s',  # "ì˜", "ì²´ì˜" ë“±ìœ¼ë¡œ ì‹œì‘
            r'í• \s*ìˆ˜\s*ìˆ$',  # "í•  ìˆ˜ ìˆ"ìœ¼ë¡œ ëë‚¨
            r'^[ã„±-ã…ã…-ã…£]{1}',  # ììŒ/ëª¨ìŒìœ¼ë¡œ ì‹œì‘
        ]

        for sentence in sentences:
            sentence = sentence.strip()

            # 1. ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            if len(sentence) < 15:
                continue

            # 2. ëª…í™•í•œ ë©”íƒ€ì •ë³´ ì œì™¸
            if any(meta in sentence for meta in exclude_keywords):
                continue

            # 3. ì œì™¸ íŒ¨í„´ ì²´í¬
            is_excluded = False
            for pattern in exclude_patterns:
                if re.search(pattern, sentence):
                    is_excluded = True
                    break
            if is_excluded:
                continue

            # 4. ë¶ˆì™„ì „ íŒ¨í„´ ì œì™¸
            is_incomplete = False
            for pattern in incomplete_patterns:
                if re.search(pattern, sentence):
                    is_incomplete = True
                    break
            if is_incomplete:
                continue

            # 5. ë³µì§€ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
            has_priority_keyword = any(keyword in sentence for keyword in priority_keywords)

            # 6. ì§ˆë¬¸ ê´€ë ¨ì„± ì²´í¬ (ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´)
            has_question_keyword = False
            if question_keywords:
                has_question_keyword = any(kw in sentence for kw in question_keywords)

            # ìš°ì„ ìˆœìœ„: ì§ˆë¬¸ í‚¤ì›Œë“œ + ë³µì§€ í‚¤ì›Œë“œ ëª¨ë‘ í¬í•¨
            # ë˜ëŠ” ë³µì§€ í‚¤ì›Œë“œë§Œ í¬í•¨
            if (has_question_keyword and has_priority_keyword) or has_priority_keyword:
                # ë¬¸ì¥ ë‹¤ë“¬ê¸°
                cleaned_sentence = sentence
                # ë¶ˆë¦¿í¬ì¸íŠ¸ ì œê±°
                cleaned_sentence = re.sub(r'^[â€¢Â·âˆ™â€»\-]\s*', '', cleaned_sentence)
                # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                cleaned_sentence = re.sub(r'[ã†]', ' ', cleaned_sentence)
                cleaned_sentence = re.sub(r'\s+', ' ', cleaned_sentence)

                # ì§ˆë¬¸ í‚¤ì›Œë“œ í¬í•¨ ë¬¸ì¥ì€ ì•ìª½ì— ë°°ì¹˜
                if has_question_keyword:
                    key_sentences.insert(0, (cleaned_sentence.strip(), 2))  # ë†’ì€ ìš°ì„ ìˆœìœ„
                else:
                    key_sentences.append((cleaned_sentence.strip(), 1))  # ë‚®ì€ ìš°ì„ ìˆœìœ„

            if len(key_sentences) >= max_sentences * 3:  # ì—¬ìœ ìˆê²Œ ìˆ˜ì§‘
                break

        # ìš°ì„ ìˆœìœ„ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        key_sentences.sort(key=lambda x: x[1], reverse=True)
        unique_sentences = []
        seen = set()
        for sent, priority in key_sentences:
            if sent not in seen and len(sent) >= 15:
                unique_sentences.append(sent)
                seen.add(sent)
                if len(unique_sentences) >= max_sentences:
                    break

        return '\n'.join(unique_sentences[:max_sentences])

    def _generate_welfare_response(self, question: str, relevant_docs: List[Dict], target_region: Optional[str] = None) -> Dict[str, Any]:
        """ë³µì§€ ê´€ë ¨ ì‘ë‹µ ìƒì„± - OpenAI LLM ê¸°ë°˜ + ê²€ì¦ìš© ì›ë³¸ ë²„ì „ ì œê³µ"""
        logger.info("ğŸ¤– OpenAI LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œì‘...")

        # 1ë‹¨ê³„: Enhanced Policy Formatterë¡œ ì›ë³¸ ë²„ì „ ìƒì„± (ê²€ì¦ìš©)
        formatted_response = self._generate_formatted_response_fallback(question, relevant_docs, target_region)
        formatted_answer = formatted_response.get('answer', '')

        # OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ í´ë°± (ì›ë³¸ ë²„ì „ë§Œ ë°˜í™˜)
        if not self.client:
            logger.warning("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ - ì›ë³¸ ë²„ì „ë§Œ ë°˜í™˜")
            formatted_response['answer_before_llm'] = formatted_answer
            formatted_response['answer_after_llm'] = formatted_answer
            return formatted_response

        # 2ë‹¨ê³„: ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (URL í¬í•¨)
        context_parts = []
        seen_regions = set()
        policy_urls = {}  # ì •ì±…ëª… -> URL ë§¤í•‘

        for doc in relevant_docs[:10]:  # ìµœëŒ€ 10ê°œ ë¬¸ì„œ ì‚¬ìš©
            region = doc['metadata'].get('region', 'ì§€ì—­ë¯¸ìƒ')
            filename = doc['metadata'].get('filename', 'ì •ì±…ë¬¸ì„œ.pdf')

            # Enhanced Policy Formatterë¡œ ì •ë³´ ì¶”ì¶œ
            policy_result = self.policy_formatter.format_document(doc, question=question)

            if policy_result and policy_result.get('has_content'):
                formatted_text = policy_result.get('formatted_text', '')
                policy_name = policy_result.get('policy_name', '')

                if len(formatted_text) > 20:
                    # URL ì°¾ê¸° - CSVì— ì§€ì—­+ì •ì±…ëª…ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©
                    url = None

                    if policy_name:
                        # CSVì—ì„œ region_ì •ì±…ëª… ì¡°í•© í™•ì¸ (ì •í™•í•œ ì¼ì¹˜ë§Œ í—ˆìš©)
                        region_policy_key = f"{region}_{policy_name}"

                        if region_policy_key in self.policy_url_mapping:
                            # ì •í™•íˆ ì¼ì¹˜: CSVì— í•´ë‹¹ ì§€ì—­+ì •ì±… ì¡°í•© ì¡´ì¬
                            url = self.policy_url_mapping[region_policy_key]
                            logger.info(f"âœ… URL ë§¤ì¹­ ì„±ê³µ: {region} - {policy_name}")
                        else:
                            # CSVì— ì—†ìŒ: URL ì œì™¸
                            logger.debug(f"âš ï¸ CSVì— ì—†ëŠ” ì •ì±… ì¡°í•© (URL ì œì™¸): {region} - {policy_name}")

                    # ì»¨í…ìŠ¤íŠ¸ì— URL ì •ë³´ ì¶”ê°€ (CSVì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ)
                    context_entry = f"[{region} - {filename}]\n{formatted_text}"
                    if url:
                        context_entry += f"\n[ì‹ ì²­/ìƒì„¸ì •ë³´ URL: {url}]"
                        policy_urls[policy_name or filename] = url

                    context_parts.append(context_entry)
                    seen_regions.add(region)

        if not context_parts:
            logger.warning("âš ï¸ ì¶”ì¶œëœ ì •ì±… ì •ë³´ ì—†ìŒ")
            return self._generate_no_docs_response(question)

        context = "\n\n---\n\n".join(context_parts[:8])  # ìµœëŒ€ 8ê°œ ì»¨í…ìŠ¤íŠ¸
        logger.info(f"ğŸ“„ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {len(context_parts)}ê°œ ì •ì±…, {len(context)}ì")

        # 2ë‹¨ê³„: LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_llm_prompt(question, context, target_region, seen_regions)

        # 3ë‹¨ê³„: OpenAI API í˜¸ì¶œ
        try:
            logger.info("ğŸ”„ OpenAI API í˜¸ì¶œ ì¤‘...")
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ë…¸ì¸ë³µì§€ ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì œê³µëœ ì •ì±… ìë£Œë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
ë³µì§€ ì •ì±…ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”."""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS
            )

            llm_answer = response.choices[0].message.content.strip()
            logger.info(f"âœ… LLM ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(llm_answer)}ì)")

            # 4ë‹¨ê³„: ì¶”ê°€ ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€
            answer_parts = [llm_answer]
            answer_parts.append(f"\n\nğŸ’¡ **ì¶”ê°€ ë¬¸ì˜**")
            answer_parts.append(f"\nâ€¢ ë” ìì„¸í•œ ë‚´ìš©ì€ ê´€í•  ì£¼ë¯¼ì„¼í„°ë‚˜ êµ¬ì²­ì— ë¬¸ì˜í•˜ì„¸ìš”.")
            answer_parts.append(f"\nâ€¢ ë³µì§€ë¡œ í™ˆí˜ì´ì§€(www.bokjiro.go.kr)ì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            answer_parts.append(f"\nâ€¢ ë³´ê±´ë³µì§€ìƒë‹´ì„¼í„°: 129")

            llm_final_answer = "\n".join(answer_parts)
            sources = self._extract_sources(relevant_docs)

            return {
                "answer": llm_final_answer,  # ê¸°ë³¸ ë‹µë³€ (ì¼ë°˜ ì±—ë´‡ìš©)
                "answer_before_llm": formatted_answer,  # LLM ì „ (ê²€ì¦ìš©)
                "answer_after_llm": llm_final_answer,   # LLM í›„ (ê²€ì¦ìš©)
                "intent": "welfare_inquiry",
                "sources": sources,
                "confidence": "high",
                "context_used": len(relevant_docs),
                "method": "openai_llm"
            }

        except Exception as e:
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œì—ë„ ë‘ ë²„ì „ ì œê³µ (ê°™ì€ ë‚´ìš©)
            fallback_response = self._generate_formatted_response_fallback(question, relevant_docs, target_region)
            fallback_answer = fallback_response.get('answer', '')
            fallback_response['answer_before_llm'] = fallback_answer
            fallback_response['answer_after_llm'] = fallback_answer
            return fallback_response

    def _generate_no_docs_response(self, question: str) -> Dict[str, Any]:
        """ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ ì‘ë‹µ"""
        response = f"""ì§ˆë¬¸í•˜ì‹  '{question}'ì— ëŒ€í•œ ì •í™•í•œ ì •ë³´ë¥¼ í˜„ì¬ ë³´ìœ í•œ ë³µì§€ ì •ì±… ìë£Œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë„ì›€ì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
â€¢ ê°€ê¹Œìš´ ì£¼ë¯¼ì„¼í„° ë˜ëŠ” êµ¬ì²­ ë°©ë¬¸
â€¢ ë³µì§€ë¡œ í™ˆí˜ì´ì§€(www.bokjiro.go.kr) í™•ì¸
â€¢ ë³´ê±´ë³µì§€ìƒë‹´ì„¼í„°(129) ì „í™”ìƒë‹´

ë” êµ¬ì²´ì ì¸ ë³µì§€ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""

        return {
            "answer": response,
            "intent": "welfare_inquiry",
            "sources": [],
            "confidence": "low"
        }

    def _generate_fallback_response(self, question: str, relevant_docs: List[Dict]) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ - DB ë°ì´í„° ì •ì œ ì‚¬ìš©"""
        if relevant_docs:
            answer_parts = []
            answer_parts.append(f"'{question}'ì— ëŒ€í•œ ê´€ë ¨ ë³µì§€ ì •ì±… ì •ë³´ì…ë‹ˆë‹¤.\n")

            for i, doc in enumerate(relevant_docs[:5], 1):
                region = doc['metadata'].get('region', 'ì§€ì—­ë¯¸ìƒ')
                filename = doc['metadata'].get('filename', 'ì •ì±…ë¬¸ì„œ')
                content = doc['content']

                # í…ìŠ¤íŠ¸ ì •ì œ
                cleaned_content = self._clean_pdf_text(content)
                key_content = self._extract_key_sentences(cleaned_content, max_sentences=2)

                if key_content and len(key_content) > 20:
                    answer_parts.append(f"\n**{i}. {region} ì§€ì—­ ì •ì±…**\n")
                    answer_parts.append(f"â€¢ {key_content}\n")
                    answer_parts.append(f"  (ì¶œì²˜: {filename})\n")

            answer_parts.append(f"\nğŸ’¡ **ì¶”ê°€ ì •ë³´**\n")
            answer_parts.append(f"â€¢ ë” ìì„¸í•œ ë‚´ìš©ì€ ê´€í•  ì£¼ë¯¼ì„¼í„°ë‚˜ êµ¬ì²­ì— ë¬¸ì˜í•˜ì„¸ìš”.\n")
            answer_parts.append(f"â€¢ ë³µì§€ë¡œ í™ˆí˜ì´ì§€(www.bokjiro.go.kr)ì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

            response = "".join(answer_parts)
        else:
            response = self._generate_no_docs_response(question)["answer"]

        sources = self._extract_sources(relevant_docs)

        return {
            "answer": response,
            "intent": "welfare_inquiry",
            "sources": sources,
            "confidence": "medium",
            "method": "db_cleaned_fallback"
        }

    def _build_context(self, relevant_docs: List[Dict]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []

        for i, doc in enumerate(relevant_docs[:5]):
            filename = doc['metadata'].get('filename', f'ë¬¸ì„œ{i+1}')
            region = doc['metadata'].get('region', 'ì§€ì—­ë¯¸ìƒ')
            content = doc['content'][:500]  # ê¸¸ì´ ì œí•œ

            context_parts.append(f"[{filename} - {region}ì§€ì—­]\n{content}")

        return "\n\n---\n\n".join(context_parts)

    def _build_welfare_prompt(self, question: str, context: str, target_region: Optional[str] = None) -> str:
        """ë³µì§€ ì •ì±… í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        region_instruction = ""
        if target_region:
            region_instruction = f"\n- **ì¤‘ìš”**: ì‚¬ìš©ìì˜ ê±°ì£¼ ì§€ì—­ì€ '{target_region}'ì…ë‹ˆë‹¤. ì´ ì§€ì—­ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì œê³µí•˜ë˜, ì „êµ­ ê³µí†µ ì •ì±…ë„ í•¨ê»˜ ì•ˆë‚´í•´ì£¼ì„¸ìš”."
        else:
            region_instruction = "\n- ì§€ì—­ë³„ë¡œ ì •ì±…ì´ ë‹¤ë¥¼ ê²½ìš°, ê°€ëŠ¥í•œ ëª¨ë“  ì§€ì—­ì˜ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì•ˆë‚´í•´ì£¼ì„¸ìš”."

        return f"""ë‹¤ìŒ ë³µì§€ ì •ì±… ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ã€ì •ì±… ìë£Œã€‘
{context}

ã€ì§ˆë¬¸ã€‘
{question}

ã€ë‹µë³€ ì§€ì¹¨ã€‘{region_instruction}
- ì œê³µëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì¡°ê±´ì´ ìˆë‹¤ë©´ ëª…ì‹œí•´ì£¼ì„¸ìš”
- ì‹ ì²­ ë°©ë²•ì´ë‚˜ ì ˆì°¨ê°€ ìˆë‹¤ë©´ ì•ˆë‚´í•´ì£¼ì„¸ìš”
- ì—¬ëŸ¬ ì§€ì—­ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ê° ì§€ì—­ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ê´€ë ¨ ê¸°ê´€ ë¬¸ì˜ë¥¼ ê¶Œí•´ì£¼ì„¸ìš”

ë‹µë³€:"""

    def _build_llm_prompt(self, question: str, context: str, target_region: Optional[str], seen_regions: set) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± - í™˜ê° ë°©ì§€ ê°•í™” + ì§€ì—­ í‘œì‹œ ê°•ì œ + 2ê°œ ì •ì±… ì œí•œ"""
        region_info = ""
        if target_region:
            region_info = f"\n**ì¤‘ìš”**: ì‚¬ìš©ìì˜ ê±°ì£¼ ì§€ì—­ì€ '{target_region}'ì…ë‹ˆë‹¤. ì´ ì§€ì—­ì˜ ì •ì±…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        elif seen_regions:
            regions_str = ', '.join(sorted(seen_regions))
            region_info = f"\n**ì°¸ê³ **: í˜„ì¬ ìë£Œì—ëŠ” ë‹¤ìŒ ì§€ì—­ì˜ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤: {regions_str}"

        return f"""ì•„ë˜ ë³µì§€ ì •ì±… ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ã€ì •ì±… ìë£Œã€‘
{context}

ã€ì§ˆë¬¸ã€‘
{question}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ã€‘{region_info}
1. **ì œê³µëœ ìë£Œì—ë§Œ ê¸°ë°˜**í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

2. **ì •í™•íˆ 2ê°œì˜ ì •ì±…ë§Œ ì„¤ëª…í•˜ì„¸ìš”** (í•„ìˆ˜! ì ˆëŒ€ 3ê°œ ì´ìƒ ì“°ì§€ ë§ˆì„¸ìš”):
   - ìš°ì„ ìˆœìœ„ 1: ì „êµ­ ì •ì±… 1ê°œ (ìë£Œì— 'ì „êµ­'ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ë¨¼ì €)
   - ìš°ì„ ìˆœìœ„ 2: ì‹œë„ ì§€ì—­ ì •ì±… 1ê°œ (ì‚¬ìš©ì ì§€ì—­ ìš°ì„ , ì—†ìœ¼ë©´ ìë£Œ ì¤‘ í•˜ë‚˜)
   - ì´ 2ê°œ ì´ˆê³¼í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!

3. **ì •ì±… ì„¤ëª… ì‹œ ë°˜ë“œì‹œ ì§€ì—­ì„ ëª…ì‹œí•˜ì„¸ìš”**:
   - ê° ì •ì±… ì•ì— ë°˜ë“œì‹œ ì§€ì—­ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
   - ì˜ˆì‹œ: "**ë¶€ì‚° ì§€ì—­ - ë…¸ì¸ì¼ìë¦¬ì‚¬ì—…**" ë˜ëŠ” "**ì „êµ­ - ê¸°ì´ˆì—°ê¸ˆ**"
   - ì§€ì—­ ì •ë³´ê°€ ì—†ëŠ” ì •ì±… ì„¤ëª…ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”

4. **ê¹”ë”í•œ í¬ë§·íŒ… ì‚¬ìš©**:
   - ì •ì±…ë³„ë¡œ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
   - ë¶ˆë¦¿ í¬ì¸íŠ¸(â€¢)ë‚˜ í•˜ì´í”ˆ(-) ëŒ€ì‹  **ë³¼ë“œì²´**ì™€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”
   - ê° ì„¹ì…˜(ì§€ì› ëŒ€ìƒ, ì§€ì› ë‚´ìš©, ì‹ ì²­ ë°©ë²•)ì€ ë³¼ë“œì²´ë¡œ ì œëª©ì„ í‘œì‹œí•˜ì„¸ìš”

5. ì •ì±…ëª…, ì§€ì› ëŒ€ìƒ, ì§€ì› ë‚´ìš©, ì‹ ì²­ ë°©ë²•ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.

6. êµ¬ì²´ì ì¸ ê¸ˆì•¡, ì¡°ê±´, ê¸°ì¤€ì´ ìˆë‹¤ë©´ ì •í™•íˆ ëª…ì‹œí•˜ì„¸ìš”.

7. **ì‹ ì²­ ë°©ë²• ì‘ì„± ì‹œ**: ìë£Œì— "[ì‹ ì²­/ìƒì„¸ì •ë³´ URL: ...]" í˜•ì‹ì˜ ë§í¬ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì‹ ì²­ ë°©ë²• ì„¹ì…˜ì— í¬í•¨í•˜ì„¸ìš”.
   ì˜ˆì‹œ: "**ì‹ ì²­ ë°©ë²•**: ê±°ì£¼ì§€ ì£¼ë¯¼ì„¼í„° ë°©ë¬¸ ë˜ëŠ” ì˜¨ë¼ì¸ ì‹ ì²­ ([ìƒì„¸ì •ë³´ ë³´ê¸°](URL))"

8. **ì¤‘ìš”**: ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ì •ë³´ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ì§€ì—­ì´ë‚˜ ì •ì±…ì— ëŒ€í•´ "ì—†ë‹¤"ê³  ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

9. ì œê³µëœ ìë£Œì— ìˆëŠ” ì •ì±…ë§Œ ì„¤ëª…í•˜ê³ , ìë£Œì— ì—†ëŠ” ì§€ì—­/ì •ì±…ì€ ì•„ì˜ˆ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

10. ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ã€ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ - ì •í™•íˆ 2ê°œë§Œ!ã€‘
## ì „êµ­ - êµ­ë¯¼ê¸°ì´ˆìƒí™œë³´ì¥

**ì§€ì› ëŒ€ìƒ**
ì†Œë“ì¸ì •ì•¡ì´ ê¸°ì¤€ ì¤‘ìœ„ì†Œë“ 30% ì´í•˜ì¸ ê°€êµ¬

**ì§€ì› ë‚´ìš©**
ìƒê³„ê¸‰ì—¬, ì˜ë£Œê¸‰ì—¬, ì£¼ê±°ê¸‰ì—¬, êµìœ¡ê¸‰ì—¬ ì œê³µ

**ì‹ ì²­ ë°©ë²•**
ê±°ì£¼ì§€ ì£¼ë¯¼ì„¼í„° ë°©ë¬¸ ì‹ ì²­

---

## ë¶€ì‚° ì§€ì—­ - ë…¸ì¸ì¼ìë¦¬ ë° ì‚¬íšŒí™œë™ ì§€ì›ì‚¬ì—…

**ì§€ì› ëŒ€ìƒ**
ì§€ì—­ì‚¬íšŒ ë‚´ ë…¸ì¸

**ì§€ì› ë‚´ìš©**
ê³µìµ ì„œë¹„ìŠ¤ í™œë™ ì§€ì›

**ì‹ ì²­ ë°©ë²•**
ê±°ì£¼ì§€ ë…¸ì¸ë³µì§€ê´€ ë˜ëŠ” ì£¼ë¯¼ì„¼í„° ì‹ ì²­

ë‹µë³€:"""

    def _generate_formatted_response_fallback(self, question: str, relevant_docs: List[Dict], target_region: Optional[str] = None) -> Dict[str, Any]:
        """OpenAI ì‹¤íŒ¨ ì‹œ í´ë°± - Enhanced Policy Formatter ì‚¬ìš©"""
        logger.warning("âš ï¸ í´ë°± ëª¨ë“œ: Enhanced Policy Formatter ì‚¬ìš©")

        # DBì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì„œë“¤ë¡œ ë‹µë³€ êµ¬ì„±
        answer_parts = []
        answer_parts.append(f"'{question}'ì— ëŒ€í•œ ë³µì§€ ì •ì±… ì •ë³´ë¥¼ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n")

        # ë¬¸ì„œë³„ë¡œ ì •ë³´ ì •ë¦¬
        seen_regions = set()
        doc_info_by_region = {}

        for doc in relevant_docs:
            region = doc['metadata'].get('region', 'ì§€ì—­ë¯¸ìƒ')
            filename = doc['metadata'].get('filename', 'ì •ì±…ë¬¸ì„œ.pdf')

            # Enhanced Policy Formatterë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì±… ì •ë³´ ì¶”ì¶œ
            policy_result = self.policy_formatter.format_document(doc, question=question)

            # ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if not policy_result or not policy_result.get('has_content'):
                continue

            formatted_text = policy_result.get('formatted_text')
            policy_name = policy_result.get('policy_name', '')

            if not formatted_text or len(formatted_text) < 20:
                continue

            # URL ì°¾ê¸° - CSVì— ì§€ì—­+ì •ì±…ëª…ì´ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©
            url = None
            if policy_name:
                region_policy_key = f"{region}_{policy_name}"
                if region_policy_key in self.policy_url_mapping:
                    url = self.policy_url_mapping[region_policy_key]
                    logger.info(f"âœ… [í´ë°±] URL ë§¤ì¹­ ì„±ê³µ: {region} - {policy_name}")
                else:
                    logger.debug(f"âš ï¸ [í´ë°±] CSVì— ì—†ëŠ” ì •ì±… ì¡°í•© (URL ì œì™¸): {region} - {policy_name}")

            # URLì´ ìˆìœ¼ë©´ formatted_textì— ì¶”ê°€
            if url:
                formatted_text += f"\n**ì‹ ì²­/ìƒì„¸ì •ë³´**: [ë°”ë¡œê°€ê¸°]({url})"

            # ì§€ì—­ë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í™”
            if region not in doc_info_by_region:
                doc_info_by_region[region] = []

            # ì¤‘ë³µ ë°©ì§€: ê°™ì€ ë‚´ìš©ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if formatted_text not in str(doc_info_by_region[region]):
                doc_info_by_region[region].append({
                    'filename': filename,
                    'region': region,
                    'content': formatted_text
                })

        # ===== í•µì‹¬ ë¡œì§: ì •í™•íˆ 2ê°œì˜ ì •ì±…ë§Œ í‘œì‹œ =====
        # 0ìˆœìœ„: ì „êµ­ ì •ì±… 1ê°œ (ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë¨¼ì €!)
        # 1ìˆœìœ„: ì‹œë„ ì •ì±… 1ê°œ (target_region > ëœë¤)

        policies_shown = 0  # í‘œì‹œëœ ì •ì±… ìˆ˜ ì¶”ì 

        # 0ìˆœìœ„: ì „êµ­ ì •ì±… 1ê°œ
        if 'ì „êµ­' in doc_info_by_region and policies_shown < 2:
            answer_parts.append(f"\n{doc_info_by_region['ì „êµ­'][0]['content']}\n")
            seen_regions.add('ì „êµ­')
            policies_shown += 1
            logger.info(f"âœ… [0ìˆœìœ„] ì „êµ­ ì •ì±… 1ê°œ ì¶”ê°€")

        # 1ìˆœìœ„: ì‹œë„ ì •ì±… 1ê°œ
        if policies_shown < 2:
            if target_region and target_region in doc_info_by_region:
                # 1-1ìˆœìœ„: target_region (ì‚¬ìš©ì í”„ë¡œí•„ ë˜ëŠ” ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ)
                answer_parts.append(f"\n{doc_info_by_region[target_region][0]['content']}\n")
                seen_regions.add(target_region)
                policies_shown += 1
                logger.info(f"âœ… [1ìˆœìœ„] {target_region} ì§€ì—­ ì •ì±… 1ê°œ ì¶”ê°€")
            else:
                # 1-2ìˆœìœ„: ëœë¤ ì§€ì—­ (target_regionì´ ì—†ê±°ë‚˜ ë°ì´í„° ì—†ì„ ë•Œ)
                other_regions = [r for r in doc_info_by_region.keys() if r not in seen_regions]
                if other_regions:
                    random_region = random.choice(other_regions)
                    answer_parts.append(f"\n{doc_info_by_region[random_region][0]['content']}\n")
                    policies_shown += 1
                    logger.info(f"âœ… [1ìˆœìœ„-ëœë¤] {random_region} ì§€ì—­ ì •ì±… 1ê°œ ì¶”ê°€")

        logger.info(f"ğŸ“Š ì´ {policies_shown}ê°œ ì •ì±… í‘œì‹œë¨ (ëª©í‘œ: 2ê°œ)")

        # ì¶”ê°€ ì•ˆë‚´ ë©”ì‹œì§€
        answer_parts.append(f"\nğŸ’¡ **ì¶”ê°€ ë¬¸ì˜**\n")
        answer_parts.append(f"â€¢ ë” ìì„¸í•œ ë‚´ìš©ì€ ê´€í•  ì£¼ë¯¼ì„¼í„°ë‚˜ êµ¬ì²­ì— ë¬¸ì˜í•˜ì„¸ìš”.\n")
        answer_parts.append(f"â€¢ ë³µì§€ë¡œ í™ˆí˜ì´ì§€(www.bokjiro.go.kr)ì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
        answer_parts.append(f"â€¢ ë³´ê±´ë³µì§€ìƒë‹´ì„¼í„°: 129\n")

        answer = "".join(answer_parts)

        # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        sources = self._extract_sources(relevant_docs)

        logger.info(f"âœ… í´ë°± ë‹µë³€ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(answer)}ì)")

        return {
            "answer": answer,
            "intent": "welfare_inquiry",
            "sources": sources,
            "confidence": "medium",
            "context_used": len(relevant_docs),
            "method": "enhanced_policy_extraction_fallback"
        }

    def _extract_sources(self, relevant_docs: List[Dict]) -> List[Dict[str, str]]:
        """ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        seen_files = set()

        for doc in relevant_docs:
            metadata = doc['metadata']
            filename = metadata.get('filename', 'ë¬¸ì„œëª… ì—†ìŒ')

            if filename not in seen_files:
                sources.append({
                    'filename': filename,
                    'region': metadata.get('region', 'ì§€ì—­ë¯¸ìƒ'),
                    'file_type': metadata.get('file_type', 'ìœ í˜•ë¯¸ìƒ')
                })
                seen_files.add(filename)

        return sources

class SimpleWelfareBot:
    """ë‹¨ìˆœí™”ëœ ë³µì§€ ë´‡ ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, rag_chain: WelfareRAGChain):
        self.rag_chain = rag_chain

    def get_response(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ë°˜í™˜"""
        if not question or not question.strip():
            return {
                "answer": "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                "intent": "empty",
                "sources": [],
                "confidence": "high"
            }

        # RAG ì²´ì¸ì„ í†µí•œ ì‘ë‹µ ìƒì„±
        return self.rag_chain.process_question(question.strip())

    def get_formatted_response(self, question: str) -> str:
        """í¬ë§·ëœ ì‘ë‹µ ë°˜í™˜"""
        result = self.get_response(question)

        answer = result.get("answer", "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sources = result.get("sources", [])

        # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        if sources:
            source_text = "\n\nğŸ“š **ì°¸ê³ ìë£Œ:**\n"
            for i, source in enumerate(sources[:3]):
                source_text += f"{i+1}. {source['filename']} ({source['region']})\n"
            answer += source_text

        return answer
