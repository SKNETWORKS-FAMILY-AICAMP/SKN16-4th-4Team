"""
ê³ ê¸‰ RAG ê¸°ëŠ¥ ëª¨ë“ˆ
OpenAI API ì—†ì´ ì„±ëŠ¥ í–¥ìƒ

ê¸°ë²•:
1. Query Expansion - ë™ì˜ì–´/ìœ ì‚¬ì–´ í™•ì¥
2. Reranking - TF-IDF ê¸°ë°˜ ì¬ìˆœìœ„í™”
3. Multi-Query - ì—¬ëŸ¬ ê°ë„ë¡œ ì§ˆë¬¸ ì¬êµ¬ì„±
4. Answer Synthesis - êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
"""

import logging
from typing import List, Dict, Any, Optional
from collections import Counter
import re

logger = logging.getLogger(__name__)


class QueryExpander:
    """ì§ˆë¬¸ í™•ì¥ - ë™ì˜ì–´/ìœ ì‚¬ì–´ ì¶”ê°€"""

    def __init__(self):
        # ë…¸ì¸ë³µì§€ ë„ë©”ì¸ íŠ¹í™” ë™ì˜ì–´ ì‚¬ì „
        self.synonym_dict = {
            'ë…¸ì¸': ['ì–´ë¥´ì‹ ', 'ê³ ë ¹ì', 'ê²½ë¡œ', 'ë…¸ë…„ì¸µ', 'ì‹¤ë²„'],
            'ë³µì§€': ['í˜œíƒ', 'ì§€ì›', 'ì„œë¹„ìŠ¤', 'ì •ì±…'],
            'ìˆ˜ë‹¹': ['ê¸‰ì—¬', 'ë³´ì¡°ê¸ˆ', 'ì§€ì›ê¸ˆ'],
            'ì˜ë£Œ': ['ê±´ê°•', 'ì§„ë£Œ', 'ì¹˜ë£Œ'],
            'ìš”ì–‘': ['ëŒë´„', 'ê°„ë³‘', 'ì¼€ì–´'],
            'ê²½ë¡œë‹¹': ['ë…¸ì¸íšŒê´€', 'ë³µì§€ê´€'],
            'ì—°ê¸ˆ': ['ê¸°ì´ˆì—°ê¸ˆ', 'ë…¸ë ¹ì—°ê¸ˆ'],
            'ì§€ì›': ['í˜œíƒ', 'ì„œë¹„ìŠ¤', 'ë³´ì¡°', 'ì œê³µ'],
            'ì‹ ì²­': ['ì ‘ìˆ˜', 'ë“±ë¡', 'ê°€ì…'],
            'í• ì¸': ['ê°ë©´', 'ê²½ê°', 'ìš°ëŒ€'],
            'ë³‘ì›': ['ì˜ë£Œê¸°ê´€', 'ìš”ì–‘ë³‘ì›'],
            'êµí†µ': ['ë²„ìŠ¤', 'ì§€í•˜ì² ', 'ëŒ€ì¤‘êµí†µ'],
            'ê¸´ê¸‰': ['ì‘ê¸‰', 'ê¸‰ì—¬', 'ì¦‰ì‹œ'],
        }

        # ë³µì§€ ì •ì±… ê´€ë ¨ í™•ì¥ í‚¤ì›Œë“œ
        self.expansion_keywords = {
            'ì—°ê¸ˆ': ['ê¸°ì´ˆì—°ê¸ˆ', 'êµ­ë¯¼ì—°ê¸ˆ', 'ë…¸ë ¹ì—°ê¸ˆ'],
            'ì˜ë£Œë¹„': ['ì§„ë£Œë¹„', 'ì¹˜ë£Œë¹„', 'ì•½ê°’', 'ë³‘ì›ë¹„'],
            'ëŒë´„': ['ìš”ì–‘', 'ê°„ë³‘', 'ëŒë´„ì„œë¹„ìŠ¤', 'ë°©ë¬¸ìš”ì–‘'],
            'í• ì¸': ['ê²½ë¡œìš°ëŒ€', 'í• ì¸í˜œíƒ', 'ë¬´ë£Œì´ìš©'],
        }

    def expand_query(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ì—¬ëŸ¬ ë²„ì „ ìƒì„±"""
        queries = [query]  # ì›ë³¸ ì§ˆë¬¸ í¬í•¨

        # ë™ì˜ì–´ í™•ì¥
        for original, synonyms in self.synonym_dict.items():
            if original in query:
                for synonym in synonyms[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    expanded = query.replace(original, synonym)
                    if expanded != query:
                        queries.append(expanded)

        # í™•ì¥ í‚¤ì›Œë“œ ì¶”ê°€
        for keyword, expansions in self.expansion_keywords.items():
            if keyword in query:
                for expansion in expansions[:1]:  # 1ê°œë§Œ
                    queries.append(f"{query} {expansion}")

        # ì¤‘ë³µ ì œê±°
        queries = list(dict.fromkeys(queries))

        logger.info(f"Query Expansion: {len(queries)}ê°œ ì¿¼ë¦¬ ìƒì„±")
        for i, q in enumerate(queries[:5], 1):
            logger.info(f"  {i}. {q}")

        return queries[:5]  # ìµœëŒ€ 5ê°œ


class DocumentReranker:
    """ë¬¸ì„œ ì¬ìˆœìœ„í™” - TF-IDF ê¸°ë°˜"""

    def __init__(self):
        # ë¶ˆìš©ì–´ (stopwords)
        self.stopwords = set([
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì—', 'ì˜', 'ê°€', 'ì„', 'ë¥¼',
            'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”'
        ])

    def _tokenize(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        tokens = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        # ë¶ˆìš©ì–´ ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        tokens = [t.lower() for t in tokens if t not in self.stopwords and len(t) > 1]
        return tokens

    def _calculate_tf(self, tokens: List[str]) -> Dict[str, float]:
        """TF (Term Frequency) ê³„ì‚°"""
        tf = {}
        total = len(tokens)
        counter = Counter(tokens)

        for token, count in counter.items():
            tf[token] = count / total if total > 0 else 0

        return tf

    def _calculate_idf(self, all_docs: List[List[str]]) -> Dict[str, float]:
        """IDF (Inverse Document Frequency) ê³„ì‚°"""
        import math

        idf = {}
        total_docs = len(all_docs)

        # ê° í† í°ì´ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        token_doc_count = {}
        for doc_tokens in all_docs:
            unique_tokens = set(doc_tokens)
            for token in unique_tokens:
                token_doc_count[token] = token_doc_count.get(token, 0) + 1

        # IDF ê³„ì‚°
        for token, doc_count in token_doc_count.items():
            idf[token] = math.log((total_docs + 1) / (doc_count + 1))

        return idf

    def rerank(self, query: str, documents: List[Dict], top_k: int = 5) -> List[Dict]:
        """TF-IDF ê¸°ë°˜ ì¬ìˆœìœ„í™”"""
        if not documents:
            return []

        logger.info(f"ğŸ“Š Reranking {len(documents)}ê°œ ë¬¸ì„œ...")

        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self._tokenize(query)

        # ëª¨ë“  ë¬¸ì„œ í† í°í™”
        docs_tokens = []
        for doc in documents:
            content = doc.get('content', '')
            tokens = self._tokenize(content)
            docs_tokens.append(tokens)

        # IDF ê³„ì‚°
        idf = self._calculate_idf(docs_tokens)

        # ê° ë¬¸ì„œì˜ ì ìˆ˜ ê³„ì‚°
        scores = []
        for i, doc_tokens in enumerate(docs_tokens):
            # TF ê³„ì‚°
            tf = self._calculate_tf(doc_tokens)

            # TF-IDF ì ìˆ˜ ê³„ì‚° (ì¿¼ë¦¬ í† í°ì— ëŒ€í•´ì„œë§Œ)
            score = 0.0
            for token in query_tokens:
                if token in tf:
                    tfidf = tf[token] * idf.get(token, 0)
                    score += tfidf

            # ê¸°ì¡´ ìœ ì‚¬ë„ì™€ ê²°í•© (ê°€ì¤‘ì¹˜ 7:3)
            original_similarity = documents[i].get('similarity', 0.5)
            combined_score = 0.7 * original_similarity + 0.3 * min(score, 1.0)

            scores.append((i, combined_score))

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        scores.sort(key=lambda x: x[1], reverse=True)

        # ìƒìœ„ kê°œ ë°˜í™˜
        reranked = []
        for idx, score in scores[:top_k]:
            doc = documents[idx].copy()
            doc['rerank_score'] = score
            reranked.append(doc)

        logger.info(f"âœ… Reranking ì™„ë£Œ: ìƒìœ„ {len(reranked)}ê°œ ì„ íƒ")
        for i, doc in enumerate(reranked[:3], 1):
            filename = doc['metadata'].get('filename', 'unknown')
            rerank_score = doc.get('rerank_score', 0)
            logger.info(f"  {i}. {filename} (score: {rerank_score:.4f})")

        return reranked


class MultiQueryGenerator:
    """Multi-Query RAG - ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°ë„ë¡œ ì¬êµ¬ì„±"""

    def __init__(self):
        # ì§ˆë¬¸ ì¬êµ¬ì„± í…œí”Œë¦¿
        self.query_templates = {
            'ì •ì˜': '{keyword}ì´ë€ ë¬´ì—‡ì¸ê°€?',
            'ëŒ€ìƒ': '{keyword}ì˜ ëŒ€ìƒìëŠ” ëˆ„êµ¬ì¸ê°€?',
            'ì‹ ì²­': '{keyword}ì€ ì–´ë–»ê²Œ ì‹ ì²­í•˜ë‚˜?',
            'í˜œíƒ': '{keyword}ì˜ í˜œíƒì€ ë¬´ì—‡ì¸ê°€?',
            'ì¡°ê±´': '{keyword}ì˜ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€?',
        }

    def extract_keywords(self, query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë³µì§€ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        welfare_keywords = [
            'ê¸°ì´ˆì—°ê¸ˆ', 'ë…¸ì¸ìˆ˜ë‹¹', 'ì˜ë£Œë¹„', 'ìš”ì–‘', 'ëŒë´„', 'ê²½ë¡œë‹¹',
            'ê¸´ê¸‰ë³µì§€', 'ì¥ê¸°ìš”ì–‘', 'ê±´ê°•ë³´í—˜', 'í‹€ë‹ˆ', 'ë³´ì²­ê¸°',
            'ë³µì§€ì •ì±…', 'ë…¸ì¸ë³µì§€', 'ê²½ë¡œìš°ëŒ€', 'í• ì¸'
        ]

        keywords = []
        for keyword in welfare_keywords:
            if keyword in query:
                keywords.append(keyword)

        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        if not keywords:
            # 2ê¸€ì ì´ìƒ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£]{2,}', query)
            keywords = words[:2]  # ìµœëŒ€ 2ê°œ

        return keywords

    def generate_multi_queries(self, query: str) -> List[str]:
        """ì›ë³¸ ì§ˆë¬¸ì—ì„œ ì—¬ëŸ¬ í•˜ìœ„ ì§ˆë¬¸ ìƒì„±"""
        queries = [query]  # ì›ë³¸ í¬í•¨

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(query)

        if keywords:
            main_keyword = keywords[0]

            # í…œí”Œë¦¿ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (2ê°œë§Œ)
            for template_type in ['ëŒ€ìƒ', 'í˜œíƒ']:
                if template_type in self.query_templates:
                    new_query = self.query_templates[template_type].format(keyword=main_keyword)
                    queries.append(new_query)

        logger.info(f"Multi-Query: {len(queries)}ê°œ ì§ˆë¬¸ ìƒì„±")
        for i, q in enumerate(queries, 1):
            logger.info(f"  {i}. {q}")

        return queries[:3]  # ìµœëŒ€ 3ê°œ


class AdvancedAnswerSynthesizer:
    """ê³ ê¸‰ ë‹µë³€ í•©ì„± - êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±"""

    def __init__(self):
        self.answer_template = {
            'ì •ì˜': '**{policy}ì´ë€?**\n{content}',
            'ëŒ€ìƒ': '**ì§€ì› ëŒ€ìƒ**\n{content}',
            'ì¡°ê±´': '**ì‹ ì²­ ì¡°ê±´**\n{content}',
            'í˜œíƒ': '**ì œê³µ í˜œíƒ**\n{content}',
            'ì‹ ì²­': '**ì‹ ì²­ ë°©ë²•**\n{content}',
        }

    def categorize_content(self, content: str) -> str:
        """ë‚´ìš©ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
        if any(kw in content for kw in ['ëŒ€ìƒ', 'í•´ë‹¹', 'ìê²©']):
            return 'ëŒ€ìƒ'
        elif any(kw in content for kw in ['ì¡°ê±´', 'ê¸°ì¤€', 'ìš”ê±´']):
            return 'ì¡°ê±´'
        elif any(kw in content for kw in ['í˜œíƒ', 'ì§€ì›', 'ê¸‰ì—¬', 'ì„œë¹„ìŠ¤']):
            return 'í˜œíƒ'
        elif any(kw in content for kw in ['ì‹ ì²­', 'ì ‘ìˆ˜', 'ë“±ë¡', 'ì œì¶œ']):
            return 'ì‹ ì²­'
        else:
            return 'ì •ì˜'

    def synthesize_structured_answer(
        self,
        question: str,
        doc_contents: List[Dict[str, str]]
    ) -> str:
        """ë¬¸ì„œë“¤ì„ êµ¬ì¡°í™”ëœ ë‹µë³€ìœ¼ë¡œ í•©ì„±"""

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        categorized = {}
        for doc in doc_contents:
            content = doc.get('content', '')
            region = doc.get('region', 'ì§€ì—­ë¯¸ìƒ')
            filename = doc.get('filename', 'ì •ì±…ë¬¸ì„œ')

            category = self.categorize_content(content)

            if category not in categorized:
                categorized[category] = []

            categorized[category].append({
                'content': content,
                'region': region,
                'filename': filename
            })

        # êµ¬ì¡°í™”ëœ ë‹µë³€ ìƒì„±
        answer_parts = []
        answer_parts.append(f"'{question}'ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì•ˆë‚´í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n")

        # ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„
        category_order = ['ì •ì˜', 'ëŒ€ìƒ', 'ì¡°ê±´', 'í˜œíƒ', 'ì‹ ì²­']

        for category in category_order:
            if category in categorized:
                # ì„¹ì…˜ í—¤ë”
                if category == 'ì •ì˜':
                    answer_parts.append("\nğŸ“‹ **ì •ì±… ê°œìš”**\n")
                elif category == 'ëŒ€ìƒ':
                    answer_parts.append("\nğŸ‘¥ **ì§€ì› ëŒ€ìƒ**\n")
                elif category == 'ì¡°ê±´':
                    answer_parts.append("\nğŸ“Œ **ì‹ ì²­ ì¡°ê±´**\n")
                elif category == 'í˜œíƒ':
                    answer_parts.append("\nğŸ’° **ì œê³µ í˜œíƒ**\n")
                elif category == 'ì‹ ì²­':
                    answer_parts.append("\nğŸ“ **ì‹ ì²­ ë°©ë²•**\n")

                # ë‚´ìš© ì¶”ê°€ (ìµœëŒ€ 2ê°œ)
                for i, item in enumerate(categorized[category][:2], 1):
                    answer_parts.append(f"{i}. {item['content']}\n")
                    answer_parts.append(f"   (ì¶œì²˜: {item['filename']}, {item['region']})\n\n")

        # ì¶”ê°€ ì•ˆë‚´
        answer_parts.append("\nğŸ’¡ **ì¶”ê°€ ë¬¸ì˜**\n")
        answer_parts.append("â€¢ ë” ìì„¸í•œ ë‚´ìš©ì€ ê´€í•  ì£¼ë¯¼ì„¼í„°ë‚˜ êµ¬ì²­ì— ë¬¸ì˜í•˜ì„¸ìš”.\n")
        answer_parts.append("â€¢ ë³µì§€ë¡œ í™ˆí˜ì´ì§€(www.bokjiro.go.kr)ì—ì„œë„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

        return "".join(answer_parts)


class AdvancedRAGPipeline:
    """í†µí•© ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        self.query_expander = QueryExpander()
        self.reranker = DocumentReranker()
        self.multi_query_gen = MultiQueryGenerator()
        self.answer_synthesizer = AdvancedAnswerSynthesizer()

    def process(
        self,
        query: str,
        vector_store,
        embedder,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ Advanced RAG Pipeline ì‹œì‘")
        logger.info(f"{'='*60}")

        # Step 1: Query Expansion
        logger.info("\n[Step 1] Query Expansion")
        expanded_queries = self.query_expander.expand_query(query)

        # Step 2: Multi-Query ê²€ìƒ‰
        logger.info("\n[Step 2] Multi-Query Search")
        all_docs = []
        seen_ids = set()

        for exp_query in expanded_queries:
            docs = vector_store.similarity_search(exp_query, embedder, k=top_k * 2)
            for doc in docs:
                # ì¤‘ë³µ ì œê±° (content ê¸°ë°˜)
                doc_id = doc.get('content', '')[:100]
                if doc_id not in seen_ids:
                    all_docs.append(doc)
                    seen_ids.add(doc_id)

        logger.info(f"ğŸ“Š ì´ {len(all_docs)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„)")

        # Step 3: Reranking
        logger.info("\n[Step 3] Reranking")
        reranked_docs = self.reranker.rerank(query, all_docs, top_k=top_k)

        # Step 4: Answer Synthesis
        logger.info("\n[Step 4] Answer Synthesis")
        if reranked_docs:
            doc_contents = []
            for doc in reranked_docs:
                doc_contents.append({
                    'content': doc.get('content', ''),
                    'region': doc['metadata'].get('region', 'ì§€ì—­ë¯¸ìƒ'),
                    'filename': doc['metadata'].get('filename', 'ì •ì±…ë¬¸ì„œ')
                })

            answer = self.answer_synthesizer.synthesize_structured_answer(
                query, doc_contents
            )

            return {
                'answer': answer,
                'documents': reranked_docs,
                'method': 'advanced_rag'
            }
        else:
            return {
                'answer': f"'{query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                'documents': [],
                'method': 'advanced_rag_no_docs'
            }
